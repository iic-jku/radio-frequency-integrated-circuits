[
  {
    "objectID": "rfic.html#sec-intro-wireless",
    "href": "rfic.html#sec-intro-wireless",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "1.1 Wireless Transmission",
    "text": "1.1 Wireless Transmission\nIn wireless transmission, we usually want to transmit data via a transmitter (TX) and a connected antenna to a receiver (RX) using an electromagnetic (EM) wave. This arrangement is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: The block diagram of a simple wireless system.\n\n\n\n\n\nUnfortunately, wireless transmission is hard. The wireless channel, i.e., the usage of electromagnetic waves to transmit information from a transmitter to a receiver, while tremendously useful, unfortunately has quite a few undesired features:\n\nThe wireless channel is shared between all users.\nAs a consequence, the available bandwidth is shared; this means that bandwidth is a scarce resource.\nThe wireless channel has significant losses.\nThe channel is time variant, as usually the transmitter and/or the receiver move, and/or the environment changes.\n\nIn order to estimate the power \\(P_\\mathrm{R}\\) of the wireless transmission at the receiver we can use Friis’ transmission formula (Pozar 2011):\n\\[\nP_\\mathrm{R} = \\frac{P_\\mathrm{T}}{4 \\pi d^2} \\cdot A_\\mathrm{R} = P_\\mathrm{T} \\cdot \\frac{A_\\mathrm{R} \\cdot A_\\mathrm{T}}{d^2 \\lambda^2}\n\\tag{1}\\]\nHere, \\(A_\\mathrm{R}\\) (and \\(A_\\mathrm{T}\\)) is the effective area of the receive/transmit antenna, while \\(d\\) is the distance (line of sight) between the two antennas. The effective area of an antenna depends on the type and construction, but generally we can say that\n\\[\nA \\propto \\lambda^2\n\\]\nFor an isotropic antenna (a theoretical construct where the radiation is equal in all directions) \\(A = \\lambda^2 / (4 \\pi)\\), while for a \\(\\lambda/2\\)-dipole \\(A = 0.13 \\lambda^2\\). Of course, the speed of light \\(c\\) relates frequency \\(f\\) and wavelength \\(\\lambda\\) of an electromagnetic wave by\n\\[\nc = \\lambda f.\n\\]\nGenerally speaking, the size of an electromagnetic antenna is proportional to the wavelength of the EM wave used for transmission. For many devices, we seek antennas on the order of a few centimeters, and this is why frequencies in the hundreds of MHz to GHz are so popular. Table 1 lists a few typical applications and their frequency and wavelength.\n\n\n\nTable 1: Typical RF applications with their operating frequencies and corresponding wavelengths\n\n\n\n\n\nApplication\nFrequency\nWavelength\n\n\n\n\nFM Radio\n88–108 MHz\n2.8-3.4 m\n\n\nWiFi (lowband)\n2.4 GHz\n12.5 cm\n\n\nWiFi (highband)\n5 GHz\n6 cm\n\n\nBluetooth\n2.4 GHz\n12.5 cm\n\n\nCellular\n0.6–5 GHz\n6-50 cm\n\n\nGNSS\n1.575 GHz\n19 cm\n\n\n\n\n\n\nAs you can see in Table 1 many of these antennas would not fit into the used device form factors, i.e., often we have to use electrically small antennas. As antenna design is not part of this course but critically important for wireless systems, please refer to (Balanis 2005) or this website for more information on antenna design.\n\n\n\n\n\n\nNote 1: Wavelength Calculation\n\n\n\nLet’s calculate the wavelength for a Bluetooth signal at 2.4 GHz. Given:\n\nFrequency \\(f = 2.4\\) GHz \\(= 2.4 \\times 10^9\\) Hz\n\nSpeed of light \\(c = 3 \\times 10^8\\) m/s\n\nUsing the relationship \\(c = \\lambda f\\), we can solve for wavelength:\n\\[\\lambda = \\frac{c}{f} = \\frac{3 \\times 10^8 \\text{ m/s}}{2.4 \\times 10^9 \\text{ Hz}} = 0.125 \\text{ m} = 12.5 \\text{ cm}\\]\nThis means that a quarter-wavelength monopole antenna for 2.4 GHz Bluetooth would be approximately 3.1 cm long, which easily fits into most mobile devices.\n\n\nIn order to get a feeling for the attenuation experienced in wireless communication, we now calculate the following exemplary transmission. We will use the unit of dBm which is often used in RF design and is defined as\n\\[\nP|_\\text{dBm} = 10 \\cdot \\log_{10} \\left( \\frac{P|_\\text{W}}{1\\,\\text{mW}} \\right)\n\\tag{2}\\]\n\n\n\n\n\n\nNote 2: Wireless Transmission\n\n\n\nWe use the following parameters:\n\nTransmit power \\(P_\\mathrm{T} = 1\\) W\nFrequency \\(f = 2.4\\) GHz\nCommunication distance \\(d = 10\\) km\nUsing \\(\\lambda/2\\) dipoles on both ends\n\nUsing Equation 1 we calculate\n\\[\nP_\\mathrm{R} = P_\\mathrm{T} \\cdot \\frac{0.13 \\lambda^2 \\cdot 0.13 \\lambda^2}{d^2 \\lambda^2} = P_\\mathrm{T} \\cdot 0.13^2 \\left( \\frac{\\lambda}{d} \\right)^2 =  2.64\\,\\text{pW} = -85.8\\,\\text{dBm}\n\\]\nWith the transmit power of 1 W = 30 dBm we have an attenuation of 116 dB! This is a very large number!\n\n\nThe loss we calculated in Note 2 is called the free-space path loss (FSPL). It is the minimum loss we can expect in a wireless communication system. In reality, the situation is often even worse. The free-space path loss \\(\\text{FSPL}\\) (in dB) can be calculated as\n\\[\n\\text{FSPL} = 20 \\cdot \\log_{10}(d / \\text{m}) + 20 \\cdot \\log_{10}(f / \\text{Hz}) + 20 \\cdot \\log_{10} \\left( \\frac{4\\pi}{c} \\text{m/s} \\right).\n\\tag{3}\\]\nEquation 3 can be readily derived from Equation 1 and Equation 2 assuming isotropic antennas at transmitter and receiver. Using Equation 3 we can easily calculate the FSPL for different distances and frequencies. The results are shown in Figure 2. It should be noted that the FSPL increases by 20 dB per decade of distance and 20 dB per decade of frequency, making higher frequencies and longer distances very challenging.\n\n\n\n\n\n\n\n\nFigure 2: Free space path loss vs. distance for different frequencies (1 GHz, 10 GHz, and 100 GHz).\n\n\n\n\n\nAs dire as the situation of Figure 2 already looks, this is not even all factors considered:\n\nThe given attenuation is for line-of-sight paths; often, the attenuation is significantly higher than this due to blockage by buildings, mountains, rain, or foliage.\nIn the absence of a direct line-of-sight path, the EM wave is redirected by reflections, causing additional attenuation, and potential destructive interference by multi-path reception.\n\nThe consequences of this are (among others):\n\nThe transmitter needs to generate enough transmit power to overcome the transmission loss; this has to be done often with high efficiency, as the transmit device is battery operated or limited by cooling.\nThe receiver has to be able to process weak signals, i.e., the noise level of the signal processing has to be very low.\nOften, the receive signal is very weak, while there are strong signals at other frequencies (i.e., other wireless transmitters are located close to the receiver). This means the receiver has to be able to process a weak signal while simultaneously tolerating large interfering signals (called blockers).\nSince the frequency spectrum is shared among many users and wireless applications, the transmit information has to be packed efficiently into a small bandwidth.\nVery often, wireless devices are battery-operated. This means transmit and receive functions have to be implemented using minimum power consumption.\n\nAs stated in the beginning, designing wireless systems is hard."
  },
  {
    "objectID": "rfic.html#sec-intro-standards",
    "href": "rfic.html#sec-intro-standards",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "1.2 Wireless Standards",
    "text": "1.2 Wireless Standards\nIn order to allow communication between different devices, different operators, and different manufacturers, wireless communication is standardized. There are many different standards, each with its own characteristics. Wireless standards define every aspect of wireless communication, and can be documents with hundreds or thousands of pages. Here, we mainly focus on the radio-frequency and analog aspects of wireless standards. Summarized in Table 2 are a few popular wireless standards with their main characteristics.\n\n\n\nTable 2: Comparison of wireless communication standards\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard\nGSM (2G)\nWCDMA (3G)\nLTE (4G)5G NR\nWiFi\nBluetooth\nGNSS\n\n\n\n\nFrequency range (MHz)\n850, 900, 1800, 1900\n850, 900, 1700, 1900, 2100\nMultiple bands 450…7100 (FR1), 24000…48000 (FR2)\n2400, 5000, 6000\n2400\n1500, 1200\n\n\nModulation\nGMSK, 8PSK (EDGE)\nQPSK (DL), BPSK (UL), 16QAM (HSPA), 64QAM (HSPA)\nQPSK, 16QAM, 64QAM (DL+UL) 256QAM (DL+UL)\nBPSK, QPSK, 16QAM, 64QAM, 256QAM, 1024QAM, 4096QAM\nGFSK (m=0.28…0.35), \\(\\pi\\)/4-DQPSK, 8DPSK\nBPSK, QPSK\n\n\nTransmission/ multiple access\nTDMA, FDMA\nDS-CDMA\nOFDMA (DL+UL), SC-FDMA/DFT-s-OFDM (UL)\nOFDM, CSMA/CA\nFHSS\nCDMA\n\n\nDuplex\nFDD\nFDD\nFDD, TDD\nTDD\nTDD\nn/a\n\n\nChannel bandwidth\n200 kHz\n5 MHz\n1.4, 3, 5, 10, 15, 20, …, 100 MHz (FR1), 400 MHz (FR2)\n10, 20, 40, 80, 160, 320 MHz\n1 MHz\n16…24 MHz\n\n\nSymbol rate\n270.833 ksym/s\n3.84 Msym/s\n15/30/60 ksym/s\n312.5 ksym/s\n1 Msym/s\n50 sym/s\n\n\nPulse shaping\nGaussian (BT=0.3)\nRoot Raised Cosine (\\(\\alpha\\)=0.22)\nRectangular\nRectangular\nGaussian (BT=0.5)\nRectangular\n\n\nTransmit power\n1…2 W\n250 mW\n200 mW (FDD), 400 mW (TDD)\n100 mW\n1…100 mW\nn/a\n\n\nPAR (UL)\n0 dB (GMSK), 3 dB (8PSK)\n3…8 dB\n6…8 dB\nUp to 12 dB\n0 dB (GFSK), 3 dB (8DPSK)\nn/a\n\n\nMIMO\nno\nNot realized (DL 2x2)\nDL 4x4 (up to 8x8)\n2x2 (up to 8x8)\nno\nno\n\n\nChannel bond\nno\nUp to 4x5 MHz\nUp to 7x20/4x100 MHz\nUp to 80+80+80+80 MHz\nno\nno\n\n\n\n\n\n\nDuring this course, we will learn what these terms mean and how they impact the design of RF integrated circuits.\nAs you can see in Table 2, since LTE (4G) and 5G NR, many additional bands have been defined in the sub-6 GHz range (FR1) and also in the mm-wave range (FR2, 24.25 to 52.6 GHz). This means that modern wireless devices have to support many different frequency bands, which makes the design of RF frontends even more challenging. A good overview of the different frequency bands is given here for LTE and 5G NR.\nRFIC design is a multidisciplinary field, requiring knowledge from various engineering domains, as shown in Figure 3. This makes RFIC design challenging, but also very interesting!\n\n\n\n\n\n\n\n\nFigure 3: RF design as a multidisciplinary field requiring knowledge from various engineering domains (adapted from (Behzad Razavi 2011)).\n\n\n\n\n\nFurther, RFIC design requires careful consideration of many different aspects, as shown in Figure 4. Many parameters are often tightly coupled, requiring careful trade-offs during the design process.\n\n\n\n\n\n\n\n\nFigure 4: RFIC require careful design considerations and trade-offs (adapted from (Behzad Razavi 2011))."
  },
  {
    "objectID": "rfic.html#sec-channel-capacity",
    "href": "rfic.html#sec-channel-capacity",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.1 Channel Capacity",
    "text": "2.1 Channel Capacity\nIn Section 1 we have already discussed the fact that we need to pack information into a minimum bandwidth, as the available spectrum is limited. To appreciate the limits of information transfer, we need to understand how much information can be transmitted over a given bandwidth. This limit is given by the Shannon-Hartley theorem, which gives the maximum data rate \\(C\\) (in bit/s) that can be transmitted over a communication channel with bandwidth \\(B\\) (in Hz) and signal-to-noise ratio \\(\\text{SNR}\\) (with linear units):\n\\[\nC = B \\cdot \\log_2(1 + \\text{SNR})\n\\tag{4}\\]\nThis formula gives us a theoretical upper limit on the data rate that can be achieved with a given bandwidth and SNR under optimal conditions. It is important to note that this limit is only achievable with ideal coding and modulation schemes, which are not practical in real-world systems. However, it provides a useful benchmark for evaluating the performance of communication systems.\n\n\n\n\n\n\nNote 3: Channel Capacity Example\n\n\n\nLet us calculate the channel capacity for a system with a bandwidth of 2 MHz and an SNR of 7 dB (the BW and minimum SNR of Bluetooth LE for 1 Mbps). First, we need to convert the SNR from dB to linear units:\n\\[\n\\text{SNR} = 10^{{7}/{10}} \\approx 5\n\\]\nNow we can use Equation 4 to calculate the channel capacity:\n\\[\nC = B \\cdot \\log_2(1 + \\text{SNR}) = 2\\,\\text{MHz} \\cdot \\log_2(1 + 5) = 2\\,\\text{MHz} \\cdot 2.585 \\approx 5.2\\,\\text{Mbps}\n\\]\nThis sounds reasonable, as the user data rate for Bluetooth LE is 1 Mbps for the given SNR, which allows for quite some overhead for coding and protocols."
  },
  {
    "objectID": "rfic.html#sec-linearity",
    "href": "rfic.html#sec-linearity",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.2 Linearity",
    "text": "2.2 Linearity\nAs we have already seen in Section 1.1 the transmitter has to process large signals without distorting them, while the receiver has to process small signals in the presence of large signals. Both situations mean we need metrics and models to quantify and discuss linearity properties.\nWe are going to use a very simple, time-invariant model to study linearity, based on a Taylor polynomial.\n\n\n\n\n\n\nImportantLinearity and Time Invariance in RF Systems\n\n\n\nWe use time invariance to simplify the mathematics. In practice, many circuits and systems will show time variant behavior which leads to quite a few very interesting and important phenomena! A time-invariant nonlinear system is also called a “memoryless” system, as the output at time \\(t\\) only depends on the input at time \\(t\\).\nIn contrast, a system with memory (i.e., time-variant) will have an output at time \\(t\\) which depends on the input at time \\(t\\) and also on past inputs (e.g., at times \\(t - \\Delta T\\), \\(t - 2 \\Delta T\\), etc.). Examples of systems with memory are filters, which have a frequency-dependent response, or power amplifiers with thermal memory effects.\n\n\nWe model a nonlinear circuit block with the following Taylor polynomial:\n\\[\ny(t) = \\alpha_0 + \\alpha_1 x(t) + \\alpha_2 x(t)^2 + \\alpha_3 x(t)^3 + \\ldots\n\\tag{5}\\]\nUsually, the blocks under study will have higher order nonlinear terms but we often stop at 3rd order to keep things simple. For practical work, higher order terms should be included if necessary.\nWhich \\(x(t)\\) should we use to study wireless systems? Often, the bandwidth \\(f_\\mathrm{BW}\\) of a transmit signal is much smaller than the center frequency \\(f_0\\), i.e., \\(f_\\mathrm{BW} \\ll f_0\\). In this case using a sinusoidal signal as a model is both simple to handle and approximately correct.\n\n2.2.1 Single-Tone Linearity\nWe thus set (with \\(A\\) being the amplitude of the input signal and \\(\\omega = 2 \\pi f\\) the angular frequency)\n\\[\nx(t) = A \\cos(\\omega t)\n\\]\nand insert it into Equation 5. After some simple trigonometric manipulations we are at\n\\[\n\\begin{split}\ny(t) &=\n\\underbrace{ \\frac{1}{2} \\alpha_2 A^2 }_\\text{dc component}\n+\n\\underbrace{ \\left(\\alpha_1 A + \\frac{3}{4} \\alpha_3 A^3 \\right) \\cos(\\omega t) }_\\text{fundamental} \\\\\n&+\n\\underbrace{ \\frac{1}{2} \\alpha_2 A^2 \\cos(2 \\omega t) }_\\text{2nd harmonic}\n+\n\\underbrace{ \\frac{1}{4} \\alpha_3 A^3 \\cos(3 \\omega t) }_\\text{3rd harmonic}\n\\end{split}\n\\tag{6}\\]\nLooking at Equation 6 we can make a few interesting observations:\n\nEven-order nonlinearity (\\(\\alpha_2\\)) creates low-frequency components; it effectively adds frequency components related to the envelope \\(A\\). If \\(A\\) is a constant then this results in a dc term; if \\(A(t)\\) is time variant it will create a squared version of it at low frequencies.\nThe \\(\\alpha_1\\) term is the gain of the circuit block.\nOdd-order nonlinearity (\\(\\alpha_3\\)) can impact the gain of the fundamental term passing through the block. Depending on the sign of \\(\\alpha_3\\) this can lead to gain contraction or expansion.\nEven- and odd-order nonlinearities create additional frequency components, so-called harmonics of the fundamental frequency. These harmonics are often unwanted, as they are far outside the wanted transmission frequency range, and need to be minimized, by either\n\nuse a lowpass filter to filter these harmonics, or\nincrease the linearity, i.e., make the \\(\\alpha_2\\), \\(\\alpha_3\\), etc., small enough.\n\n\nThe created harmonics are illustrated in Figure 5. Note that measuring harmonics to quantify the nonlinearity metrics like \\(\\alpha_2\\) and \\(\\alpha_3\\) is often not very accurate, as these harmonics are often filtered in bandwidth-limited systems.\n\n\n\n\n\n\n\n\nFigure 5: Single-tone test showing created harmonics at 2ω and 3ω.\n\n\n\n\n\nHow can we quantify the nonlinearity with a one-tone test? We can sweep the input signal \\(x(t)\\) in amplitude, and observe the output \\(y(t)\\). If the observed gain drops by 1 dB from the small-signal value we note the input power, and call this point the 1dB compression point (\\(P_\\mathrm{1dB}\\)). We should always add whether this 1dB compression point is input- or output-referred to avoid ambiguity. The diagram in Figure 6 shows this test (\\(\\alpha_1 = 100\\), \\(\\alpha_3 = -0.2\\)).\n\n\n\n\n\n\n\n\nFigure 6: 1dB compression point test showing input vs output power relationship and the definition of P1dB.\n\n\n\n\n\n\n\n\n\n\n\nImportantCompressive vs. Expansive Behavior\n\n\n\nNote that for compressive behaviour, \\(\\alpha_3\\) and \\(\\alpha_1\\) have different signs, while for expansive behaviour, they have the same sign.\nAt some point, every circuit block will show compressive behavior, as the maximum signal amplitude will be limited by power supply voltages, device breakdown voltages, etc.\n\n\n\n\n2.2.2 Multi-Tone Linearity\nWe now elevate our investigations and apply two sinusoids with different frequencies and different amplitudes and see which signals we get at the output of the nonlinear block. The two-tone test and resulting third-order intermodulation products (IM3) are illustrated in Figure 7.\n\\[\nx(t) = A_1 \\cos(\\omega_1 t) + A_2 \\cos(\\omega_2 t)\n\\]\nWe apply the above stimulus to our nonlinear model described by Equation 5 and again, after some trigonometric manipulations, arrive at:\n\\[\ny(t) = y'(t) + y''(t) + y'''(t)\n\\tag{7}\\]\nAs many different frequency components are created by this simple two-tone test (and nonlinearity only up to 3rd order) we split the result into different equations and look at the result separately.\nFirst, we start with the fundamental tones:\n\\[\n\\begin{split}\n    y'(t) &= \\left( \\underbrace{\\alpha_1 A_1 + \\frac{3}{4} \\alpha_3 A_1^3}_\\text{compression/expansion} + \\underbrace{\\frac{3}{2} \\alpha_3 A_1 A_2^2}_\\text{cross-modulation/desens} \\right) \\cos(\\omega_1 t) \\\\\n    &+ \\left( \\underbrace{\\alpha_1 A_2 + \\frac{3}{4} \\alpha_3 A_2^3}_\\text{compression/expansion} + \\underbrace{\\frac{3}{2} \\alpha_3 A_2 A_1^2}_\\text{cross-modulation/desens} \\right) \\cos(\\omega_2 t)\n\\end{split}\n\\tag{8}\\]\nAs shown in Equation 8, interesting things happen:\n\nWe (again) have the gain compression/expansion effect as already discussed in Section 2.2.1.\nIn addition, we have cross-modulation, i.e., the envelope of one tone (e.g., \\(A_2(t)\\) of the tone at \\(\\omega_2\\)) impacts the envelope of the other tone at \\(\\omega_1\\). This can lead to unwanted signal distortion, even if there is a large frequency separation between \\(\\omega_1\\) and \\(\\omega_2\\)!\nFurther, since the sign of \\(\\alpha_3\\) is usually opposite to \\(\\alpha_1\\), this can also lead to desensitization (“desens”). If, for example, \\(A_2 \\gg A_1\\), then there would be no compression due to the tone \\(\\omega_1\\) itself, however, the large tone at \\(\\omega_2\\) will lead to gain compression of the tone at \\(\\omega_1\\); this effect is called desense.\n\nWe now look at the next class of generated tones:\n\\[\n\\begin{split}\n    y''(t) &= \\frac{1}{2} \\alpha_2 A_1^2 + \\frac{1}{2} \\alpha_2 A_2^2 \\\\\n    &+ \\alpha_2 A_1 A_2 \\cos[ (\\omega_1 - \\omega_2) t] \\\\\n    &+ \\alpha_2 A_1 A_2 \\cos[ (\\omega_1 + \\omega_2) t]\n\\end{split}\n\\tag{9}\\]\nAs we can see in Equation 9 new tones are created (besides the low frequency components we already know from the single-tone test) at the sum and difference of \\(\\omega_1\\) and \\(\\omega_2\\). These new frequency components are called “intermodulation products of second order” (IM2). These tones are created by the even-order nonlinearity (\\(\\alpha_2\\)). These IM2 products are far away from the wanted tones, so are often not very problematic in amplifiers (but there can be exceptions!). However, they can be very problematic in frequency conversion blocks like mixers. We will come back to this point when discussing zero-IF receivers.\nWe now investigate the next couple of tones:\n\\[\n\\begin{split}\n    y'''(t) &= \\frac{3}{4} \\alpha_3 A_1^2 A_2 \\cos[(2 \\omega_1 + \\omega_2) t] \\\\\n    &+ \\frac{3}{4} \\alpha_3 A_1^2 A_2 \\cos[(2 \\omega_1 - \\omega_2) t] \\\\\n    &+ \\frac{3}{4} \\alpha_3 A_1 A_2^2 \\cos[(2 \\omega_2 + \\omega_1) t] \\\\\n    &+ \\frac{3}{4} \\alpha_3 A_1 A_2^2 \\cos[(2 \\omega_2 - \\omega_1) t]\n\\end{split}\n\\tag{10}\\]\nThe tones shown in Equation 10 are called “intermodulation products of third order” (IM3), and are caused by the odd nonlinearities (like \\(\\alpha_3\\)). While the IM3 tones located at \\(2 \\omega_1 + \\omega_2\\) and \\(\\omega_1 + 2 \\omega_2\\) are similar to the sum IM2 tone and far away from \\(\\omega_1\\) and \\(\\omega_2\\), the other two tones are concerning.\nExpressing \\(\\Delta \\omega = \\omega_2 - \\omega_1\\) (and assuming \\(\\omega_1 &lt; \\omega_2\\)), the building law of \\(2 \\omega_1 - \\omega_2 = \\omega_1 - \\Delta \\omega\\) and \\(2 \\omega_2 - \\omega_1 = \\omega_2 + \\Delta \\omega\\) results in new tones right besides \\(\\omega_1\\) and \\(\\omega_2\\), with a frequency separation only defined by \\(\\Delta \\omega\\). This situation is illustrated in Figure 7.\n\n\n\n\n\n\n\n\nFigure 7: Two-tone test showing fundamental frequencies ω₁, ω₂ and third-order intermodulation products (IM3) at 2ω₁-ω₂ and 2ω₂-ω₁.\n\n\n\n\n\nThis close localization of the IM3 tones can also be utilized to characterize nonlinear performance. Using gain compression or harmonic generation (H3) it can be very difficult to extract nonlinearity of third order (\\(\\alpha_3\\)). However, using a two-tone test, the IM3 tones can be readily measured, even if the measured signal path shows a bandpass characteristic! As RF systems frequently employ bandpass filters to suppress out-of-band signals, this is a very important property of the two-tone test.\nThe resulting test is called a two-tone test yielding the third-order intercept point (IP3). This test is widely used in RF design to characterize the linearity of amplifiers, mixers, and complete transceiver systems. The power relationship between fundamental tones and IM3 products as a function of input power is shown in Figure 8.\n\n\n\n\n\n\n\n\nFigure 8: Two-tone IM3 test showing fundamental and IM3 product power vs. input power, with IP3 intercept point definition. Equal input power per tone is assumed.\n\n\n\n\n\nNote that, as shown in Figure 8, the IM3 products rise with a slope of 3 dB/dB, i.e., if the input power is increased by 1 dB, the IM3 products increase by 3 dB. The fundamental tones rise with a slope of 1 dB/dB (as long as we are in the linear region). The IP3 point is defined as the intersection of the extrapolated linear lines of fundamental and IM3 products. As both lines have different slopes, this intersection point is usually far outside the actual operating range of the circuit block under test!\nWhen calculating the IIP3 (input-referred IP3) we can use the following formula, assuming equal input power per tone. It is important to always check the slope of the IM3 products to ensure that we are indeed in the third-order region! If the input power per tone is \\(P_\\mathrm{in}\\) (in dBm) and the input-referred power of one IM3 tone is \\(P_\\mathrm{IM3}\\) (in dBm), then the input-referred IP3 is given by\n\\[\n\\text{IIP3} = P_\\mathrm{in} + \\frac{P_\\mathrm{in} - P_\\mathrm{IM3}}{2}\n\\tag{11}\\]\nFurther, for mildly nonlinear systems (i.e., \\(\\alpha_3\\) is dominating), the IIP3 can be approximated from the 1dB compression point as\n\\[\n\\text{IIP3}|_\\mathrm{dBm} \\approx P_\\mathrm{1dB}|_\\mathrm{dBm} + 9.6\\,\\text{dB}.\n\\tag{12}\\]\nIf we have two blocks which are cascaded, and we know the gain and IIP3 of both blocks, we can calculate the overall IIP3 of the cascade with the following approximation. An exact calculation is very involved, as the nonlinearities of the first block (and the resulting tones) will be processed by the second block, creating even more tones; this process escalates very quickly. However, for practical purposes, the following approximation is often sufficient:\n\\[\n\\frac{1}{\\text{IIP3}_\\text{total}} \\approx \\frac{1}{\\text{IIP3}_1} + \\frac{G_1}{\\text{IIP3}_2} + \\frac{G_1 G_2}{\\text{IIP3}_3}\n\\tag{13}\\]\nHere \\(G_1\\) is the linear gain of the first block, and \\(\\text{IIP3}_1\\), \\(\\text{IIP3}_2\\) are the input-referred IP3 of the first and second block, respectively. Note that all powers have to be in linear units (i.e., Watts) when using Equation 13. An even more simplified version of Equation 13 can be used with all quantities given in dBm and dB, respectively:\n\\[\n\\text{IIP3}_\\text{total} \\approx \\min \\{  \\text{IIP3}_1, \\text{IIP3}_2 - G_1, \\text{IIP3}_3 - G_1 - G_2 \\}\n\\tag{14}\\]\nA typical RF system cascade with multiple blocks and their individual IIP3 contributions is shown in Figure 9.\n\n\n\n\n\n\n\n\nFigure 9: Block cascade for IIP3 calculation showing multiple stages with gains and individual IIP3 values.\n\n\n\n\n\n\n\n\n\n\n\nNote 4: Simple IIP3 Cascade Calculation\n\n\n\nLet’s calculate the overall IIP3 of two cascaded blocks. The first block is a low-noise amplifier with an IIP3 of -10 dBm and a gain of 20 dB. The second block is a mixer that has a gain of 10 dB and an IIP3 of 5 dBm. What is the overall IIP3?\nUsing Equation 14 we can quickly estimate:\n\\[\n\\text{IIP3}_\\text{total} \\approx \\min \\{ -10\\,\\text{dBm}, 5\\,\\text{dBm} - 20\\,\\text{dB} = -15\\,\\text{dBm} \\} = -15\\,\\text{dBm}\n\\]\nWe see that the overall IIP3 is limited by the linearity of the second block, as the first block amplifies all signals (including blockers) by 20 dB before they reach the second block."
  },
  {
    "objectID": "rfic.html#sec-noise",
    "href": "rfic.html#sec-noise",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.3 Noise",
    "text": "2.3 Noise\nJust as nonlinearity is a limiting factor for large signals, noise is the limiting factor for small signals. Noise is present in all electronic circuits and systems, and it is impossible to avoid it. However, we can try to minimize its impact on system performance.\nNoise is usually characterized by its power spectral density (PSD) in units of Watts per Hertz (W/Hz). For example, thermal noise at room temperature has a PSD of approximately \\(k T = 4 \\times 10^{-21}\\) W/Hz, or –174 dBm/Hz (with the Boltzmann constant \\(k = 1.38 \\times 10^{-23}\\,\\text{J/K}\\)). This means that if we have a bandwidth of 1 MHz, the total thermal noise power would be:\n\\[\nP_\\mathrm{thermal} = \\text{PSD} \\cdot B = -174\\,\\text{dBm/Hz} + 10 \\log_{10} \\left( \\frac{1\\,\\text{MHz}}{1\\,\\text{Hz}} \\right) = -114\\,\\text{dBm}\n\\]\nThe PSD of noise can be flat vs. frequency (which is called “white noise”), or can decrease with frequency (e.g., “flicker noise” or “1/f noise”). Further, noise can be generated by resistors (thermal noise), semiconductors (shot noise, generation-recombination noise), etc. A detailed discussion of noise sources can be found in (Gray et al. 2009) or (Behzad Razavi 2017).\n\n2.3.1 Types of Noise Generation\nResistors generate thermal noise, which is white noise with a PSD of \\(4 k T R\\) (in V\\(^2\\)/Hz) when looking at the voltage across the resistor, or \\(4 k T / R\\) (in A\\(^2\\)/Hz) when looking at the current through the resistor. This noise is generated by the random thermal motion of charge carriers in the resistor.\n\n\n\n\n\n\nImportantThermal Noise\n\n\n\nNote that the simple approximation given above is only valid for reasonably low frequencies and typical temperatures, and is known as the Rayleigh-Jeans approximation of Planck’s blackbody radiation accounting for quantum effects and is given by (Pozar 2011)\n\\[\n\\text{PSD} = \\frac{4 R h f}{e^{{h f}/{k T}} - 1}\n\\]\nwhere \\(h\\) is the Planck constant (\\(h = 6.626 \\times 10^{-34}\\) Js) and \\(f\\) is the frequency. The Rayleigh-Jeans approximation is valid for \\(f \\ll k T / h\\), which is approximately 6 THz at room temperature (290 K).\nWe can integrate the above PSD over the full frequency range and show the rms noise voltage of a resistor \\(R\\) is bounded to\n\\[\n\\overline{v_\\mathrm{n}^2} = \\int_0^\\infty \\frac{4 R h f}{e^{{h f}/{k T}} - 1} df = \\frac{2 (\\pi k T)^2}{3 h} \\cdot R\n\\]\nwhich equates to approximately 13 mVrms noise voltage for a 1 k\\(\\Omega\\) resistor at room temperature (which is impossible to measure in practice, as there will be some form of bandwidth limitation in any real measurement setup).\n\n\nMOSFETs generate several types of noise, the most important ones being the thermal noise of the channel and flicker noise.\nThe thermal noise of the channel can be modeled as a current noise source between drain and source with a PSD of \\(\\overline{I_\\mathrm{n}^2} = 4 k T \\gamma g_{d0}\\) (in A\\(^2\\)/Hz), where \\(\\gamma\\) is a process-dependent parameter (usually between 2/3 and 2). The parameter \\(g_{d0}\\) is the small-signal output conductance of the MOSFET in triode, i.e., \\(g_{d0} = g_\\mathrm{ds}\\), or equal to \\(g_{d0} = g_\\mathrm{m}\\) when in saturation.\nIn saturation, it is often useful to express the thermal noise as a voltage noise source at the gate with a PSD of \\(\\overline{V_\\mathrm{n}^2} = 4 k T \\gamma / g_\\mathrm{m}\\) (in V\\(^2\\)/Hz). We can see that we can lower this noise of the MOSFET by increasing the transconductance \\(g_\\mathrm{m}\\), which can be achieved by increasing the bias current.\nIn addition, at high frequencies, the MOSFET also has induced gate-current noise, which is correlated with the channel thermal noise. A detailed discussion of this noise source can be found in (Behzad Razavi 2017).\nFlicker noise is usually modeled as a voltage noise source at the gate with a PSD of \\(K_f / (C'_\\mathrm{ox}W L f)\\) (in V\\(^2\\)/Hz), where \\(K_f\\) is a process-dependent parameter, \\(C'_\\mathrm{ox}\\) is the oxide capacitance per unit area, \\(L\\) and \\(W\\) are the length and width of the MOSFET, and \\(f\\) is the frequency. Note that we can lower the flicker noise by increasing the area of the MOSFET (\\(W L\\)), however, this increases the parasitic capacitances associated with the MOSFET, and this often prohibitive for RF operation!\nIn bipolar junction transistors (BJTs), the most important noise source is the shot noise due to the diffusion current in the base-emitter junction. Its PSD can be modeled as a current noise source between collector and emitter with a PSD of \\(2 q I_\\mathrm{C}\\) (in A\\(^2\\)/Hz), where \\(q\\) is the elementary charge (\\(q = 1.6 \\times 10^{-19}\\) C) and \\(I_\\mathrm{C}\\) is the DC collector current.\n\n\n\n\n\n\nImportantEquivalence of Shot and Thermal Noise\n\n\n\nNote that it has been shown in (Sarpeshkar, Delbruck, and Mead 1993) that thermal noise and shot noise are actually equivalent, as both are generated by the random, thermally agitated motion of charge carriers!\n\n\nIdeal capacitors and inductors do not generate noise, however, real capacitors and inductors have parasitic resistances which generate thermal noise.\nIn RF systems additional noise sources can be present. One noteworthy example is the cosmic microwave background radiation, which can be modeled as a noise temperature of approximately 3 K. While this is negligible compared to thermal noise at room temperature (approximately 290 K), it can be significant in very low-noise systems, such as radio telescopes pointing to the sky. Another important noise source in RF systems is the atmospheric noise, which is generated by natural phenomena like lightning or in the ionosphere.\n\n\n\n\n\n\nNoteA Note on Circuit Noise Calculations\n\n\n\nWhen doing circuit noise calculations, it is instructive to keep the following points in mind:\n\nFor circuit calculations involving noise sources it is convenient to replace the power spectral density by equivalent sinusoidal generators in small bandwidths.\nThe noise power spectral density in a small bandwidth \\(\\Delta f\\) is given by \\(\\overline{V_\\mathrm{n}^2} = \\overline{v_\\mathrm{n}^2} / \\Delta f\\) and \\(\\overline{I_\\mathrm{n}^2} = \\overline{i_\\mathrm{n}^2} / \\Delta f\\).\nThe quantities \\(\\overline{V_\\mathrm{n}^2}\\) and \\(\\overline{I_\\mathrm{n}^2}\\) can be considered the mean-square value of sinusoidal generators. Using these values, network noise calculations reduce to familiar sinusoidal circuit-analysis calculations using \\(V_\\mathrm{n}\\) and \\(I_\\mathrm{n}\\).\nMultiple independent noise sources can be calculated individually at the output, and the total noise in bandwidth \\(\\Delta f\\) is calculated as a mean-square value by adding the individual mean-square contributions from each sinusoid.\n\n\n\n\n\n2.3.2 Noise in Impedance-Matched Systems\nWe now want to calculate the maximum noise power that can be extracted from a noisy source. We assume the following situation as shown in Figure 10. Note that the voltage source \\(\\overline{V_\\mathrm{n,s}^2}\\) models the thermal noise of the source resistor \\(R_\\mathrm{s}\\) resulting in a Thevenin equivalent circuit.\n\n\n\n\n\n\n\n\nFigure 10: A noise-matched system with source and load impedances.\n\n\n\n\n\nWe know that the noise of the source resistor is given by \\(\\overline{V_\\mathrm{n,s}^2} = 4 k T R_\\mathrm{s}\\). We assume the load resistor \\(R_\\mathrm{load}\\) as noiseless and matched to the source resistor, i.e., \\(R_\\mathrm{load} = R_\\mathrm{s}\\) for maximum power transfer. The noise power spectral density delivered to the load resistor is then given by\n\\[\nP_\\mathrm{n,load} = \\frac{\\overline{V_\\mathrm{n,load}^2}}{R_\\mathrm{load}} = \\frac{\\overline{V_\\mathrm{n,d}^2}}{4 R_\\mathrm{s}} = k T\n\\tag{15}\\]\nThe calculation of Equation 15 confirms the initial statement that the maximum noise power spectral density that can be extracted from a noisy source is \\(k T\\) (in W/Hz). This result is independent of the actual value of the source resistance \\(R_\\mathrm{s}\\).\nWe can further generalize the thermal noise of any impedance as\n\\[\n\\overline{V_\\mathrm{n}^2} = 4 k T \\Re \\{ Z \\}\n\\tag{16}\\]\nas for example in the complex impedance \\(Z_\\mathrm{ant}\\) of an antenna. Since an antenna is a reciprocal device, if we measure its radiation impedance \\(Z_\\mathrm{rad}\\) (for example with a vector network analyzer), we can calculate its thermal noise with Equation 16 to \\(\\overline{V_\\mathrm{n}^2} = 4 k T \\Re \\{ Z_\\mathrm{rad} \\}\\).\n\n\n2.3.3 Noise Figure\nIn RF systems, we often want to quantify the noise performance of a circuit block or a complete system. The most widely used metric is the noise factor (F), which is defined as the ratio of the signal-to-noise ratio (SNR) at the input to the SNR at the output of a circuit block or system. If we express the noise factor in dB, we call it the noise figure (NF) (Pozar 2011). The noise factor is given by\n\\[\nF = \\frac{\\text{SNR}_\\mathrm{in}}{\\text{SNR}_\\mathrm{out}} = \\frac{(P_\\mathrm{s}/P_\\mathrm{n})_\\mathrm{in}}{(P_\\mathrm{s}/P_\\mathrm{n})_\\mathrm{out}}\n\\tag{17}\\]\nwhere \\(P_\\mathrm{s}\\) is the signal power and \\(P_\\mathrm{n}\\) is the noise power. The noise factor is always larger than or equal to 1 (or 0 dB), as no circuit can improve the SNR!\n\n\n\n\n\n\nImportantSNR Improvement\n\n\n\nNote that the SNR can be improved by filtering, as filtering reduces the noise power. If the noise bandwidth is larger than the signal bandwidth, then the SNR can be improved without affecting the signal. However, this is not considered in the noise factor, as the noise factor assumes that both signal and noise pass through the same bandwidth.\n\n\nLet us look at a simple model of a noise circuit block as shown in Figure 11. The input signal \\(S_\\mathrm{in}\\) is accompanied by noise \\(N_\\mathrm{in}\\). By definition it is assumed that the input noise power results from a matched resistor at \\(T_0 = 290\\,\\text{K}\\), so that \\(N_\\mathrm{in} = k T_0\\). The circuit block has a power gain \\(G\\) and adds its own noise \\(N_\\mathrm{dut}\\) to the output signal. For simplicity, we assume that the input and output of the circuit block are impedance matched to avoid reflections.\n\n\n\n\n\n\n\n\nFigure 11: A noise-matched system with source and load impedances and a noisy circuit block.\n\n\n\n\n\nThe output signal and noise powers are then given by\n\\[\nS_\\mathrm{out} = G S_\\mathrm{in}\n\\]\n\\[\nN_\\mathrm{out} = G N_\\mathrm{in} + N_\\mathrm{dut}\n\\] The resulting noise factor can then be calculated as\n\\[\nF = \\frac{{S_\\mathrm{in}}/{N_\\mathrm{in}}}{{S_\\mathrm{out}}/{N_\\mathrm{out}}} = \\frac{1}{G} \\frac{G N_\\mathrm{in} + N_\\mathrm{dut}}{N_\\mathrm{in}} = 1 + \\frac{N_\\mathrm{dut}}{G N_\\mathrm{in}},\n\\]\nin other words, the noise factor is 1 plus the ratio of the noise added by the device under test (DUT) to the amplified input noise.\nNote that a noiseless block (\\(N_\\mathrm{dut} = 0\\)) has a noise factor of \\(F=1\\). A passive block with loss factor \\(L\\) (and impedance matched at input and output) has a noise factor of \\(F=L\\) (in linear units), as it attenuates the signal and \\(N_\\mathrm{out} = N_\\mathrm{in} = k T\\) if everything is in thermal equilibrium.\n\n\n\n\n\n\n\n\nFigure 12: Block cascade for noise factor calculation showing multiple stages with gains and individual noise factors.\n\n\n\n\n\nIf we have a cascade of multiple blocks, as shown in Figure 12, we can calculate the overall noise factor with the Friis formula (Pozar 2011)\n\\[\nF_\\mathrm{total} = 1 + (F_1 - 1) + \\frac{F_2 - 1}{G_1} + \\frac{F_3 - 1}{G_1 G_2}\n\\tag{18}\\]\nwhere \\(F_i\\) and \\(G_i\\) are the noise factor and power gain of the \\(i\\)-th block, respectively. Note that all gains have to be in linear units (not dB) when using Equation 18. We can interpret Equation 18 as follows:\n\nThe overall noise factor \\(F_\\mathrm{total}\\) is always larger than or equal to the noise factor of the first block (\\(F_1\\)).\nThe noise factor of the first block is the most important one, as the noise factors of the following blocks are reduced by the gain of all preceding blocks. This is especially important in RF receivers, where the first block is usually a low-noise amplifier (LNA) with a very low noise figure (e.g., 1 dB or less) and a high gain (e.g., 10 dB or more). This ensures that the noise of the following blocks is negligible.\nThe noise factor of the last block is reduced by the gain of all preceding blocks, so it is usually not very important.\n\nHere we also see a trade-off between noise and linearity, as shown by Equation 13 and Equation 18. For low noise, we should try to maximize \\(G_1\\), however, this will affect linearity (IIP3) in a negative way. As in many other situation in RF design, we have to find a good compromise between conflicting requirements.\n\n\n2.3.4 Sensitivity\nIn RF receivers, we often want to know the minimum input signal power that can be detected with a certain SNR. This minimum input signal power is called the sensitivity of the receiver. The sensitivity can be calculated as\n\\[\nP_\\mathrm{in, min} = P_\\mathrm{n} \\cdot \\text{SNR}_\\mathrm{min} \\cdot F\n\\tag{19}\\]\nwhere \\(P_\\mathrm{n}\\) is the noise power at the input, \\(\\text{SNR}_\\mathrm{min}\\) is the minimum detectable SNR, and \\(F\\) is the noise factor of the receiver. The input noise power can be calculated as\n\\[\nP_\\mathrm{n} = k T B\n\\]\nwhere \\(k\\) is the Boltzmann constant, \\(T\\) is the temperature in Kelvin, and \\(B\\) is the bandwidth of the receiver. Expressing Equation 19 in dBm we get the following formula:\n\\[\nP_\\mathrm{in, min}|_\\mathrm{dBm} = -174\\,\\text{dBm/Hz} + \\text{NF} + 10 \\log_{10}(B/\\text{Hz}) + \\text{SNR}_\\mathrm{min}|_\\mathrm{dB}\n\\tag{20}\\]\nwhere -174 dBm/Hz is the thermal noise PSD at room temperature (290 K). We can see that the sensitivity improves with lower noise figure, smaller bandwidth, and lower minimum detectable SNR.\n\n\n\n\n\n\nNote 5: Sensitivity Calculation for WiFi\n\n\n\nLet’s calculate the sensitivity of a WiFi receiver operating at 5 GHz with a bandwidth of \\(B = 80\\,\\text{MHz}\\), a noise figure of \\(NF = 7\\,\\text{dB}\\), and a minimum detectable SNR of 25 dB. This high SNR means that a high-order modulation scheme (like 64-QAM) is used for high data rates.\nUsing Equation 20 we get: \\[\nP_\\mathrm{in, min} = -174\\,\\text{dBm/Hz} + 7\\,\\text{dB} + 10 \\log_{10} (80 \\times 10^6) + 25\\,\\text{dB} \\approx -63\\,\\text{dBm}\n\\]\nThis means that the minimum input signal power that can be detected by the WiFi receiver is approximately -63 dBm."
  },
  {
    "objectID": "rfic.html#sec-fundamentals-modulation",
    "href": "rfic.html#sec-fundamentals-modulation",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.4 Modulation",
    "text": "2.4 Modulation\nIn order to transmit information via an EM wave, we need to modulate the EM wave with the information signal. Looking at a simple sinusoidal carrier wave\n\\[\ns(t) = A \\cos(\\omega_0 t + \\varphi)\n\\]\nwe see that we can change one or more of the following parameters to encode information:\n\nAmplitude \\(A(t)\\) (amplitude modulation, AM; the digital form is called amplitude-shift keying, ASK)\nFrequency \\(\\omega_0(t)\\) (frequency modulation, FM; the digital form is called frequency-shift keying, FSK)\nPhase \\(\\varphi(t)\\) (phase modulation, PM; the digital form is called phase-shift keying, PSK)\nAmplitude \\(A(t)\\) and phase \\(\\varphi(t)\\) (quadrature amplitude modulation, QAM)\n\nThe modulation formats FM and PM have the advantage that the carrier amplitude is constant, which makes them more robust against nonlinear distortion.\nQAM is widely used in modern communication systems, as it allows to transmit more bits per symbol by combining amplitude and phase modulation. The form with 4 different symbols is called QPSK. Higher-order modulation like 16-QAM, for example, uses 16 different symbols, which can encode 4 bits per symbol (as \\(2^4 = 16\\)). Even higher-order QAM formats like 64-QAM (6 bits per symbol), 256-QAM (8 bits per symbol), 1024-QAM (10 bits per symbol), or 4096-QAM (12 bits per symbol) are also used in modern systems like WiFi or LTE.\n\n\n\n\n\n\n\n\nFigure 13: 16-QAM constellation diagram with Gray code labeling of constellation points.\n\n\n\n\n\nShown in Figure 13 is the “constellation diagram” of a 16-QAM modulation format. The constellation points are arranged in a square grid, with each point representing a unique combination of amplitude and phase. The distance between the constellation points determines the robustness against noise and interference; larger distances result in better performance, but also require more bandwidth. The mapping of bits to constellation points is called “bit mapping” or “symbol mapping”. The example in Figure 13 uses a Gray code mapping, which minimizes the number of bit errors in case of a symbol error.\nThe constellation diagram can be imagined as a complex plane, where the x-axis represents the in-phase component (I) and the y-axis represents the quadrature component (Q) of the modulated signal. During transmission of a specific symbol, the RF carrier is modulated to the corresponding amplitude and phase, resulting in a specific point in the constellation diagram. In Figure 13, the amplitude and phase information for two consecutive symbols, \\(A_i\\)/\\(\\varphi_i\\) and \\(A_{i+1}\\)/\\(\\varphi_{i+1}\\), is shown. If we have a bitrate with a bit duration of \\(T_\\mathrm{b}\\), the symbol duration for 16-QAM (4 bits per symbol) is \\(T_\\mathrm{s} = 4 T_\\mathrm{b}\\). During the first \\(T_\\mathrm{s}\\), the carrier is modulated to \\(A_i\\)/\\(\\varphi_i\\), and during the next \\(T_\\mathrm{s}\\), it is modulated to \\(A_{i+1}\\)/\\(\\varphi_{i+1}\\).\nThe table below shows the SNR requirements for different modulation formats to achieve a bit error rate (BER) of \\(10^{-5}\\) in an additive white Gaussian noise (AWGN) channel. As we can see, higher-order modulation formats require higher SNR to achieve the same BER. Note that for the SNR values of this table no error correction coding is assumed; with error correction coding the required SNR can be significantly reduced!\n\n\n\nTable 3: SNR requirements for different modulation schemes to achieve BER = \\(10^{-5}\\) in AWGN channel\n\n\n\n\n\nModulation\nBits/Symbol\nRequired SNR (dB)\n\n\n\n\nBPSK\n1\n9.6\n\n\nQPSK\n2\n12.6\n\n\n16-QAM\n4\n18.2\n\n\n64-QAM\n6\n24.4\n\n\n256-QAM\n8\n30.6\n\n\n1024-QAM\n10\n36.9\n\n\n4096-QAM\n12\n43.2"
  },
  {
    "objectID": "rfic.html#sec-fundamentals-pulse-shaping",
    "href": "rfic.html#sec-fundamentals-pulse-shaping",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.5 Pulse Shaping and Spectral Efficiency",
    "text": "2.5 Pulse Shaping and Spectral Efficiency\nWhen we modulate symbols onto a carrier, we usually do not transmit the symbols as pure sinusoids, but rather as pulses with a certain shape. The pulse shape determines the bandwidth of the transmitted signal and its spectral efficiency. A common pulse shape is the rectangular pulse, which has a sinc-shaped spectrum. However, the sinc function \\(\\sin(\\pi x)/ \\pi x\\) has side lobes that extend to infinity, which can cause interference with adjacent channels.\nFor reference, the spectrum of a random binary sequence with equal probability of 0s and 1s, using rectangular pulses with a duration of \\(T_\\mathrm{b}\\) is given by (\\(S(f)\\) is the two-sided power spectral density):\n\\[\nS(f) = \\frac{T_\\mathrm{b}}{4} \\text{sinc}^2(fT_\\mathrm{b}) + \\frac{1}{4} \\delta(f) = \\frac{T_\\mathrm{b}}{4} \\left( \\frac{\\sin(\\pi f T_\\mathrm{b})}{\\pi f T_\\mathrm{b}} \\right)^2 + \\frac{1}{4} \\delta(f)\n\\]\nTo avoid this, we can use pulse shapes that have better spectral properties, such as the raised cosine pulse or the root-raised cosine pulse. The raised-cosine pulse has a roll-off factor \\(\\alpha\\) that determines the excess bandwidth beyond the Nyquist bandwidth. The root-raised cosine (RRC) pulse is used in practical systems (with half the pulse filter implemented at the TX, and half at the RX), as it can be implemented with a matched filter at the receiver.\nThe raised-cosine pulse \\(p(t)\\) (with a spectrum shaped like a raised cosine) is given by:\n\\[\np(t) = \\frac{\\sin(\\pi t / T_\\mathrm{b})}{\\pi t / T_\\mathrm{b}} \\cdot \\frac{\\cos(\\alpha \\pi t / T_\\mathrm{b})}{1 - (2 \\alpha t / T_\\mathrm{b})^2}\n\\]\nSetting \\(\\alpha = 0\\) results in a sinc pulse in the time domain (with a perfect bandwidth containment in the frequency domain), while \\(\\alpha = 1\\) results in a pulse with double the Nyquist bandwidth. The pulse shape for \\(\\alpha = 0\\) and \\(\\alpha = 0.22\\) (used in 3G) is shown in Figure 14.\n\n\n\n\n\n\n\n\nFigure 14: Raised cosine pulse shaping in time and frequency domain for different roll-off factors α.\n\n\n\n\n\nAnother often-used pulse shape is the Gaussian pulse, which is used in Gaussian minimum-shift keying (GMSK, used in 2G) modulation, or in Gaussian frequency-shift keying (GFSK, used in Bluetooth). The Gaussian pulse has a smooth shape and a narrow spectrum. The Gaussian pulse is given by:\n\\[\np(t) = \\frac{\\sqrt{\\pi}}{\\alpha} e^{-(\\pi t / \\alpha)^2} \\quad \\text{with} \\quad \\alpha = \\frac{\\sqrt{\\ln 2}}{\\sqrt{2}} \\cdot \\frac{T_\\mathrm{b}}{B T_\\mathrm{b}}\n\\]\nwhere \\(B T_\\mathrm{b}\\) controls the width of the pulse. The spectrum of the Gaussian pulse is also Gaussian-shaped, which helps to minimize inter-symbol interference (ISI).\nThe Gaussian pulse for \\(BT=0.5\\) as used in Bluetooth is shown in Figure 15.\n\n\n\n\n\n\n\n\nFigure 15: Gaussian pulse shaping in time and frequency domain for different bandwidth-time products BT.\n\n\n\n\n\nFor both the raised-cosine and Gaussian pulse, the trade-off between time- and frequency-domain containment is clearly visible. This is also captured in “Küpfmüller’s uncertainty principle”, which states that the product of the time duration and the bandwidth of a pulse is lower-bounded by a constant. In other words, if we want to have a pulse that is very short in time, it will have a wide bandwidth, and vice versa."
  },
  {
    "objectID": "rfic.html#sec-fundamentals-ofdm",
    "href": "rfic.html#sec-fundamentals-ofdm",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.6 Orthogonal Frequency-Division Multiplexing (OFDM)",
    "text": "2.6 Orthogonal Frequency-Division Multiplexing (OFDM)\nAs we have seen in the previous section, if we make the symbol rate high, we need to use pulses with a wide bandwidth. The problem with a wide bandwidth in wireless communication is multi-path propagation, which causes frequency-selective fading. This means that some frequencies are attenuated more than others, which can cause errors in the received signal. Equalizing such a frequency-selective channel can be very complex, especially if the channel changes rapidly (as in mobile communication). We now face a dilemma: How can we achieve high data rates (which require high symbol rates and thus wide bandwidth) while avoiding frequency-selective fading? The key idea, implemented in OFDM, is to split the wideband channel into multiple narrowband sub-channels (subcarriers), each with a low symbol rate. This way, each subcarrier experiences flat fading, which is much easier to equalize.\nThe key question is now how to implement this idea efficiently, as we now have to apply modulation to hundreds or thousands of individual subcarriers. The solution is to use the inverse fast Fourier transform (IFFT) at the transmitter to generate the time-domain OFDM signal from the frequency-domain symbols, and the fast Fourier transform (FFT) at the receiver to recover the frequency-domain symbols from the time-domain OFDM signal. This is illustrated in Figure 16.\n\n\n\n\n\n\n\n\nFigure 16: OFDM transmission system block diagram showing transmitter and receiver processing chains.\n\n\n\n\n\nThe OFDM transmitter takes a block of \\(N\\) symbols (e.g., 64-QAM symbols) and maps them onto \\(N\\) subcarriers, thereby reducing the symbol rate for each subcarrier to \\(T_\\mathrm{b} / N\\). The IFFT then generates the time-domain OFDM signal, which is transmitted over the wireless channel. Before transmission the cyclic prefix (CP) is added to each OFDM symbol.\nAt the receiver, first the CP is removed, and then the FFT recovers the frequency-domain symbols, which can then be equalized (fairly simply by multiplying each subcarrier with a complex factor to correct amplitude and phase) and demodulated.\nA key property of OFDM is that the subcarriers are orthogonal to each other, which means that they do not interfere with each other, even if they overlap in frequency. This is achieved by choosing the subcarrier spacing \\(\\Delta f\\) such that it is equal to the symbol rate \\(1/T_\\mathrm{b}\\), i.e., \\(\\Delta f = 1/T_\\mathrm{b}\\). This way, the integral of the product of two different subcarriers over one symbol period is zero, which means that they are orthogonal.\nTo further improve the robustness against multi-path propagation, a CP is added to each OFDM symbol. The CP is a copy of the last part of the OFDM symbol, which is added to the beginning of the symbol. This way, if there are delayed copies of the OFDM symbol due to multi-path propagation, they will still fall within the CP and will not cause inter-symbol interference (ISI). The length of the CP should be longer than the maximum delay spread of the channel.\n\n\n\n\n\n\nNote 6: OFDM in LTE\n\n\n\nIn LTE OFDM is used for the downlink (base station to user equipment) with the following parameters:\n\nSubcarrier spacing: 15 kHz\nCP length: 5.2 µs (first symbol) resp. 4.7 µs (following symbols), and 16.67 µs as extended CP for scenarios with large delay spreads\nNumber of subcarriers: 1200 (for 20 MHz bandwidth)\nModulation: QPSK, 16-QAM, 64-QAM, 256-QAM\n\nFrom the subcarrier spacing we can calculate the symbol duration as \\(T_\\mathrm{b} = 1 / \\Delta f = 1 / 15\\,\\text{kHz} \\approx 66.7\\,\\mu\\text{s}\\).\nWe can calculate the raw bitrate for a 20 MHz LTE channel as\n\\[\n\\text{Bitrate} = N_\\mathrm{sc} \\cdot N_\\mathrm{sym} \\cdot \\frac{1}{T_\\mathrm{b} + T_\\mathrm{CP}} = 1200 \\cdot 8 \\cdot \\frac{1}{66.7\\,\\mu\\text{s} + 4.7\\,\\mu\\text{s}} \\approx 134\\,\\text{Mbps}\n\\]\nWithout the overhead for control channels and error correction coding a user datarate of approximately 100 Mbps can be achieved in a 20 MHz LTE channel."
  },
  {
    "objectID": "rfic.html#sec-fundamentals-multiple-access",
    "href": "rfic.html#sec-fundamentals-multiple-access",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.7 Multiple Access Techniques",
    "text": "2.7 Multiple Access Techniques\nIn wireless communication systems, multiple users need to share the same frequency spectrum. This is achieved by using multiple access techniques, which allow multiple users to transmit and receive data simultaneously without interfering with each other. The most common multiple access techniques are:\n\nTime division multiple access (TDMA): Users are assigned specific time slots for transmission, allowing multiple users to share the same frequency channel by dividing the time into slots.\nFrequency division multiple access (FDMA): Users are assigned specific frequency bands within the overall frequency spectrum, allowing multiple users to transmit simultaneously on different frequencies.\nCode division multiple access (CDMA): Users are assigned unique spreading codes, allowing them to transmit simultaneously over the same frequency band. The receiver uses the code to extract the desired signal. A variant of CDMA is frequency-hopping spread spectrum (FHSS), where the carrier frequency is changed rapidly according to a pseudo-random sequence known to both the transmitter and receiver. This is used in Bluetooth.\nOrthogonal frequency division multiple access (OFDMA): A variant of OFDM, where multiple users are assigned different subcarriers for transmission, allowing for efficient use of the frequency spectrum. This is used in 4G LTE and 5G NR.\nSpatial division multiple access (SDMA): Uses multiple antennas to create spatially separated channels, allowing multiple users to transmit simultaneously in the same frequency band.\n\nIn addition, all of these techniques can be combined to create more efficient and flexible communication systems. For example, OFDMA can be used in conjunction with SDMA to allow multiple users to share the same frequency resources while also taking advantage of spatial diversity. Also, TDMA can be combined with FDMA to create a hybrid multiple access scheme (which has been used in 2G GSM)."
  },
  {
    "objectID": "rfic.html#sec-fundamentals-mcs",
    "href": "rfic.html#sec-fundamentals-mcs",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "2.8 Modulation and Coding Schemes",
    "text": "2.8 Modulation and Coding Schemes\nIn modern wireless communication systems several parameters can be adapted to the current channel conditions to optimize the data rate and reliability. These parameters include the modulation format (e.g., QPSK, 16-QAM, 64-QAM, etc.) and the error correction coding scheme (e.g., convolutional codes, turbo codes, LDPC codes, etc.). The combination of modulation format and coding scheme is called the modulation and coding scheme (MCS). The MCS determines the number of bits per symbol and the level of error correction, which in turn affects the data rate and reliability of the communication link.\nSince there are many different MCS options available, and the receiver needs to be informed by the transmitter which MCS is used, a standardized set of MCS options is defined in most wireless communication standards. For example, the MCS options for WiFi can be seen in this table, while the MCS calculation for 5G NR can be found here.\n\n\n\n\n\n\nNoteExemplary MCS in WiFi\n\n\n\nSome operating systems and drivers show the MCS (and other parameters of the Wi-Fi link) which are currently used. A typical output might be:\n\nMCS: 7\nRSSI: -53 dBm\nInterference: -92 dBm\nBandwidth: 160 MHz\n\nLooking up MCS 7 in the WiFi MCS table linked above shows that this corresponds to 64-QAM with a coding rate of 5/6. With a bandwidth of 160 MHz, 2x MIMO and a guard interval (GI) of 0.8 µs, the maximum data rate is a whopping 1441 Mbps."
  },
  {
    "objectID": "rfic.html#sec-trx-direct-conversion",
    "href": "rfic.html#sec-trx-direct-conversion",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.1 Direct-Conversion Transceiver",
    "text": "3.1 Direct-Conversion Transceiver\nThe following typical functions have to be performed by a TRX:\n\nPulse-shaping filtering of the baseband signal (can be implemented analog or in most cases digital).\nModulating the baseband signal onto a carrier frequency (upconversion) in the TX or downconversion in the RX.\nContain the RF signal in a small bandwidth (TX), or single out the wanted signal in the RX.\nAdapt gain (and linearity) to the signal strength in the RX, and to the output power in the TX.\nGenerate the carrier frequency (local oscillator, LO) with low phase noise.\n\nThe dominant architecture for the TRX is the so-called direct-conversion (or Zero-IF) architecture, where the upconversion and downconversion is performed in a single step. This is in contrast to superheterodyne architectures, where the signal is first converted to an intermediate frequency (IF) before being converted to baseband. The direct-conversion architecture has the advantage of reduced complexity and cost, as it requires fewer components and less filtering. However, it also has some disadvantages, such as increased susceptibility to DC offsets and I/Q imbalance. A typical TRX block diagram is shown in Figure Figure 17.\n\n\n\n\n\n\n\n\nFigure 17: Block diagram of a typical transceiver (TRX) showing the main functional blocks of RX and TX. The modem provides the digital baseband processing and interfaces to the rest of the system. For implementation options of the RF front-end, see Section 3.5.\n\n\n\n\n\nAs can be seen in Figure 17, this generic example can be adapted in various ways. Generally, the amplifier gains are adjustable to adapt to different signal levels. If various channel bandwidths are to be supported, the corner frequencies of the low-pass filters (LPF) can be adjusted, as well as (optionally) the sampling rate of the ADCs and DACs. The local oscillator (LO) frequency is generated by a phase-locked loop (PLL) synthesizer, which can be tuned to the desired carrier frequency. In case of frequency-division duplex (FDD) operation, two PLLs are used to generate the TX and RX LO frequencies, which are separated by the duplex distance. In time-division duplex (TDD) operation, a single PLL is sufficient, supplying the LO signal to both RX and TX.\nThe modem that is shown in Figure 17 is responsible for the digital baseband processing, including functions like channel coding/decoding, modulation/demodulation, equalization, and error correction. The modem is usually implemented as a digital System-on-Chip (SoC) consisting of (multiple) CPUS, DSPs, and fixed-function blocks for time-critical processing. For an in-depth discussion we recommend (Sklar and Harris 2020) or (Molisch 2022)."
  },
  {
    "objectID": "rfic.html#sec-trx-mod-demod",
    "href": "rfic.html#sec-trx-mod-demod",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.2 Modulation and Demodulation",
    "text": "3.2 Modulation and Demodulation\nModulation is the process of varying a carrier signal at frequency \\(f_\\mathrm{c}\\) in order to transmit information. The complex baseband signal (after converting the real-valued digital \\(s_\\mathrm{I}\\) and \\(s_\\mathrm{Q}\\) signals to analog and pulse-shaping filtering) is represented as (\\(\\tilde{s}_\\mathrm{BB} \\in \\mathbb{C}\\); \\(s_\\mathrm{I}, s_\\mathrm{Q} \\in \\mathbb{R}\\)):\n\\[\n\\tilde{s}_\\mathrm{BB}(t) = s_\\mathrm{I}(t) + j \\cdot s_\\mathrm{Q}(t).\n\\]\nWe want to shift this signal to the carrier frequency \\(f_\\mathrm{c}\\), which can be done by multiplying with a complex exponential (\\(e^{j \\omega_\\mathrm{c} t}\\), with \\(\\omega_\\mathrm{c} = 2 \\pi f_\\mathrm{c}\\)):\n\\[\n\\tilde{s}_\\mathrm{RF}(t) = \\tilde{s}_\\mathrm{BB}(t) \\cdot e^{j \\omega_\\mathrm{c} t} = [s_\\mathrm{I}(t) + j \\cdot s_\\mathrm{Q}(t)] \\cdot [\\cos(\\omega_\\mathrm{c} t) + j \\cdot \\sin(\\omega_\\mathrm{c} t)].\n\\]\nThe real-valued RF signal is obtained by taking the real part of this expression (\\(s_\\mathrm{RF} \\in \\mathbb{R}\\), \\(\\tilde{s}_\\mathrm{RF} \\in \\mathbb{C}\\)):\n\\[\ns_\\mathrm{RF}(t) = \\Re \\{ \\tilde{s}_\\mathrm{RF}(t) \\} = s_\\mathrm{I}(t) \\cos(\\omega_\\mathrm{c} t) - s_\\mathrm{Q}(t) \\sin(\\omega_\\mathrm{c} t).\n\\tag{21}\\]\nThe process formulated in Equation 21 is done in the TX, as shown in Figure 18.\n\n\n\n\n\n\n\n\nFigure 18: TX modulator.\n\n\n\n\n\nThe RF signal generation according to Equation 21 is called quadrature modulation. This is the modulation used most often in modern communication systems, as it allows to transmit two independent signals (I and Q) in the same bandwidth. The I and Q signals are also called quadrature components, as they are 90° out of phase with each other.\nAlternatively, a modulation called polar modulation can be used, where the amplitude and phase of the carrier are varied according to the baseband signal. This is done by converting the I and Q signals to polar coordinates\n\\[\ns_\\mathrm{RF}(t) = \\Re \\{ A(t) \\cdot e^{j \\varphi(t)} \\cdot e^{j \\omega_\\mathrm{c} t} \\}\n\\]\nwith\n\\[\nA(t) = \\sqrt{s_\\mathrm{I}^2(t) + s_\\mathrm{Q}^2(t)}, \\quad \\varphi(t) = \\tan^{-1} \\left[ \\frac{s_\\mathrm{I}(t)}{s_\\mathrm{Q}(t)} \\right].\n\\]\nAs the mathematical operations required for the cartesian to polar transformation are quite nonlinear, the \\(A(t)\\) and \\(\\phi(t)\\) signals are wideband. Some wireless standards allow efficient use of polar modulation, for example Bluetooth, where basically all TX are realized as polar modulators.\nIn the RX, the received RF signal is downconverted to baseband by a similar process, as shown in Figure 19.\n\n\n\n\n\n\n\n\nFigure 19: RX demodulator.\n\n\n\n\n\nFor demodulation we have to shift the RF signal down to baseband, which mathematically is done by multiplying with the complex conjugate of the carrier (\\(e^{-j \\omega_\\mathrm{c} t}\\))\n\\[\n\\tilde{s}_\\mathrm{BB}(t) = s_\\mathrm{RF}(t) \\cdot e^{-j \\omega_\\mathrm{c} t} = s_\\mathrm{RF}(t) \\cdot [\\cos(\\omega_\\mathrm{c} t) - j \\cdot \\sin(\\omega_\\mathrm{c} t)]\n\\tag{22}\\]"
  },
  {
    "objectID": "rfic.html#filtering",
    "href": "rfic.html#filtering",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.3 Filtering",
    "text": "3.3 Filtering\nFiltering is an essential function in both TX and RX. In the TX, filtering is used to limit the bandwidth of the transmitted signal to the allocated channel bandwidth, and to suppress out-of-band emissions. In the RX, filtering is used to select the wanted signal from a crowded spectrum, and to suppress unwanted signals (blockers) that can cause interference or desensitization of the RX. A typical example of filtering in the RX is shown in Figure 20, where a bandpass filter is used to attenuate strong unwanted blockers while only slightly attenuating the wanted signal.\n\n\n\n\n\n\n\n\nFigure 20: Filtering of wanted channel amid strong unwanted blockers. Exemplary shown in an RX scenario around 900 MHz. The strong blockers (top figure) are attenuated by an RF bandpass filter (bottom figure) with a bandwidth of 20 MHz, achieving more than 40 dB rejection of the blockers while only slightly attenuating the wanted signal.\n\n\n\n\n\nIn any filter there exists a fundamental trade-off between selectivity (steepness of the filter skirts), bandwidth, and insertion loss. A very selective filter with steep skirts and large BW will have a high insertion loss. Conversely, a filter with low insertion loss will have a gentle roll-off and may not sufficiently suppress unwanted signals. A useful metric to quantify the performance of a filter is the quality factor \\(Q\\), defined as\n\\[\nQ = \\frac{f_\\mathrm{c}}{\\Delta f}\n\\]\nwhere \\(f_\\mathrm{c}\\) is the center frequency and \\(\\Delta f\\) is the –3 dB bandwidth of the filter. A higher \\(Q\\) indicates a more selective filter.\nThe achievable \\(Q\\) depends on the filter technology used. For example, on-chip LC filters can achieve \\(Q\\) values of around 10-20, while off-chip SAW or BAW/FBAR filters can achieve \\(Q\\) values of several hundreds, and a crystal filter can achieve \\(Q\\) values of several thousands. The choice of filter technology depends on the application requirements, such as frequency range, bandwidth, insertion loss, and cost. Generally speaking, the required filtering to single out the wanted signal in the RX spectrum and decrease the power of strong blockers to a tolerable level is one of the most critical design choices, and is usually distributed at different locations in the RX chain:\n\nRF filters (between antenna and LNA) provide a first level of filtering, and are usually implemented as off-chip SAW or BAW/FBAR filters. They provide high \\(Q\\) and good selectivity, but have a fixed center frequency and bandwidth. They are used to pass the wanted band of interest, and to attenuate strong out-of-band blockers.\nIF filters (in case of a super-heterodyne receiver) provide additional filtering, and can be implemented as on-chip LC filters or off-chip SAW/BAW filters. They provide moderate \\(Q\\) and selectivity, and can be tuned to some extent.\nBB filters (after downconversion) provide the final level of filtering before entering the ADCs, and are usually implemented as on-chip active RC filters. They provide channel selection, and can be easily adjusted to different bandwidths.\nDigital filters (in the DSP block) provide the final level of filtering and signal processing, and can be implemented as FIR or IIR filters. They provide high flexibility and can be easily adapted to different standards and requirements. Digital filters show now variations, so they can be designed to be very selective.\n\nIt is important to note (because this dictates a lot of choices in RF design) that high-\\(Q\\) filters are usually fixed-frequency and fixed-bandwidth. Only baseband and digital filters can be easily adjusted to different bandwidths!\n\n\n\n\n\n\nImportantFilter Technologies\n\n\n\nBaseband filters (analog) are usually implemented as active \\(RC\\) filters on-chip. They are very flexible and can have adjustable bandwidth by either changing \\(R\\) and/or \\(C\\). For medium frequencies \\(g_\\mathrm{m}-C\\) filters can be used, which are also tunable by changing the transconductance \\(g_\\mathrm{m}\\) and/or \\(C\\). For even higher bandwidths, on-chip LC filters can be used, which have a limited \\(Q\\) of around 10-20.\nBaseband filters (digital) are implemented as FIR or IIR filters in the DSP block. They are very flexible and can be easily adapted to different standards and requirements. Digital filters show no variations, so they can be designed to be very selective.\nSurface acoustic wave (SAW) and bulk acoustic wave (BAW/FBAR) filters are off-chip components that can achieve high \\(Q\\) values of several hundreds. They have a fixed center frequency and bandwidth. Usually 1-2 such filters are required per supported band of interest.\nCrystal filters can achieve very high \\(Q\\) values of several thousands, but are usually bulky and expensive.\nLC filters can be either implemented off-chip (using discrete components) or on-chip. Off-chip LC filters can achieve higher \\(Q\\) values than on-chip LC filters, but are usually larger and more expensive. On-chip LC filters are limited in \\(Q\\) (around 10-20), but are very compact and can be integrated into the RFIC. Off-chip LC filters can achieve \\(Q\\) values of around 50-100, depending on the frequency and component quality.\nCeramic filters are another off-chip filter technology that can achieve moderate to high \\(Q\\) values (up to several hundreds). They are usually smaller and less expensive than SAW or BAW/FBAR filters, but also lower performance.\nWaveguide filters are used at very high frequencies (above 10 GHz) and can achieve very high \\(Q\\) values (up to several thousands). They are usually bulky and expensive, and are not commonly used in mobile applications, but rather in fixed installations like base stations or satellite communication.\n\n\nFundamentally, the choice of filter technology is a trade-off between performance, size, cost, and flexibility. In most cases, a combination of different filter technologies is used to achieve the desired performance."
  },
  {
    "objectID": "rfic.html#direct-conversion-architecture",
    "href": "rfic.html#direct-conversion-architecture",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.4 Direct-Conversion Architecture",
    "text": "3.4 Direct-Conversion Architecture\nThe transceiver architecture shown in Figure 17 is called direct-conversion or zero-IF architecture, as the downconversion in the RX and upconversion in the TX is done in a single step. This architecture has several advantages:\n\nPer RX and TX a single LO is required (which can even be shared between RX and TX in TDD operation).\nThere are a minimum number of RF blocks, which is good for cost and power consumption.\nThis architecture is very flexible and can be easily adapted to different standards and requirements, and generally shows very good performance if the disadvantages can be overcome by good design.\nThis architecture allows a high integration level, as basically all blocks can be implemented on-chip.\nDirect conversion is the de-facto standard architecture for cellular, WiFi, Bluetooth (with the exception of the TX), and GNSS.\n\nHowever, the direct-conversion architecture also has some disadvantages:\n\nLO-RF coupling can cause self-mixing and desensitization of the RX, as well as LO leakage in the TX. This is an issue because the LO frequency is the same as the RF frequency.\nEven-order distortion products (especially IIP2) cause sensitivity degradation due to strong amplitude-modulated blockers.\nLO pulling can occur in the TX (again, LO and RF are at the same frequency).\nIQ errors (gain and phase mismatch) of the \\(I\\) and \\(Q\\) paths can cause constellation distortion leading to increased error vector magnitude (EVM).\nDC offsets can occur due to self-mixing of LO leakage and even-order distortion products.\nFlicker noise (1/f noise) upconversion can cause increased phase noise close to the carrier, as well as increased RX noise figure.\n\nNowadays there exist good design techniques to mitigate these disadvantages. However, in some cases (for example very high linearity requirements, or very high frequencies) other architectures like low-IF or super-heterodyne may be preferred."
  },
  {
    "objectID": "rfic.html#sec-trx-duplexing",
    "href": "rfic.html#sec-trx-duplexing",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.5 Duplexing",
    "text": "3.5 Duplexing\nIn the block diagram of Figure 17, we have not yet considered how to share the antenna between RX and TX. Essentially, there are two main methods to achieve this: frequency-division duplex (FDD) and time-division duplex (TDD).\n\n3.5.1 Frequency-Division Duplex (FDD)\nIn FDD, the RX and TX operate at different frequencies, separated by a duplex distance. This allows simultaneous transmission and reception, which is beneficial for applications like voice communication where low latency is required. However, FDD requires two separate frequency bands, which can be a limitation in terms of spectrum availability. Additionally, FDD requires two PLLs to generate the RX and TX LO frequencies, which increases complexity and power consumption.\nThe RF RX and TX paths are connected to the antenna via a duplexer, which is a three-port device that allows signals to pass between the antenna and the RX or TX path, while isolating the RX and TX paths from each other. A typical FDD TRX block diagram is shown in Figure 21.\n\n\n\n\n\n\n\n\nFigure 21: Block diagram of an FDD RF front-end.\n\n\n\n\n\nAdvantages of FDD:\n\nRX and TX can operate simultaneously, which is beneficial for low-latency applications.\nThere is no need for fast switching between RX and TX, which simplifies the design.\nRelaxed synchronization requirements between RX and TX and different users.\n\nDisadvantages of FDD:\n\nDuplexers are costly components with significant insertion loss depending on filtering requirements.\nRequires two separate frequency bands, which can be a limitation in terms of spectrum availability, and MIMO channel estimation.\nThe strong TX causes severe desensitization of the RX, which requires high linearity and good filtering (50 dB to 60 dB).\n\n\n\n3.5.2 Time-Division Duplex (TDD)\nIn TDD, the RX and TX share the same frequency band but operate at different times. This allows for more efficient use of the available spectrum, as the same frequency can be used for both transmission and reception. TDD is particularly well-suited for applications with asymmetric traffic patterns, where the data rate in one direction is significantly higher than in the other. However, TDD requires precise timing control to avoid interference between RX and TX periods, which can increase complexity. In TDD, a single PLL can be used to generate the LO frequency for both RX and TX, which reduces complexity and power consumption. The RF RX and TX paths are connected to the antenna via a switch, which alternates between connecting the antenna to the RX path and the TX path. A typical TDD TRX block diagram is shown in Figure 22.\n\n\n\n\n\n\n\n\nFigure 22: Block diagram of a TDD RF front-end.\n\n\n\n\n\nAdvantages of TDD:\n\nMore efficient use of the available spectrum, as the same frequency can be used for both RX and TX.\nA single PLL can be used for both RX and TX, which reduces complexity and power consumption.\nNo duplexer is required (just a single band filter), which reduces cost and insertion loss.\nNo RX blocking by own TX, which relaxes linearity and filtering requirements.\nEasier to implement MIMO, as all antennas can operate in the same frequency band.\n\nDisadvantages of TDD:\n\nRX and TX cannot operate simultaneously, which can be a limitation for low-latency applications.\nRequires precise timing control to avoid interference between RX and TX periods, which can increase complexity.\nSynchronization between RX and TX and different users is required, which can be challenging in some scenarios.\n\n\n\n3.5.3 Comparison of FDD and TDD\nBelow is a summary of important wireless standards and their duplexing method as shown in Table 4:\n\n\n\nTable 4: Comparison of duplexing methods used by major wireless standards\n\n\n\n\n\n\n\n\n\n\nWireless Standard\nDuplexing Method\nComments\n\n\n\n\nGSM (2G)\nFDD & TDMA\nTX and RX operate at different frequencies (FDD) and different times (TDMA)\n\n\nUMTS (3G)\nFDD\nTraditional cellular standard using paired spectrum\n\n\nLTE (4G)\nFDD/TDD\nFDD is used mostly &lt;2.7 GHz, TDD is used &gt;2.3 GHz\n\n\n5G NR\nFDD/TDD\nFDD is used mostly &lt;2.7 GHz, TDD is used &gt;2.3 GHz\n\n\nWiFi (802.11)\nTDD\nUnlicensed spectrum operation\n\n\nBluetooth\nTDD\nShort-range personal area network\n\n\nZigbee\nTDD\nLow-power IoT applications\n\n\n\n\n\n\nAs you can see in Table 4, there is a tendency to use FDD for lower frequencies and long communication distances, while TDD is preferred for higher frequencies and shorter distances."
  },
  {
    "objectID": "rfic.html#specialty-architectures",
    "href": "rfic.html#specialty-architectures",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.6 Specialty Architectures",
    "text": "3.6 Specialty Architectures\nIn some cases, other architectures may be preferred over the direct-conversion architecture. During the evolution of wireless communication, many different architectures have been proposed and used. However, only a few of them are still relevant today. Some examples are shown next.\n\n3.6.1 Super-Heterodyne Architecture\nThe super-heterodyne architecture is a widely used approach in radio. It works by mixing the incoming/outgoing RF signal with an LO to produce an intermediate frequency (IF) signal. This IF signal is then amplified and processed, allowing for better selectivity and sensitivity compared to direct-conversion architectures. Super-heterodyne receivers/transmitters are known for their excellent performance in terms of image rejection and dynamic range, making them suitable for a variety of applications, including traditional analog TV and radio broadcasting. A simplified block diagram of a super-heterodyne transceiver is shown in Figure 23.\n\n\n\n\n\n\n\n\nFigure 23: Block diagram of a super-heterodyne transceiver (TRX) showing the main functional blocks of RX and TX.\n\n\n\n\n\nWhen you compare Figure 17 with Figure 23, you can immediately appreciate the increased complexity of the super-heterodyne architecture. It requires two PLLs to generate the RX and TX LO frequencies, as well as additional mixers and filters for the IF stage. This increases cost, power consumption, and size. However, the super-heterodyne architecture can provide better performance in terms of selectivity and sensitivity, especially in challenging RF environments with strong blockers, as it allows filtering at RF, IF, and baseband frequencies.\nOne important aspect of super-heterodyne receivers is the choice of the intermediate frequency (IF). The IF should be high enough to allow for effective filtering and image rejection, but low enough to avoid excessive complexity and power consumption. Common IF frequencies range from a few MHz to several hundred MHz, depending on the application and frequency band.\nAn important issue in super-heterodyne receivers is the image frequency. The image frequency is a spurious frequency that can interfere with the desired signal, and is located at \\(f_\\mathrm{image} = f_\\mathrm{RF} \\pm 2 f_\\mathrm{IF}\\) (the sign depends on the choice of high-side or low-side mixing). To suppress the image frequency, an image-reject filter is either placed before (RX) or after (TX) the mixer. The design of this filter is critical, as it must provide sufficient attenuation of the image frequency while maintaining low insertion loss for the desired signal.\nAn alternative to image filtering is the use of active image rejection techniques, such as the Hartley or Weaver architectures. These techniques use additional mixers and phase shifters to cancel out the image frequency, allowing for improved performance without the need for a dedicated image-reject filter.\n\n\n3.6.2 Low-IF Architecture\nTo avoid some of the issues of direct-conversion architectures (like dc offsets and flicker noise), a low-IF architecture can be used. In a low-IF architecture, the RX and TX signals are mixed to a low intermediate frequency (typically a few MHz to tens of MHz) instead of directly to baseband. This allows for easier filtering of DC offsets and flicker noise, while still maintaining the benefits of a single LO and reduced complexity compared to super-heterodyne architectures. A low-IF architecture is shown in Figure 24.\n\n\n\n\n\n\n\n\nFigure 24: Block diagram of a low-IF transceiver (TRX) showing the main functional blocks of RX and TX. Note the usage of complex analog and digital baseband filters. Otherwise, the structure is similar to a zero-IF TRX as shown in Figure 17.\n\n\n\n\n\nThe low-IF architecture is the de facto standard for Bluetooth receivers. Its advantage compared to direct-conversion vanishes for larger channel bandwidths, which is why it is not used for cellular or WiFi (GSM receivers might be an exception).\nOne noteworthy disadvantage of low-IF architectures is the required 2xBW compared to direct-conversion. This might cause increased power consumption in the analog baseband filters and ADCs/DACs. Additionally, the low-IF architecture still requires careful design to mitigate issues like IQ imbalance and LO leakage, although these issues are generally less severe than in direct-conversion architectures.\n\n\n3.6.3 Super Simple Architecture\nFor some applications with very low cost and low performance requirements, a super simple architecture can be used (think garage door opener). In this architecture, the RX and TX paths are stripped down to the bare minimum. A super simple receiver just uses a bandpass filter and an envelope detector, while a super simple transmitter uses an oscillator and power amplifier. These simplified architectures are shown in Figure 25.\n\n\n\n\n\n\n\n\nFigure 25: Block diagram of a super simple TX and RX.\n\n\n\n\n\nDespite the simple architecture, digital amplitude-shift-keying (ASK) or on-off-keying (OOK) can be used. If the receiver is able to discriminate between frequencies (e.g., by using two RF filters with an envelope detector each), also frequency-shift-keying (FSK) can be used."
  },
  {
    "objectID": "rfic.html#iq-imbalance",
    "href": "rfic.html#iq-imbalance",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "3.7 I/Q Imbalance",
    "text": "3.7 I/Q Imbalance\nIn direct-conversion and low-IF architectures, the I and Q paths are used to process the in-phase and quadrature components of the signal. Ideally, these paths should have identical gain and a 90° phase difference. However, in practice, there are always some mismatches between the I and Q paths, leading to I/Q imbalance. This imbalance can cause constellation distortion, leading to increased error vector magnitude (EVM) and degraded system performance.\nI/Q imbalance can be characterized by two parameters: gain mismatch (\\(\\Delta G\\)) and phase mismatch (\\(\\Delta \\varphi\\)). Gain mismatch refers to the difference in gain between the I and Q paths, while phase mismatch refers to the deviation from the ideal 90° phase difference. The impact of I/Q imbalance on system performance depends on the modulation scheme used, with higher-order modulations being more sensitive to these impairments.\nThere are two ways to quantify I/Q imbalance:\n\nImage rejection ratio (IRR): The IRR is a measure of how well the receiver can reject the image frequency caused by I/Q imbalance. It is defined as the ratio of the power of the desired signal to the power of the image (unwanted) signal, typically expressed in dB. A higher IRR indicates better performance, with values above 30 dB to 40 dB generally considered acceptable for most applications.\nError vector magnitude (EVM): The EVM is a measure of the difference between the ideal transmitted signal and the received signal, expressed as a percentage of the signal’s magnitude. It quantifies the overall distortion in the received signal, including the effects of I/Q imbalance. Lower EVM values indicate better performance, with typical requirements ranging from 1% to 10% depending on the modulation scheme and application.\n\nThe EVM (in rms) is defined as\n\\[\n\\text{EVM} = \\frac{\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} |s_\\mathrm{ideal}(i) - s_\\mathrm{meas}(i)|^2}}{\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} |s_\\mathrm{ideal}(i)|^2}}\n\\tag{23}\\]\nwhere \\(s_\\mathrm{ideal}(i)\\) is the ideal transmitted symbol, \\(s_\\mathrm{meas}(i)\\) is the measured received symbol, and \\(N\\) is the number of symbols. EVM is expressed either in percent or in dB using\n\\[\n\\text{EVM}|_\\mathrm{dB} = 20 \\cdot \\log_{10}(\\text{EVM}).\n\\]\nIn order to make the I/Q mismatch sufficiently small, among the possible techniques are:\n\nCareful layout and matching of the components in the I and Q paths to minimize gain and phase mismatches. This usually involves good layout techniques. Further, the LO I/Q generation should be done with high accuracy.\nCalibration techniques can be used to measure and compensate for I/Q imbalance. This can be done either in the analog domain (e.g., using variable gain amplifiers and phase shifters) or in the digital domain (e.g., using digital signal processing algorithms). Digital compensation is usually preferred, as it is more flexible and can adapt to changing conditions. A CORDIC can be readily used for this purpose."
  },
  {
    "objectID": "rfic.html#sec-lna-resistive-matching",
    "href": "rfic.html#sec-lna-resistive-matching",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "4.1 Resistively Matched Common-Source LNA",
    "text": "4.1 Resistively Matched Common-Source LNA\nThe key question is now how to design an LNA with low noise figure and an input impedance matched to 50 Ω? In order to appreciate this design challenge, we will first try a naive approach, using a common-source amplifier with resistive termination, as shown in Figure 28.\n\n\n\n\n\n\n\n\nFigure 28: A simple LNA with resistive input matching and a tank circuit as a load (biasing details are omitted). The LNA is driven by a 50 Ω source.\n\n\n\n\n\nIf we assume the gate capacitance of \\(M_1\\) is negligible, we can achieve good input impedance matching by choosing \\(R_\\mathrm{s} = R_\\mathrm{p} = 50\\,\\Omega\\). The voltage gain of this simple common-source LNA is given by \\(A_\\mathrm{v} = -g_\\mathrm{m}R_\\mathrm{D}\\), neglecting capacitances and \\(g_\\mathrm{ds}\\) of \\(M_1\\) (we assume that the load tank is tuned to the desired frequency with \\(\\omega_0 = 1 / \\sqrt{L C}\\)).\nHow can we calculate the noise figure of this simple LNA? We formulate\n\\[\nF = \\frac{\\text{total noise at output}}{\\text{noise at output due to source only}}.\n\\tag{24}\\]\nWe derive a small-signal equivalent circuit of Figure 28, which is shown in Figure 29, to calculate the total noise at the output of the LNA.\n\n\n\n\n\n\n\n\nFigure 29: Equivalent circuit of resistively matched common-source LNA.\n\n\n\n\n\nWith the help of Figure 29, we can calculate the total output noise power spectral density as\n\\[\n\\overline{V_\\mathrm{n,out,1}^2} = A_\\mathrm{v}^2 \\cdot 4 k T (R_\\mathrm{s} \\parallel R_\\mathrm{p}) = (g_\\mathrm{m}R_\\mathrm{D})^2 \\cdot 4 k T (R_\\mathrm{s} \\parallel R_\\mathrm{p})\n\\]\nand\n\\[\n\\overline{I_\\mathrm{n,out}^2} = 4 k T \\gamma g_\\mathrm{m}+ \\frac{4 k T}{R_\\mathrm{D}}\n\\] \\[\n\\overline{V_\\mathrm{n,out,2}^2} = R_\\mathrm{D}^2 \\cdot \\overline{I_\\mathrm{n,out}^2} = 4 k T \\gamma g_\\mathrm{m}R_\\mathrm{D}^2 + 4 k T R_\\mathrm{D}\n\\]\nso that in total\n\\[\n\\overline{V_\\mathrm{n,out}^2} = \\overline{V_\\mathrm{n,out,1}^2} + \\overline{V_\\mathrm{n,out,2}^2} = 4 k T \\left[ (g_\\mathrm{m}R_\\mathrm{D})^2 (R_\\mathrm{s} \\parallel R_\\mathrm{p}) + \\gamma g_\\mathrm{m}R_\\mathrm{D}^2 + R_\\mathrm{D} \\right].\n\\tag{25}\\]\nWe now need to find the output noise coming from the source only. For this we can use the equivalent circuit in Figure 30, to formulate the output noise due to the source only.\n\n\n\n\n\n\n\n\nFigure 30: Eqivalent circuit to calculate the output noise from the input.\n\n\n\n\n\nWe find that\n\\[\n\\overline{V_\\mathrm{n,out,s}^2} = A_\\mathrm{v}^2 \\cdot 4 k T R_\\mathrm{s} \\cdot \\left(\\frac{R_\\mathrm{p}}{R_\\mathrm{s} + R_\\mathrm{p}} \\right)^2.\n\\tag{26}\\]\nFinally, we can use Equation 25 and Equation 26 with Equation 24 to calculate the noise figure of the simple resistively matched common-source LNA as\n\\[\nF = \\frac{\\overline{V_\\mathrm{n,out}^2}}{\\overline{V_\\mathrm{n,out,s}^2}} = 1 + \\frac{R_\\mathrm{s}}{R_\\mathrm{p}} + \\frac{\\gamma R_\\mathrm{s}}{g_\\mathrm{m}(R_\\mathrm{s} \\parallel R_\\mathrm{p})^2} + \\frac{R_\\mathrm{s}}{g_\\mathrm{m}^2 (R_\\mathrm{s} \\parallel R_\\mathrm{p})^2 R_\\mathrm{D}}.\n\\tag{27}\\]\n\n\n\n\n\n\nNoteCommon-source LNA with Resistive Matching\n\n\n\nAs an exercise to calculate circuits with noise, re-confirm and derive yourself the result of Equation 27.\n\n\nHow can we interpret Equation 27? We see that we can minimize the noise factor by making \\(g_\\mathrm{m}\\) large. Then we have a noise factor of\n\\[\nF = 1 + \\frac{R_\\mathrm{s}}{R_\\mathrm{p}} = 2\n\\]\nso we see that we are limited to a minimum noise figure of 3 dB, even if we spend the bias current to make \\(g_\\mathrm{m}\\) very large. We can go below 3 dB noise figure only if we choose \\(R_\\mathrm{p} &gt; R_\\mathrm{s}\\), however, this means that the input is no longer matched to 50 Ω, which is usually not acceptable. Hence, this simple resistively matched common-source LNA is not a good choice for a low-noise amplifier, with one exception: For very wideband amplifiers, where a NF of larger than 3 dB is acceptable, this configuration might be a good choice.\nWe see that we are stuck at high noise figures if we realize the real part of the input impedance with a resistor. This leaves us with the question of how to realize a real part of the input impedance then. We will answer this question in the next section."
  },
  {
    "objectID": "rfic.html#sec-lna-common-gate",
    "href": "rfic.html#sec-lna-common-gate",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "4.2 Common-Gate LNA",
    "text": "4.2 Common-Gate LNA\nWe remember from our analog circuit design lecture that the common-gate configuration has an input impedance of \\(1/g_\\mathrm{m}\\), neglecting parasitic capacitances. Hence, if we choose \\(g_\\mathrm{m}= 1/50\\,\\Omega = 20\\,\\text{mS}\\), we can achieve input matching to 50 Ω without using a resistor at the input. This is the key idea of the common-gate LNA, which is shown in Figure 31.\n\n\n\n\n\n\n\n\nFigure 31: Circuit diagram of a common-gate LNA.\n\n\n\n\n\nBy inspecting Figure 31 and following the practice from Section 4.1, we can directly write down the output noise voltage as (with \\(1/g_\\mathrm{m}= R_\\mathrm{s}\\))\n\\[\n\\overline{V_\\mathrm{n,out}^2} = k T \\left[ (g_\\mathrm{m}R_\\mathrm{D})^2 R_\\mathrm{s} + \\gamma g_\\mathrm{m}R_\\mathrm{D}^2 + 4 R_\\mathrm{D} \\right] = k T \\left( \\frac{R_\\mathrm{D}^2}{R_\\mathrm{s}} + \\gamma \\frac{R_\\mathrm{D}^2}{R_\\mathrm{s}} + 4 R_\\mathrm{D} \\right).\n\\tag{28}\\]\nThe output noise due to the source only is given by\n\\[\n\\overline{V_\\mathrm{n,out,s}^2} = \\frac{R_\\mathrm{D}^2}{R_\\mathrm{s}}.\n\\tag{29}\\]\nFinally, we can use Equation 28 and Equation 29 with Equation 24 to calculate the noise figure of the common-gate LNA as\n\\[\nF = 1 + \\gamma + \\frac{4 R_\\mathrm{s}}{R_\\mathrm{D}}  \\xrightarrow{R_\\mathrm{D} \\gg R_\\mathrm{s}} F = 1 + \\gamma .\n\\tag{30}\\]\nWith a classical long-channel \\(\\gamma = 2/3\\), we can achieve a minimum noise figure of 2.2 dB, which is already better than the resistively matched common-source LNA. However, with modern short-channel devices, \\(\\gamma\\) is often larger than 1, so that the minimum noise figure of the common-gate LNA is often larger than 3 dB (Behzad Razavi 2011)."
  },
  {
    "objectID": "rfic.html#sec-lna-inductive-degeneration",
    "href": "rfic.html#sec-lna-inductive-degeneration",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "4.3 Inductively-Degenerated Common-Source LNA",
    "text": "4.3 Inductively-Degenerated Common-Source LNA\nAs we have seen in Section 4.2, using circuit techniques can realize a real part of an input impedance without the associated thermal noise of a resistor. We now try something different, in the hope that it will result in an even lower noise figure. We construct an LNA based on a common-source MOSFET amplifier, but we add an impedance \\(Z_\\mathrm{deg}\\) into the source line. This arrangement is shown in Figure 32.\n\n\n\n\n\n\n\n\nFigure 32: A common-source MOSFET stage with degeneration impedance.\n\n\n\n\n\nWe now extract the small-signal equivalent circuit of Figure 32, which is shown in Figure 33, to calculate the input impedance.\n\n\n\n\n\n\n\n\nFigure 33: Equivalent small-signal circuit of the input stage around \\(M_1\\).\n\n\n\n\n\nWe find that\n\\[\nV_\\mathrm{x} = V_\\mathrm{gs}+ Z_\\mathrm{deg} (I_\\mathrm{x} + g_\\mathrm{m}V_\\mathrm{gs}), \\quad V_\\mathrm{gs}= \\frac{I_\\mathrm{x}}{s C_\\mathrm{gs}}\n\\]\nso that we can write the input impedance as\n\\[\nZ_\\mathrm{in} = \\frac{V_\\mathrm{x}}{I_\\mathrm{x}} = \\frac{1}{s C_\\mathrm{gs}} + Z_\\mathrm{deg} + \\frac{g_\\mathrm{m}Z_\\mathrm{deg}}{s C_\\mathrm{gs}}.\n\\tag{31}\\]\nThe final term in Equation 31 is the interesting one: By choosing \\(Z_\\mathrm{deg}\\) to be inductive (which we can do by either use an on-chip or off-chip inductor), we can realize a real part of the input impedance. If we choose \\(Z_\\mathrm{deg} = s L\\), we find that\n\\[\nZ_\\mathrm{in} = \\frac{1}{s C_\\mathrm{gs}} + s L + \\frac{g_\\mathrm{m}L}{C_\\mathrm{gs}}.\n\\]\nBy proper choice of \\(L\\) and \\(C_\\mathrm{gs}\\), we can achieve input matching to 50 Ω at the desired frequency \\(\\omega_0\\). We find that the real part of the input impedance is given by\n\\[\n\\Re \\{ Z_\\mathrm{in} \\} = \\frac{g_\\mathrm{m}L}{C_\\mathrm{gs}}\n\\]\nWithout proof (refer to (Behzad Razavi 2011) or (Darabi 2020) for a derivation) we find for the noise factor of this input stage (with some simplification) as\n\\[\nF = 1 + \\frac{\\gamma R_\\mathrm{s} \\omega_0^2 C_\\mathrm{gs}^2}{g_\\mathrm{m}}.\n\\tag{32}\\]\nFinally, we have an LNA input stage configuration that allows us to achieve a noise figure below 3 dB, even with \\(\\gamma &gt; 1\\), by proper choice of \\(g_\\mathrm{m}\\). Making \\(g_\\mathrm{m}\\) large (by spending more bias current) results in (to first order) arbitrarily low noise figure. The inductively-degenerated common-source LNA is a widely used LNA input stage configuration in modern RFICs. A somewhat detailed schematic is shown in Figure 34.\n\n\n\n\n\n\n\n\nFigure 34: An (almost complete) common-source MOSFET stage with degeneration impedance and cascode.\n\n\n\n\n\nThe inductor \\(L_\\mathrm{match}\\) is used to match the input impedance to 50 Ω at the desired frequency, \\(L_\\mathrm{deg}\\) is used to realize the real part of the input impedance, \\(R_\\mathrm{bias}\\) is used to set the bias current of \\(M_1\\), \\(M_2\\) is a cascode transistor which increases the output impedance and thus the gain of the stage (plus it improves the reverse isolation), and \\(R_\\mathrm{D}\\), \\(L_\\mathrm{D}\\), and \\(C_\\mathrm{D}\\) form a load tank which provides high gain at the desired frequency. A dc block is used at the input so that the bias point of \\(M_1\\) is not corrupted by the input signal source. The bias voltage \\(V_\\mathrm{bias2}\\) sets the operating point of the cascode transistor \\(M_2\\).\nWhat is missing in Figure 34 is any form of frequency tuning of the load to the frequency of interest, and the support of different gain modes. Apart from these details this LNA circuit is a good starting point for a practical LNA design."
  },
  {
    "objectID": "rfic.html#sec-lna-feedback",
    "href": "rfic.html#sec-lna-feedback",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "4.4 Feedback LNA",
    "text": "4.4 Feedback LNA\nOne drawback of the inductively-degenerated common-source LNA is the usage of at least one inductor. If the inductor is placed on-chip, it has a comparatively large size, and if it is implemented in the package (via a bondwire) or on the PCB, it adds to the bill-of-materials (BOM) cost.\nIf the CMOS technology is sufficiently fast, a shunt feedback LNA, as shown in Figure 35, might be a good choice.\n\n\n\n\n\n\n\n\nFigure 35: A shunt-feedback LNA.\n\n\n\n\n\nWithout proof, the input impedance of the shunt feedback LNA is given by\n\\[\nZ_\\mathrm{in} = \\frac{Z_\\mathrm{F} + Z_\\mathrm{L}}{1 + g_\\mathrm{m}Z_\\mathrm{L}}.\n\\tag{33}\\]\nThe noise factor of the shunt feedback LNA is given by\n\\[\nF = 1 + \\left| \\frac{Z_\\mathrm{F} + R_\\mathrm{s}}{g_\\mathrm{m}Z_\\mathrm{F} + 1} \\right|^2 \\cdot \\frac{\\gamma g_\\mathrm{m}+ \\Re \\{ Y_\\mathrm{L} \\} }{\\Re \\{ Z_\\mathrm{in} \\}}\n\\tag{34}\\]\nAs you can see from Equation 34, by making \\(g_\\mathrm{m}\\) large (by spending more bias current) the noise figure can be made arbitrarily small! Depending on the choice of \\(Z_\\mathrm{F}\\) and \\(Z_\\mathrm{L}\\), the input impedance of this LNA can be changed in interesting ways.\nBy setting \\(Z_\\mathrm{L} \\rightarrow \\infty\\) (e.g., by biasing with a current source and high-impedance loading), we find that\n\\[\nZ_\\mathrm{in} = \\frac{1}{g_\\mathrm{m}}\n\\]\nwhich is independent of \\(Z_\\mathrm{F}\\) and is a well-known result for a common-source stage. The disadvantage of this configuration is the noise factor, which (given that \\(Z_\\mathrm{F}\\) is sufficiently large) tends to \\(F = 1 + \\gamma\\), which is the same as for the common-gate LNA.\nA bit more interesting is the case when \\(g_\\mathrm{m}Z_\\mathrm{L} = A_0\\) and \\(Z_\\mathrm{L} \\gg Z_\\mathrm{F}\\), which results in\n\\[\nZ_\\mathrm{in} = \\frac{Z_\\mathrm{L}}{1 + A_0}\n\\]\nwhich is the well-known result that the input impedance of an amplifier with feedback is reduced by the factor \\(1 + A_0\\), where \\(A_0\\) is the open-loop gain of the amplifier. The noise factor can be made small by making \\(g_\\mathrm{m}\\) large, as we have already noted above.\nA very interesting case can be achieved by choosing \\(Z_\\mathrm{L} = 1 / s C_\\mathrm{L}\\) and \\(Z_\\mathrm{F} = 1 / s C_\\mathrm{F}\\), which results in\n\\[\nY_\\mathrm{in} = \\frac{1}{Z_\\mathrm{in}} = \\frac{g_\\mathrm{m}C_\\mathrm{F}}{C_\\mathrm{L} + C_\\mathrm{F}} + s \\frac{C_\\mathrm{L} C_\\mathrm{F}}{C_\\mathrm{L} + C_\\mathrm{F}}\n\\tag{35}\\]\nLooking at Equation 35, we see that the input admittance has a real part! By proper choice of components, we can achieve an input impedance matched to 50 Ω at the desired frequency. The noise factor can again be made small by making \\(g_\\mathrm{m}\\) large.\nThere is also an option, by proper choice of \\(Z_\\mathrm{F}\\) and \\(Z_\\mathrm{L}\\), to achieve an inductive input impedance component, which can be used to resonate out the input capacitance of the LNA, similar to the inductively-degenerated common-source LNA. However, in contrast to the inductively-degenerated common-source LNA, no inductor is required in this case. This configuration is called a reactance-canceling LNA."
  },
  {
    "objectID": "rfic.html#sec-mixer-nonlinear",
    "href": "rfic.html#sec-mixer-nonlinear",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "5.1 Non-Linear Mixer",
    "text": "5.1 Non-Linear Mixer\nGenerally speaking, we can use any non-linear device to implement a mixer (often, this generation of new frequency components in a nonlinear system is highly undesired; in a mixer, we want this effect). As a simple example we look at the case of a square function, shown in Figure 37.\n\n\n\n\n\n\n\n\nFigure 37: A squarer as a nonlinear mixer.\n\n\n\n\n\nWhen we apply the sum of the signals \\(s_\\mathrm{in}(t) = \\cos( \\omega_\\mathrm{in} t)\\) and \\(s_\\mathrm{LO}(t) = \\cos(\\omega_\\mathrm{LO} t)\\) to the input of a squarer, we get the output signal\n\\[\n\\begin{split}\ns_\\mathrm{out}(t) = [ s_\\mathrm{in}(t) + s_\\mathrm{LO}(t) ]^2 &= \\cos(\\omega_\\mathrm{in} t + \\omega_\\mathrm{LO} t) + \\cos(\\omega_\\mathrm{in} t - \\omega_\\mathrm{LO} t) \\\\\n&+ 1 + \\frac{1}{2} \\cos(2 \\omega_\\mathrm{in} t) + \\frac{1}{2} \\cos(2 \\omega_\\mathrm{LO} t).\n\\end{split}\n\\tag{36}\\]\nWe see that the output signal contains the desired frequency components at \\(\\omega_\\mathrm{in} \\pm \\omega_\\mathrm{LO}\\), but also additional components at dc (0 Hz), \\(2 \\omega_\\mathrm{in}\\), and \\(2 \\omega_\\mathrm{LO}\\). These additional components are usually unwanted and have to be filtered out in a subsequent filtering stage. As a side-note, in some circuits a frequency doubler is desired, which can be implemented by simply filtering out the other frequency components. In a doubler no LO signal is required, as the input signal is simply squared.\nA simple example circuit is a diode, which has a non-linear current-voltage characteristic. The block diagram of a simple diode mixer is shown in Figure 38.\n\n\n\n\n\n\n\n\nFigure 38: A diode mixer.\n\n\n\n\n\nThis structure is conceptionally simple, it just requires a (fast) diode, filters to couple the desired frequency components in and out, and an RF choke to provide a dc bias voltage for the diode. However, the performance of such a simple mixer is usually not very good, as the diode is a highly non-linear device, which generates many spurious frequency components. More advanced mixer circuits use more complex non-linear devices (for example, a ring of diodes) to improve the performance. The advantage of these mixers is that they can operate up to very high frequencies (in the mm-wave range and beyond). If you can make a nonlinear device, you can make a mixer (think also of nonlinear optics, for example)."
  },
  {
    "objectID": "rfic.html#sec-mixer-time-variant",
    "href": "rfic.html#sec-mixer-time-variant",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "5.2 Time-Variant Mixer",
    "text": "5.2 Time-Variant Mixer\nIn contrast to the non-linear mixer, a time-variant mixer uses an ideally linear device, but changes its properties over time. A simple example is a switch, which is opened and closed at the LO frequency. The block diagram of such a mixer is shown in Figure 39. This system is ideally linear from input to output, but very nonlinear when considering the LO input.\n\n\n\n\n\n\n\n\nFigure 39: A switch as a time-variant mixer.\n\n\n\n\n\nWhen the switch is closed (\\(s_\\mathrm{LO} \\ge 0\\)), the input signal is passed to the output, i.e., \\(s_\\mathrm{out}(t) = s_\\mathrm{in}(t)\\); when the switch is open (\\(s_\\mathrm{LO} &lt; 0\\)), no signal is passed, i.e., \\(s_\\mathrm{out}(t) = 0\\). The output signal can be expressed as\n\\[\n\\begin{split}\ns_\\mathrm{out}(t) &= s_\\mathrm{in}(t) \\cdot \\underbrace{\\frac{1}{2} \\left\\{ 1 + \\mathrm{sgn}[ s_\\mathrm{LO}(t) ] \\right\\}}_\\text{Fourier series} \\\\\n&= s_\\mathrm{in}(t) \\cdot \\left[ \\frac{1}{2} + \\frac{2}{\\pi} \\sin(\\omega_\\mathrm{LO} t) + \\frac{2}{3 \\pi} \\sin(3 \\omega_\\mathrm{LO} t) + \\frac{2}{5 \\pi} \\sin(5 \\omega_\\mathrm{LO} t) + \\ldots\\right]\n\\end{split}\n\\]\nwhere \\(\\mathrm{sgn}(\\cdot)\\) is the signum function. The term in curly brackets is a square wave that switches between 0 and 1 at the LO frequency. This square wave can be expressed as a Fourier series, which contains the fundamental frequency at \\(\\omega_\\mathrm{LO}\\) and all odd harmonics \\(3 \\omega_\\mathrm{LO}\\), \\(5 \\omega_\\mathrm{LO}\\), and so on. In conclusion, if the input signal is expressed as \\(\\cos(\\omega_\\mathrm{in} t)\\), we get the output signal\n\\[\ns_\\mathrm{out}(t) = \\frac{1}{\\pi} \\cos(\\omega_\\mathrm{in} t \\pm \\omega_\\mathrm{LO} t) + \\ldots\n\\tag{37}\\]\nAgain, we see that the output signal contains the desired frequency components at \\(\\omega_\\mathrm{in} \\pm \\omega_\\mathrm{LO}\\), but also additional components at \\(3 \\omega_\\mathrm{LO}\\), \\(5 \\omega_\\mathrm{LO}\\), and so on. These additional components are usually unwanted and have to be filtered out in a subsequent filtering stage. The advantage of a time-variant mixer is that it can be implemented readily in CMOS, as shown in Figure 40.\n\n\n\n\n\n\n\n\nFigure 40: A MOSFET as a switch used as a mixer, with ac-coupled LO signal.\n\n\n\n\n\nThe implementation in Figure 40 uses a single NMOS transistor as a switch. The LO signal is ac-coupled to the gate of the transistor, so that the dc operating point is set by the bias voltage \\(V_\\mathrm{bias}\\). When the LO signal is high enough, the transistor is switched on and the input signal \\(v_\\mathrm{in}(t)\\) is passed to the output; when the LO signal is low, the transistor is switched off and no signal is passed. Note that with the MOSFET we can switch voltages as well as currents, so the mixer can work in both modes, voltage mode or current mode.\nA big disadvantage of this simple implementation is that the input signal is “lost” for half of the cycle when the MOSFET switch is open. A fully differential implementation (having differential ports at input, output and LO input) can alleviate this problem, as the input signal is always connected to one of the two output ports. This configuration is called a “double-balanced mixer” and is widely used in practice. It is shown in Figure 41.\n\n\n\n\n\n\n\n\nFigure 41: A fully-differential double-balanced MOSFET mixer.\n\n\n\n\n\nNote that with a double-balanced mixer, we have 6 dB more conversion gain compared to the single-balanced mixer (refer to Equation 37), as can be seen in Equation 38. Note that the factor of \\(2/\\pi\\) represents the conversion loss of an ideal double-balanced mixer of 3.9 dB.\n\\[\ns_\\mathrm{out}(t) = \\frac{2}{\\pi} \\cos(\\omega_\\mathrm{in} t \\pm \\omega_\\mathrm{LO} t) + \\ldots\n\\tag{38}\\]\nNote that no dc current needs to flow through the mixer structures shown in Figure 40 and Figure 41, which is a huge advantage when thinking of flicker noise. Flicker noise in a MOSFET is proportional to the dc current flowing through the device, so if no dc current flows, no flicker noise is generated!\nIn order to look at the full picture, we embed the mixer of Figure 41 in a complete RX front-end, as shown in Figure 42. An LNA (essentially a \\(g_\\mathrm{m}\\) cell) creates a current signal from the received voltage signal at the antenna. This current signal is then fed to the mixer in the current domain, and further sinked into a transimpedance amplifier (TIA), which also implements a lowpass pole. The simplified circuit is shown in Figure 42.\n\n\n\n\n\n\n\n\nFigure 42: An RX front-end using a current-mode (passive) mixer.\n\n\n\n\n\nThe capacitors connected directly to ground at the mixer outputs are good practice, as they shunt high-frequency switching noise to ground, and in this way help the performance of the TIA, which otherwise would have to sink these currents. Note that all blocker currents originating at the LNA output and passing through the mixer are shunt by the feedback capacitors to the output of the TIA, and need to be actively driven by the TIA’s output stage!"
  },
  {
    "objectID": "rfic.html#sec-mixer-gilbert",
    "href": "rfic.html#sec-mixer-gilbert",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "5.3 Gilbert Cell Mixer",
    "text": "5.3 Gilbert Cell Mixer\nAs we have seen in Section 5.2, in CMOS we have quite a few options to implement a mixer as the MOSFET is a good current and voltage switch. This is in stark contrast to the BJT, as the bipolar transistor can only be used as an (excellent) current switch in the differential pair configuration. Hence, we need to implement a current-mode mixer in bipolar technology. The most widely used structure is the so-called “Gilbert cell”, shown in Figure 43.\n\n\n\n\n\n\n\n\nFigure 43: A Gilbert mixer based on bipolar differential pairs.\n\n\n\n\n\nThe idea is that an input transconductor creates currents, which are switched in the double differential pair, and converts the mixed currents back to voltage in the load impedances. There are many known variations of this circuit, like skipping the current source at the bottom to increase linearity and headroom, or swap the input transconductor for another structure. It might also be useful to “bleed” some bias current from the LO switching stage to improve noise."
  },
  {
    "objectID": "rfic.html#sec-mixer-npath",
    "href": "rfic.html#sec-mixer-npath",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "5.4 N-Path Filter",
    "text": "5.4 N-Path Filter\nWithout much deliberation or diving into the background, it has to be stated that passive mixers (based on MOSFET switches) have a few very interesting properties. One of them is that they can be used to implement very high-quality bandpass filters, called “N-path filters” (Lepage, Cahn, and Brown 1953). The basic idea is shown in Figure 44. We use 4 phases of an LO signal to switch the 4 capacitors to ground in a round-robin fashion. The duty cycle of each LO phase is 25% to have a non-overlapping clock.\n\n\n\n\n\n\n\n\nFigure 44: A 4-phase N-path filter.\n\n\n\n\n\nThe key observation to make is that a switch that is opened and closed at a certain frequency (the LO frequency) can be seen as a time-variant resistor. This time-variant resistor converts impedance seen at one end to the other end, and also changes the frequency of this apparent impedance. In other words, if we connect grounded capacitors at one end of the mixer switches and look into the other end, we see a bandpass filter centered at the LO frequency! In this way we can build bandpass filters (and also bandstop filters (Vazny et al. 2010)) at very high frequencies which are precisely centered (without component variations) around \\(\\omega_\\mathrm{LO}\\).\nThe impedance characteristics of such a tank circuit exhibit first-order bandpass behavior, as shown in Figure 45. The peak impedance reaches 5.3 times the switch resistance (\\(R_\\mathrm{sw}\\)), and importantly, the bandwidth is inversely proportional to the capacitance value, i.e., \\(\\mathrm{BW} \\propto 1/C\\). This relationship allows for precise control of the filter bandwidth by adjusting the capacitor values.\n\n\n\n\n\n\n\n\nFigure 45: First-order bandpass behavior and impedance of an N-path filter."
  },
  {
    "objectID": "rfic.html#sec-mixer-lo",
    "href": "rfic.html#sec-mixer-lo",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "5.5 LO Generation",
    "text": "5.5 LO Generation\nAs you have seen in Section 3.2, for complex-valued modulation schemes (like QPSK, QAM, OFDM, and so on) we need to generate quadrature LO signals (i.e., two LO signals with a 90° phase shift). There are many ways to generate such signals, and we will now study a few of them.\n\n5.5.1 RC/CR Phase Shift Network\nThe probably simplest way to generate quadrature LO signals is to use a simple RC/CR phase shift network, as shown in Figure 46. The RC network (a first-order lowpass) generates a phase shift between 0° and -90°, while the CR network (a first-order highpass) generates a phase shift between 0° and +90°. By operating the network at \\(\\omega_0 = 1/RC\\), we can achieve a 90° phase shift between the two outputs.\n\n\n\n\n\n\n\n\nFigure 46: An RC/CR IQ generation network.\n\n\n\n\n\nThe advantage of this network is its simplicity, as it only requires a few passive components, and it can generate I and Q phases from an incoming signal at the target frequency. The disadvantage is that the phase shift is frequency-dependent, so the quadrature signals are only perfectly in phase at one frequency. Furthermore, the amplitude of the two outputs is not equal, which might require additional gain stages to equalize the amplitudes. Additionally, this passive network has a 3 dB loss, so we need additional gain stages to compensate for this loss.\n\n\n5.5.2 Polyphase Filter\nWe can create a more advanced phase shift network (following the idea of Section 5.5.1) by using a so-called “polyphase filter” (Kaukovuori et al. 2008). The idea is to use multiple RC sections to create a more ideal phase shift network. An example is shown in Figure 47.\n\n\n\n\n\n\n\n\nFigure 47: A two-stage polyphase network.\n\n\n\n\n\nAs shown in the figure above, we can use multiple stages (with \\(R_1 C_1 \\ne R_2 C_2\\)) in cascade to broaden the frequency range over which we have a good 90° phase shift between the four outputs. The more stages we use, the better the performance, but also the more components are required. Note that this network is still passive, so it has an inherent loss, which also increases with the number of stages. The idea behind this network is that the network is transparent for positive frequencies, and blocks negative frequencies (or vice versa). We enter the network with real signals (having positive and negative frequency components), and at the output we get complex signals (having only positive or negative frequency components). In this sense this network is a complex filter.\nNote that polyphase filters are popular when we have to create complex signals from real signals. We can use the polyphase filter in the LO path, or also in the signal path (e.g., in a receiver). The benefit is that we can work with a signal where input frequency is equal to the output frequency. This is in strong contrast to the approach we will discuss next.\n\n\n5.5.3 Flip-Flop Based Phase Generation\nThe most widely used approach to generate quadrature LO signals is to use digital circuits, as shown in Figure 48. The idea is to use a flip-flop (or a latch) to divide the frequency of an incoming clock signal by 2. If we drive the two flip-flops (which are connected as toggle flip-flops) by inverted clocks, then the resulting output signals are 90° phase-shifted, as shown in Figure 49.\n\n\n\n\n\n\n\n\nFigure 48: I/Q generation with a divide-by-2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49: Input and output waveforms of I/Q generation with a divide-by-2.\n\n\n\n\n\nIt is important to note that the circuit shown in Figure 48 has to be implemented in a way so that there is no phase ambiguity, i.e., the I-phase is lagging the Q-phase by exactly 90°. This can be achieved by using a reset signal to set the flip-flops to a known state at startup, or by implementing flip-flops using a differential clock input.\nIf the input clock is not 2x the target frequency but higher (e.g., 4x), then we can use a frequency divider approach to generate more than four phases. This could be useful in an N-path filter, where we could need more than four phases (see Section 5.4).\nOften, we need four-phase LO signals with 25% duty cycle (instead of the 50% duty cycle shown in Figure 49). This can be achieved by using additional logic gates to combine the outputs of the flip-flops, as shown in Figure 50. The resulting LO waveforms are shown in Figure 51.\n\n\n\n\n\n\n\n\nFigure 50: I/Q generation with a divide-by-2 and 25% duty cycle generation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 51: Input and output waveforms of I/Q generation with a divide-by-2 and 25% duty cycle generation.\n\n\n\n\n\nThe advantage of using flip-flops to generate multi-phase LO signals is that very precise phase shifts can be achieved, which can be produced over a very wide frequency range. They can be implemented in different logic styles, like CMOS, ECL, CML, and so on. The disadvantage is that we need a high-frequency clock signal above the target frequency, which might be difficult to generate. Still, I/Q generation with flip-flops is the de-facto standard approach in modern RFICs.\n\n\n5.5.4 Delay-Based Phase Generation\nThe final approach we want to mention is to use a delay line (or multiple ones) to generate phase shifts. Using transmission lines, this approach is widely used at very high frequencies, where other approaches (like the ones shown in Section 5.5.2 or Section 5.5.3) are difficult to implement. One well-known approach is the 90° hybrid coupler, realized with \\(\\lambda/4\\) transmission lines (Pozar 2011). When the frequencies are high enough (and the resulting wavelengths short enough), this approach can even be implemented on-chip. The disadvantage is that the phase shift is frequency-dependent, so the quadrature signals are only perfectly in phase at one frequency. Furthermore, this passive network has a 3 dB loss, so we need additional gain stages to compensate for this loss. A branch-line hybrid coupler is shown in Figure 52.\n\n\n\n\n\n\n\n\nFigure 52: Branch-line hybrid coupler schematic showing the transmission line structure with characteristic impedances and λ/4 length sections.\n\n\n\n\n\nThe 3-port S-parameter matrix of an ideal branch-line coupler (assuming port 4 is poperly terminated with \\(Z_0\\)) is given by\n\\[\nS = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}\n0 & -j & -1 \\\\\n-j & 0 & 0 \\\\\n-1 & 0 & 0\n\\end{bmatrix}.\n\\tag{39}\\]\nA feature of the coupler in Figure 52 is that it is single-ended; a differential implementation needs either two couplers or a coupler with subsequent baluns.\nAnother approach, which is similar to the flip-flop based approach, is to use a delay line to generate the required phase shifts. This can be done with a single delay line and multiple taps, or with multiple delay lines. The idea is that we delay the input clock signal by a certain amount of time, which corresponds to a 90° phase shift at the target frequency. This approach is shown in Figure 53. Since the delay of a delay line is frequency dependent, and the delay will change with process, voltage, and temperature (PVT) variations, we have to implement a tuning mechanism to adjust the delay. This implementation is called a delay-locked loop (DLL), which is similar to a phase-locked loop (PLL, see Section 7).\n\n\n\n\n\n\n\n\nFigure 53: LO multiphase generation by delay-locked loop (DLL). An extension to more than four phases is straightforward.\n\n\n\n\n\nWith a phase detector (PD) we can compare the phase of the output signal with the input signal, and regulate the delay per stage so that after four delays the edges line up. Then we can tap the equally-spaced phases and use them as LO signals. The advantage of this approach is that we can flexibly create a required number of phases from an input clock. This feature is also used in wireline communication systems to generate multiple phases for the data sampling."
  },
  {
    "objectID": "rfic.html#sec-oscillator-noise",
    "href": "rfic.html#sec-oscillator-noise",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.1 Oscillator Noise",
    "text": "6.1 Oscillator Noise\nFor calculating the noise of an oscillator, we assume that an LC-based oscillator as shown in Figure 56 is used. We assume that the oscillator is in steady-state operation, i.e., \\(-R_\\mathrm{amp} = R_\\mathrm{p}\\). We can then simplify the circuit to the one shown in Figure 57.\n\n\n\n\n\n\n\n\nFigure 57: LC parallel tank with oscillator in steady-state operation.\n\n\n\n\n\nWe can calculate\n\\[\nZ_\\mathrm{tank}(s) = \\frac{s L \\frac{1}{s C}}{s L + \\frac{1}{s C}} = \\frac{s L}{1 + s^2 L C}.\n\\]\nAt \\(s = j \\omega_0\\), we have \\(Z_\\mathrm{tank}(s) \\to \\infty\\), so let’s approximate around \\(\\omega_0\\):\n\\[\nZ_\\mathrm{tank}(j \\omega_0 + j \\Delta \\omega) =\\frac{j (\\omega_0 + \\Delta \\omega) L}{1 - (\\omega_0 + \\Delta \\omega)^2 L C}\n\\]\nFor \\(\\Delta \\omega \\ll \\omega_0\\), we can approximate \\((\\omega_0 + \\Delta \\omega)^2 \\approx \\omega_0^2 + 2 \\omega_0 \\Delta \\omega\\) and \\(\\omega_0 + \\Delta \\omega \\approx \\omega_0\\), which yields (using \\(\\omega_0^2 L C = 1\\))\n\\[\nZ_\\mathrm{tank}(j \\omega_0 + j \\Delta \\omega) = -\\frac{j}{2 \\Delta \\omega C}.\n\\]\nWe now use the correspondence \\(C = 1 / \\omega_0^2 L\\) and express \\(L = R_\\mathrm{p} / \\omega_0 Q\\) to get\n\\[\n|Z_\\mathrm{tank}(j \\omega_0 + j \\Delta \\omega)| = \\frac{R_\\mathrm{p}}{2 Q} \\left( \\frac{\\omega_0}{\\Delta \\omega} \\right)\n\\]\nwhich provides us with an expression for the magnitude of the tank impedance around resonance. We now calculate the noise power if a noise current is injected into this impedance. We use the single-sided noise current of \\(R_\\mathrm{p}\\) (see Section 2.3.1), increased by a factor \\(F\\) contributed by the active circuit providing \\(-R_\\mathrm{amp}\\):\n\\[\n\\overline{I_n^2} = \\frac{4 k T F}{R_\\mathrm{p}}\n\\]\nThe noise voltage across the tank at a frequency offset \\(\\Delta \\omega\\) from \\(\\omega_0\\) is then\n\\[\n\\overline{V_n^2}(\\Delta \\omega) = |Z_\\mathrm{tank}(j \\omega_0 + j \\Delta \\omega)|^2 \\cdot \\frac{4 k T F}{R_\\mathrm{p}} = \\frac{k T F R_\\mathrm{p}}{Q^2} \\left( \\frac{\\omega_0}{\\Delta \\omega} \\right)^2\n\\]\nThis absolute noise voltage is not of much interest per se. We normalize it to the oscillation amplitude \\(V_\\mathrm{p}\\), and only account for 1/2 of the noise, as the total noise power is split equally into amplitude noise and phase noise (B. Razavi 1996), and we are only interested in the phase noise. This is because we assume the amplitude noise is removed by amplitude clipping in the LO chain routing the oscillator signal to the mixer. This is often a valid assumption. We introduce the symbol \\(\\mathcal{L}\\left\\{\\cdot\\right\\}\\) to denote this normalized phase noise of the oscillator:\n\\[\n\\mathcal{L}\\left\\{\\Delta \\omega\\right\\} = \\frac{\\frac{1}{2} \\frac{k T F R_\\mathrm{p}}{Q^2} \\left( \\frac{\\omega_0}{\\Delta \\omega} \\right)^2}{\\left( \\frac{V_\\mathrm{p}}{\\sqrt{2}} \\right)^2} = \\frac{k T F R_\\mathrm{p}}{V_\\mathrm{p}^2} \\cdot \\frac{1}{Q^2} \\cdot \\left( \\frac{\\omega_0}{\\Delta \\omega} \\right)^2\n\\tag{41}\\]\nThis equation is known as “Leeson’s equation” (Leeson 1966), and it provides us with important insights on how to design low phase noise oscillators:\n\nUse a resonator with high quality factor \\(Q\\) to reduce phase noise. On-chip LC tanks usually provide moderate \\(Q\\) values (e.g., 10 to 30), while off-chip crystals can provide very high \\(Q\\) values (e.g., 10,000 to 100,000).\nMaximize the oscillation amplitude \\(V_\\mathrm{p}\\) to increase the stored energy in the resonator. In CMOS implementations, this is often limited by the supply voltage and/or device breakdown voltages.\nUse an active circuit with low noise factor \\(F\\) to minimize the noise contribution of the negative resistance \\(-R_\\mathrm{amp}\\). A good value to aim for is \\(F \\approx 2\\) or lower.\nMaximize the tank resistance \\(R_\\mathrm{p}\\) to minimize the thermal noise contribution. This can be achieved by using high-Q inductors and low-loss capacitors.\n\nThe phase noise \\(\\mathcal{L}\\left\\{\\Delta \\omega\\right\\}\\) is expressed in the unit of dBc/Hz, i.e., in decibels of phase noise relative to the carrier power per 1 Hz of bandwidth. When expressing the phase noise of an oscillator in dBc/Hz it is important to state both the oscillation frequency \\(\\omega_0\\) and the offset frequency \\(\\Delta \\omega\\) at which the phase noise is evaluated. For example, we could say that an oscillator has a phase noise of -137 dBc/Hz at 3 MHz offset from a carrier frequency of 2 GHz.\nThe phase noise expressed with Equation 41 describes an important region of the total oscillator phase noise where the phase noise decreases with \\(1 / (\\Delta \\omega)^2\\), i.e., with 20 dB per decade. This region is often called the “thermal noise region” as it is dominated by thermal noise from the tank resistor and the active circuit. However, at lower offset frequencies, other noise mechanisms (like flicker noise) can dominate, leading to different slopes of the phase noise vs. offset frequency curve. At larger offset frequencies, the phase noise can flatten out due to thermal noise floor limitation of buffer amplifiers following the oscillator. A typical phase noise plot of an LC oscillator is shown in Figure 58.\n\n\n\n\n\n\n\n\nFigure 58: Phase noise spectrum of a typical LC oscillator showing characteristic 1/f³ and 1/f² slopes vs. frequency offset from carrier. The flicker noise corner (where the 1/f³ region transitions to the 1/f² region) is around 10 kHz, and the thermal noise floor onset is marked at 10 MHz.\n\n\n\n\n\nIt is important to note that the phase noise does not go to infinity as \\(\\Delta \\omega\\) approaches zero, as Equation 41 might suggest. This is an artifact of the simplified model used to derive Leeson’s equation. In reality, the phase noise has the form of a Lorentzian spectrum around the carrier frequency, which means that it flattens out at very low offset frequencies with a trend described by\n\\[\n\\mathcal{L}\\left\\{\\Delta \\omega \\ll\\right\\} \\propto \\frac{1}{\\omega_\\mathrm{B}^2 + \\Delta \\omega^2}\n\\]\nwith \\(\\omega_\\mathrm{B}\\) being the half-power bandwidth of the oscillator spectrum.\nWhy is phase noise important?\n\nIn RX and TX the phase noise of the LO creates jitter, which degrades signal quality and contributes to EVM degradation.\nIn TX, phase noise of the LO creates noise sidebands around the carrier, which can lead to adjacent channel interference together with spectral regrowth due to nonlinearities and thermal noise.\nIn RX, reciprocal mixing of the phase noise sidebands with the desired signal creates in-band noise, which degrades the SNR and sensitivity of the RX.\nIn systems using multiple carriers or carriers with high-order modulation schemes (like in OFDM), phase noise can lead to inter-carrier interference (ICI) and symbol misinterpretation, further degrading system performance.\n\n\n\n\n\n\n\nNote 7: Phase Noise and Frequency Division\n\n\n\nWhen using a frequency divider to generate a lower-frequency LO signal from a higher-frequency oscillator (e.g., to generate IQ phases), the phase noise of the divided signal is affected by the division ratio \\(N\\) (Da Dalt and Sheikholeslami 2018). Specifically, the phase noise of the divided signal improves by \\(20 \\log_{10}(N)\\) dB compared to the original oscillator phase noise. This means that if you divide the frequency by a factor of 2, the phase noise will be lowered by 6 dB!\nThe statement above is related to the phase noise at the output coming from the input of the divider. Of course, the frequency divider itself also contributes some additional phase noise, which can degrade the overall phase noise performance. However, in many practical cases, the improvement due to frequency division outweighs the additional noise introduced by the divider."
  },
  {
    "objectID": "rfic.html#sec-oscillator-jitter",
    "href": "rfic.html#sec-oscillator-jitter",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.2 Jitter",
    "text": "6.2 Jitter\nJitter is a time-domain representation of phase noise. It quantifies the timing variations of a periodic signal, such as the zero-crossings or rising edges of a clock signal. Jitter is typically measured in seconds (or fractions thereof) and can be expressed as root mean square (RMS) jitter or peak-to-peak jitter. The relationship between phase noise and RMS jitter can be approximated by the following equation (Da Dalt and Sheikholeslami 2018): \\[\n\\sigma_\\mathrm{a} = \\frac{1}{2 \\pi f_0} \\sqrt{2 \\int_{f_1}^{f_2} \\mathcal{L}\\left\\{f\\right\\} df}\n\\tag{42}\\]\nwhere \\(\\sigma_\\mathrm{a}\\) is the absolute RMS jitter, \\(f_0\\) is the carrier frequency, and the integral is taken over the frequency offset range from \\(f_1\\) to \\(f_2\\). This equation shows that higher phase noise levels lead to increased jitter, which can adversely affect the performance of digital communication systems by causing timing errors and signal integrity issues. Note that the phase noise \\(\\mathcal{L}\\left\\{f\\right\\}\\) in Equation 42 must be expressed in linear scale (not dBc/Hz) when using this equation."
  },
  {
    "objectID": "rfic.html#sec-oscillator-reciprocal-mixing",
    "href": "rfic.html#sec-oscillator-reciprocal-mixing",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.3 Reciprocal Mixing",
    "text": "6.3 Reciprocal Mixing\nReciprocal mixing is a phenomenon that occurs in receivers when the phase noise sidebands of the local oscillator mix with strong adjacent channel signals, resulting in in-band noise that degrades the signal-to-noise ratio of the desired signal. This effect is particularly pronounced in systems with high-order modulation schemes or closely spaced channels, where even small amounts of phase noise can lead to significant performance degradation.\nTo analyze reciprocal mixing, we consider a scenario where a strong interferer is present at a frequency offset \\(\\Delta f\\) from the desired signal. The phase noise of the local oscillator at this offset frequency can be characterized by its power spectral density \\(\\mathcal{L}\\left\\{\\Delta f\\right\\}\\). When the local oscillator mixes with the interferer, the phase noise sidebands effectively “fold” into the desired signal band, creating additional noise. This noise level can be estimated by multiplying the power of the interferer by the phase noise level at the offset frequency and considering the channel bandwidth \\(B\\) of the RX\n\\[\nP_\\mathrm{RM} = P_\\mathrm{interferer} + \\mathcal{L}\\left\\{\\Delta f\\right\\} + 10 \\log_{10}(B)\n\\tag{43}\\]\nwhere \\(P_\\mathrm{RM}\\) is the power of the reciprocal mixing noise introduced into the desired signal band.\n\n\n\n\n\n\nNote 8: Reciprocal Mixing Example\n\n\n\nLet us assume the following example from a Bluetooth LE receiver: The sensitivity target for the RX is -70 dBm with an SNR of 10 dB for a 1 MHz channel. An adjacent channel interferer is present at -27 dB channel to interferer ratio at an offset of 3 MHz. How much phase noise can the LO have to meet the sensitivity target?\nFrom the sensitivity target and the SNR requirement, we can calculate the maximum allowable noise floor in the RX. We add a margin of 3 dB to account for implementation losses:\n\\[\nP_\\mathrm{noise,max} = P_\\mathrm{sens} - \\text{SNR} - P_\\mathrm{margin} = -70\\,\\text{dBm} - 10\\,\\text{dB} - 3\\,\\text{dB} = -83\\,\\text{dBm}\n\\]\nThe interferer power is:\n\\[\nP_\\mathrm{interferer} = P_\\mathrm{sens} - \\text{C/I} = -70\\,\\text{dBm} + 27\\,\\text{dB} = -43\\,\\text{dBm}\n\\]\nUsing Equation 43, we can express the maximum allowable phase noise at 3 MHz offset at 2.4 GHz as:\n\\[\n\\mathcal{L}\\left\\{3\\,\\text{MHz}\\right\\} = P_\\mathrm{noise,max} - P_\\mathrm{interferer} - 10 \\log_{10}(B) = -83\\,\\text{dBm} + 43\\,\\text{dBm} - 60\\,\\text{dB} = -100\\,\\text{dBc/Hz}\n\\]\n\n\nThe reciprocal mixing scenario described in Note 8 is visualized in Figure 59.\n\n\n\n\n\n\n\n\nFigure 59: Reciprocal mixing in a receiver: A strong interferer at offset frequency mixes with LO phase noise sidebands, creating in-band noise that degrades the desired signal SNR. The desired signal shows the same, scaled phase noise profile around it as the interferer, as both mix with the same LO phase noise."
  },
  {
    "objectID": "rfic.html#sec-single-ended-oscillators",
    "href": "rfic.html#sec-single-ended-oscillators",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.4 Single-Ended Oscillators",
    "text": "6.4 Single-Ended Oscillators\nSingle-ended oscillators are commonly used in RF applications due to their simplicity and ease of integration, especially in quartz oscillators. A negative resistance is implemented using a single transistor amplifier as is shown in Figure 60. The transistor is configured with two capacitors \\(C_1\\) and \\(C_2\\) to provide a phase-shifted feedback path.\n\n\n\n\n\n\n\n\nFigure 60: Circuit diagram of a single-ended negative resistance implementation.\n\n\n\n\n\nIt can be shown that the differential input impedance looking into the transistor gate and drain is given by\n\\[\nZ_\\mathrm{in}(s) = -\\frac{g_\\mathrm{m}}{\\omega^2 C_1 C_2} + \\frac{C_1 + C_2}{s C_1 C_2} = -R_\\mathrm{amp} + \\frac{1}{s C_\\mathrm{amp}}\n\\]\nwhich consists of the series combination of a negative resistance \\(-R_\\mathrm{amp}\\) and a capacitance \\(C_\\mathrm{amp}\\). Note that the circuit in Figure 60 does not show a ground symbol. In fact, any of the three nodes marked with blue numbers can be used as a reference node (ground), and this results in the following well-known oscillator topologies:\n\n\n\nReference Node\nOscillator Topology\n\n\n\n\nNode 1 (Gate)\nColpitts oscillator\n\n\nNode 2 (Source)\nPierce oscillator\n\n\nNode 3 (Drain)\nClapp oscillator\n\n\n\nWhen we investigate the equivalent electrical circuit of a quartz crystal, we find that it has inductive behavior between its series resonance frequency and its parallel resonance frequency, which are very close together. The quartz crystal equivalent circuit is shown in Figure 61.\n\n\n\n\n\n\n\n\nFigure 61: Quartz crystal equivalent circuit.\n\n\n\n\n\nThis means we can operate the quartz crystal as a high-Q inductor in an oscillator circuit, which results in the single-ended quartz crystal oscillator shown in Figure 62, which is a very popular choice for high-performance crystal oscillators. Note that in a simple implementation the current-bias MOSFET can be replaced by an inverter stage biased in the linear region.\n\n\n\n\n\n\n\n\nFigure 62: Circuit diagram of a single-ended Pierce crystal oscillator operating the quartz between series and parallel resonance where it acts as a large high-Q inductor. Note that the quartz crystal has no dc path, hence we need a high-ohmic bias resistor to connect M1 into a diode configuration.\n\n\n\n\n\nThe series combination of \\(C_1\\) and \\(C_2\\) provides the load capacitance required by the crystal to oscillate at its specified frequency. The oscillation frequency can be approximated by (\\(C_\\mathrm{load}^{-1} = 1/C_1 + 1/C_2\\)):\n\\[\nf_0 \\approx \\frac{1}{2 \\pi \\sqrt{L_\\mathrm{m} C_\\mathrm{load}}}\n\\]"
  },
  {
    "objectID": "rfic.html#sec-differential-oscillators",
    "href": "rfic.html#sec-differential-oscillators",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.5 Differential Oscillators",
    "text": "6.5 Differential Oscillators\nAfter discussing single-ended oscillators in Section 6.4, we now turn to differential oscillator topologies, which are widely used in integrated LC oscillators due to their superior common-mode noise rejection and reduced even-order harmonics. A popular way to create a differential negative resistance is to use a cross-coupled pair of transistors as shown in Figure 63.\n\n\n\n\n\n\n\n\nFigure 63: A cross-coupled differential pair.\n\n\n\n\n\nWe assume a symmetrical circuit by setting \\(g_\\mathrm{m1} = g_\\mathrm{m2} = g_\\mathrm{m}\\). By analyzing the small-signal equivalent circuit, we can derive the differential input impedance looking into the gates of the transistors:\n\\[\nZ_\\mathrm{in} = -\\frac{2}{g_\\mathrm{m}}\n\\]\nA practical implementation of a differential LC oscillator using the cross-coupled pair is shown in Figure 64 using a so-called “NMOS core”, as it is using an NMOS-based differential pair.\n\n\n\n\n\n\n\n\nFigure 64: An LC differential oscillator using an NMOS cross-coupled differential pair. \\(L_1\\) and \\(L_2\\) are usually implemented as a single on-chip spiral inductor with a center tap.\n\n\n\n\n\nBy putting the circuit in Figure 64 onto its head we obtain a “PMOS core” oscillator, which is also widely used in integrated LC oscillators. This configuration is shown in Figure 65.\n\n\n\n\n\n\n\n\nFigure 65: An LC differential oscillator using a PMOS cross-coupled differential pair. \\(L_1\\) and \\(L_2\\) are usually implemented as a single on-chip spiral inductor with a center tap.\n\n\n\n\n\nIn both oscillator topologies, the oscillation frequency is determined by the LC tank circuit formed by \\(L = L_1 + L_2\\) and \\(C\\). The oscillation frequency can be approximated by\n\\[\nf_0 = \\frac{1}{2 \\pi \\sqrt{L C}}\n\\]\nAs both technologies have their inductor middle point tied to a supply rail (\\(V_\\mathrm{DD}\\) for the NMOS core, \\(V_\\mathrm{SS}\\) for the PMOS core), the voltage swing across the tank can go well beyond the supply voltage, which is an advantage of these topologies. However, note that the maximum voltage swing is still limited by the device breakdown voltages of \\(M_1\\) and \\(M_2\\), which can be critical in advanced CMOS technologies with low breakdown voltages. To utilize both an NMOS and a PMOS cross-coupled pair in parallel, a so-called “complementary (or CMOS) LC oscillator” can be used, which is shown in Figure 66. Here, the voltage swing across the tank is limited to the supply voltage, so it is inherently safe regarding device breakdown. Also, the transconductance of both NMOS and PMOS devices contribute to the negative resistance, which can reduce power consumption for a given oscillation amplitude.\n\n\n\n\n\n\n\n\nFigure 66: An LC differential oscillator using a PMOS and an NMOS cross-coupled differential pair.\n\n\n\n\n\nNote that no bias current sources are shown in Figure 66, but can be added in practice. However, they require some voltage headroom, which lowers the maximum voltage swing across the tank, which is negative for phase noise performance according to Leeson’s equation in Equation 41.\nThe inductor \\(L\\) used in LC tanks is often implemented as a differential spiral inductor, which can be realized on-chip using the top metal layers of the CMOS process. If higher \\(Q\\) is sought, then off-chip inductors must be used, which is more expensive and uses package pins to connect the inductor. Alternatively, bondwire inductors can be used, which utilize the bondwires connecting the die to the package leads as inductors. Bondwire inductors can provide high \\(Q\\) values and are a cost-effective solution for improving oscillator performance without requiring off-chip components. For mm-wave frequencies, transmission line stubs can be used as inductors, which can be implemented on-chip using coplanar waveguide (CPW) or microstrip structures."
  },
  {
    "objectID": "rfic.html#sec-frequency-tuning-of-oscillators",
    "href": "rfic.html#sec-frequency-tuning-of-oscillators",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.6 Frequency Tuning of Oscillators",
    "text": "6.6 Frequency Tuning of Oscillators\nNote that all three LC oscillator topologies shown in Section 6.5 have a fixed oscillation frequency determined by the LC tank circuit. In practice, it is often desirable to be able to tune the oscillation frequency over a certain range. There are only two ways to achieve this, either by tuning the inductance \\(L\\) or the capacitance \\(C\\) of the tank circuit. In integrated LC oscillators, it is common to use a fixed inductor and a tunable capacitor, which is shown in Figure 64, Figure 65, and Figure 66.\nThe need to change the oscillator frequency can arise from various requirements:\n\nWe need to precisely tune the oscillator frequency to match a desired carrier frequency.\nThe oscillator frequency needs to be adjusted to compensate for process, voltage, and temperature (PVT) variations that can affect the resonant frequency of the LC tank.\n\nIn order to achieve a tunable capacitance, a so-called “varactor” (variable capacitor) is often used. A varactor is a semiconductor device that exhibits a capacitance that varies with the applied voltage. Common ways to implement a varactor in integrated circuits are:\n\nUse the gate-to-channel capacitance of a MOSFET operated in reverse bias. By changing the voltage applied to the gate, the depletion region width in the channel changes, which in turn changes the capacitance.\nUse a PN-junction diode operated in reverse bias. The depletion region width of the diode changes with the applied voltage, leading to a change in capacitance.\nUse a gate-to-channel capacitance of special MOSFET structure in accumulation mode, which can be achieved by using a NMOS situated in an n-well (in contrast to the usual operation in a p-well). Changing the gate-to-channel voltage changes the accumulation capacitance (Andreani and Mattisson 2000).\nUse a switched capacitor bank, where multiple capacitors are connected in parallel or series using switches (usually MOSFETs) to achieve discrete capacitance values.\n\nIn order to characterize the tuning sensitivity of an oscillator, the metric \\(K_\\mathrm{VCO}\\) (voltage-controlled oscillator gain) is often used, which is defined as\n\\[\nK_\\mathrm{VCO} = \\frac{d f_0}{d V_\\mathrm{tune}}\n\\]\nwith \\(V_\\mathrm{tune}\\) being the control voltage applied to the varactor. The unit of \\(K_\\mathrm{VCO}\\) is usually expressed in MHz/V or GHz/V. A high \\(K_\\mathrm{VCO}\\) means that a small change in tuning voltage results in a large change in oscillation frequency, which can be beneficial for wide tuning ranges but can also make the oscillator more sensitive to noise on the tuning voltage line.\nNote that the tuning sensitivity \\(K_\\mathrm{VCO}\\) is usually quite nonlinear over the tuning range, so it is common to specify \\(K_\\mathrm{VCO}\\) at a certain operating point or as an average value over the tuning range.\nIn order to achieve a wide tuning range while maintaining a sufficiently small \\(K_\\mathrm{VCO}\\) for phase noise reasons, a combination of coarse and fine tuning mechanisms can be used (Kral, Behbahani, and Abidi 1998). For example, a switched capacitor bank can provide coarse tuning steps, while a varactor can provide fine tuning within each step. It is important to ensure that the overall tuning range is free of dead zones, where the oscillator cannot be tuned to certain frequencies due to non-overlapping tuning ranges of the coarse and fine tuning elements.\nWhile there are many ways to implement a switched capacitor for use in an oscillator tuning circuit, one popular way is shown in Figure 67. Many such switched capacitors with different values of \\(C\\) (often binary weighted) can be combined to form a capacitor bank for coarse frequency tuning.\n\n\n\n\n\n\n\n\nFigure 67: An switched capacitor for use in an oscillator switched capacitor tuning bank. The bias resistors tie the drain/source nodes to ground during turn on of \\(M_1\\) (for low on resistance), while they tie the drain/source nodes to VDD during turn off of \\(M_1\\) to prevent accidential turn on of the drain/source to bulk diodes of \\(M_1\\).\n\n\n\n\n\nHere, when the control signal \\(S\\) is high, the effective capacitance is \\(C/2\\), with a parasitic series resistance \\(R_\\mathrm{on}\\) due to the switch. When \\(S\\) is low, the effective capacitance is \\(C_\\mathrm{off}\\), which is the parasitic drain-source capacitance of the MOSFET switch.\nNote that there exists a trade-off when designing the switched capacitor: A larger switch (with increased \\(W\\)) reduces \\(R_\\mathrm{on}\\), but increases \\(C_\\mathrm{off}\\). A large \\(R_\\mathrm{on}\\) leads to increased losses in the tank circuit, which degrades the quality factor \\(Q\\) and increases phase noise according to Equation 41. On the other hand, a large \\(C_\\mathrm{off}\\) reduces the effective tuning range of the switched capacitor, which can be detrimental if a wide tuning range is required.\nIn digitally-controlled oscillators (DCOs), the tuning voltage \\(V_\\mathrm{tune}\\) is replaced by a digital control word that selects different capacitance values from a capacitor bank. This approach allows for precise and repeatable frequency tuning, which is beneficial in applications requiring frequency synthesis or channel selection. These fine tuning steps can also be implemented according to Figure 67, however, the value of \\(C\\) must be sufficiently small."
  },
  {
    "objectID": "rfic.html#sec-oscillator-modeling",
    "href": "rfic.html#sec-oscillator-modeling",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "6.7 Oscillator Modelling",
    "text": "6.7 Oscillator Modelling\nThe instantaneous frequency of an oscillator can be expressed as\n\\[\n\\omega_\\mathrm{VCO}(t) = \\omega_0 + K_\\mathrm{VCO} \\cdot V_\\mathrm{tune}(t)\n\\]\nwhere \\(\\omega_0\\) is the nominal oscillation frequency, \\(K_\\mathrm{VCO}\\) is the tuning sensitivity, and \\(V_\\mathrm{tune}(t)\\) is the tuning voltage applied to the varactor. Looking at the instantaneous phase \\(\\varphi_\\mathrm{VCO}(t)\\) of the oscillator, we can integrate the instantaneous frequency \\(\\omega_\\mathrm{VCO}(t)\\) to obtain\n\\[\n\\varphi_\\mathrm{VCO}(t) = \\int_0^t \\omega_\\mathrm{VCO}(\\tau) d\\tau = \\omega_0 t + K_\\mathrm{VCO} \\int_0^t V_\\mathrm{tune}(\\tau) d\\tau.\n\\]\nInspecting this equation, we see that with respect to phase, an oscillator is a perfect integrator of the tuning voltage over time! For simulation purposes, we can therefore model an oscillator as an integrator block as shown in Figure 68.\n\n\n\n\n\n\n\n\nFigure 68: A model of a VCO as a perfect integrator for the excess phase in the \\(s\\)-domain.\n\n\n\n\n\nIn this model we look at the phase output of the oscillator as an excess phase with respect to the nominal phase \\(\\omega_0 t\\). This delta phase is obtained by integrating the tuning voltage \\(V_\\mathrm{tune}(t)\\) scaled by \\(K_\\mathrm{VCO}\\)."
  },
  {
    "objectID": "rfic.html#sec-pll-basic-architecture",
    "href": "rfic.html#sec-pll-basic-architecture",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "7.1 Basic PLL Architecture",
    "text": "7.1 Basic PLL Architecture\nWe solve the issues above by using a phase-locked loop (PLL) to stabilize and control the frequency of a tunable oscillator, mostly in the form of a VCO. A PLL is a feedback control system that locks the phase of the VCO output to the phase of a reference signal, typically generated by a crystal oscillator with excellent frequency stability. The block diagram of a basic PLL is shown in Figure 69.\n\n\n\n\n\n\n\n\nFigure 69: Block diagram of a PLL. A reference phase signal is compared to the phase of a VCO output signal in a phase detector. The phase error is low-pass filtered and used to tune the VCO frequency.\n\n\n\n\n\nA reference frequency (often coming from a high-\\(Q\\) crystal oscillator) is compared to the phase of a VCO output signal in a phase detector (PD). The phase error is low-pass filtered and used to tune the VCO frequency. By continuously adjusting the VCO based on the phase difference, the PLL ensures that the VCO output remains synchronized with the reference signal, effectively combining the stability of the reference with the tunability of the VCO. When the output signal of the VCO is fed back to the phase detector through a frequency divider (with division ratio \\(N\\)), the PLL can generate output frequencies that are integer multiples of the reference frequency, given by \\(f_\\mathrm{out} = N \\cdot f_\\mathrm{ref}\\).\nThe PD compares the phase of the reference signal with the phase of the VCO output signal. It works in the time domain and produces an output voltage proportional to the arrival time difference (phase difference) between the two input signals. This output voltage, known as the phase error signal, indicates whether the VCO is leading or lagging the reference signal in phase, expressed as\n\\[\n\\Delta \\varphi = 2 \\pi \\frac{\\Delta t}{T_\\mathrm{ref}}\n\\]\nand\n\\[\nV_\\mathrm{error} = K_\\mathrm{PD} \\cdot \\Delta \\varphi\n\\]\nwhere \\(\\Delta \\varphi\\) is the phase difference (caused by the arrival time difference) in rad, \\(K_\\mathrm{PD}\\) is the phase detector gain in V/rad, \\(\\Delta t\\) is the time difference between the two signals, and \\(T_\\mathrm{ref}\\) is the period of the reference signal.\nAn important consideration of the PD is its linear operating range. The PD can only provide a linear output voltage for small phase differences, typically within ±180° (±π rad). If the phase difference exceeds this range, the PD characteristic “wraps around,” leading to ambiguity in the phase error signal. This can cause the PLL to lose lock or behave unpredictably. Therefore, the PLL design must ensure that the phase difference remains within the linear range of the PD during normal operation, which is especially troublesome if the output frequency \\(f_\\mathrm{out}\\) is not yet close enough to its final steady-state value. Note that a simple PD can be implemented by using an XOR gate for digital signals or a mixer for analog signals.\n\n\n\n\n\n\n\n\nFigure 70: Input and output waveforms of an XOR-based phase-detector.\n\n\n\n\n\nWe can see in Figure 70 that the output of the XOR PD is high when the two input signals differ and low when they are the same. The output duty cycle is proportional to the phase difference between the two input signals. Note that this PD has no notion of frequency, only phase! Still, it can be used to lock the frequency of the VCO to the reference frequency, as a constant phase difference implies equal frequencies; however, this works only if the initial frequency difference is small enough. Also, this XOR-based PD does not evaluate the edges of the input signals, only their logic states. Therefore, it is crucial that both input signals are square waves with a 50% duty cycle to ensure proper operation of the PD.\n\n\n\n\n\n\nNoteJK Flip-Flop Phase Detector\n\n\n\nInstead of an XOR gate, which requires 50% duty cycle input signals, an edge-triggered JK flip-flop can be used. While the overall behavior is similar, the JK flip-flop-based PD is insensitive to duty cycle variations of the input signals, as it only evaluates the rising edges of the reference and feedback signals (Best 1999).\n\n\nAs the output signal of the PD contains high-frequency components (at least at twice the reference frequency), a low-pass filter (LPF) is used to smooth the phase error signal before it is applied to the VCO tuning input. The LPF also determines the dynamic response of the PLL, affecting its stability and transient behavior. A well-designed LPF ensures that the PLL can quickly respond to changes in the reference signal or disturbances while maintaining stability and minimizing overshoot or oscillations in the output frequency. Often, the LPF has voltage-mode inputs and outputs to connect to the PD and VCO, respectively.\nUsing a digital divider in the feedback path to only pass every \\(N\\)-th cycle of the VCO output to the PD allows the PLL to generate output frequencies that are integer multiples of the reference frequency. This is particularly useful in applications such as frequency synthesis, where a wide range of frequencies is required from a single stable reference source. By adjusting the division ratio \\(N \\in \\mathbb{N}\\), the PLL can produce various output frequencies while maintaining phase lock with the reference signal. However, note that the frequency resolution of the output frequencies \\(\\Delta f_\\mathrm{out}\\) is limited to integer multiples of the reference frequency!\nPutting everything together into an \\(s\\)-domain block diagram, the PLL can be modeled as shown in Figure 71.\n\n\n\n\n\n\n\n\nFigure 71: Laplace domain model of a PLL. Note that input and output signals are (excess) phase in rad.\n\n\n\n\n\nFor the loop filter transfer function \\(H(s)\\) we assume a simple first-order low-pass filter with a cutoff frequency of \\(\\omega_\\mathrm{LP} = 1 / T_\\mathrm{LP}\\) given by\n\\[\nH_\\mathrm{LP}(s) = \\frac{1}{1 + s T_\\mathrm{LP}}.\n\\]\n\n\n\n\n\n\nImportantWhy Regulate Phase Instead of Frequency (PLL vs. FLL)?\n\n\n\nBefore we proceed, it is worth discussing why PLLs regulate the phase of the VCO output rather than its frequency directly. The reason lies in the relationship between phase and frequency: Frequency is the time derivative of phase. By controlling the phase, the PLL inherently controls the frequency as well. Additionally, even if the phase regulation has a steady-state phase error \\(\\varphi_\\mathrm{err}\\), the frequency error in steady state is zero, as can be appreciated when inspecting\n\\[\n\\omega_\\mathrm{out}(t) = \\frac{d \\varphi_\\mathrm{out}(t)}{dt}  = \\frac{d}{dt} \\left[ N \\cdot \\varphi_\\mathrm{ref}(t) + \\varphi_\\mathrm{err} \\right] = N \\cdot \\frac{d \\varphi_\\mathrm{ref}(t)}{dt} = N \\cdot \\omega_\\mathrm{ref}(t).\n\\]\nIn principle, one could also design a frequency-locked loop (FLL) that compares the frequencies of the reference and VCO output signals directly (by, e.g., using counters), however, this usually results in non-zero frequency error in steady state. Nevertheless, FLLs are often used in combination with PLLs to improve acquisition time and robustness, especially in scenarios with large initial frequency offsets.\n\n\nWe now derive the closed-loop transfer function of the PLL from reference phase \\(\\varphi_\\mathrm{ref}(s)\\) to output phase \\(\\varphi_\\mathrm{out}(s)\\). First, we note that\n\\[\nV_\\mathrm{PD}(s) = K_\\mathrm{PD} \\left[ \\varphi_\\mathrm{ref}(s) - \\frac{\\varphi_\\mathrm{out}(s)}{N} \\right]\n\\]\nand\n\\[\n\\varphi_\\mathrm{out}(s) = \\frac{K_\\mathrm{VCO}}{s} \\cdot H_\\mathrm{LP}(s) \\cdot V_\\mathrm{PD}(s) = \\frac{K_\\mathrm{VCO}}{s} \\cdot \\frac{1}{1 + s T_\\mathrm{LP}} \\cdot K_\\mathrm{PD} \\left[ \\varphi_\\mathrm{ref}(s) - \\frac{\\varphi_\\mathrm{out}(s)}{N} \\right].\n\\]\nNote that this equation has one integrator \\(1/s\\) (which we can also see in Figure 71). We call PLLs with one integrator in the loop a Type-I PLL. We can now rearrange to find the closed-loop transfer function \\(H(s)\\) as\n\\[\nH(s) = \\frac{\\varphi_\\mathrm{out}(s)}{\\varphi_\\mathrm{ref}(s)} = N \\cdot \\frac{K_\\mathrm{VCO} \\cdot K_\\mathrm{PD} \\cdot \\omega_\\mathrm{LP} / N}{s^2 + s \\cdot \\omega_\\mathrm{LP} + K_\\mathrm{VCO} \\cdot K_\\mathrm{PD} \\cdot \\omega_\\mathrm{LP} / N}.\n\\tag{44}\\]\nWe can use all techniques known from control theory to analyze the stability and dynamic response of the PLL based on Equation 44, however, we have to keep the following in mind:\n\n\\(H(s)\\) is a small-signal model of the PLL around its locked operating point, only valid for small perturbations. Large-signal behavior, such as acquisition and lock range, are not captured by this model.\nA PLL is a sampled system, working at instances of \\(T_\\mathrm{ref}\\). \\(H(s)\\) is a continuous-time approximation, which is valid only if the loop bandwidth is much smaller (typically \\(1/10\\)) than the reference frequency \\(f_\\mathrm{ref}\\). If this approximation is not valid, a discrete-time model of the PLL must be used, deriving the \\(z\\)-domain transfer function \\(H(z)\\).\n\nIf we compare the canonical form of a second-order system given by\n\\[\nH(s) = K \\cdot \\frac{\\omega_\\mathrm{n}^2}{s^2 + s \\cdot 2 \\zeta \\omega_\\mathrm{n} + \\omega_\\mathrm{n}^2},\n\\]\nwith Equation 44, we can identify the natural frequency \\(\\omega_n\\) and the damping factor \\(\\zeta\\) of the PLL as\n\\[\n\\omega_\\mathrm{n} = \\sqrt{\\frac{K_\\mathrm{VCO} \\cdot K_\\mathrm{PD} \\cdot \\omega_\\mathrm{LP}}{N}}\n\\tag{45}\\]\nand\n\\[\n\\zeta = \\frac{1}{2} \\sqrt{\\frac{\\omega_\\mathrm{LP} \\cdot N}{K_\\mathrm{VCO} \\cdot K_\\mathrm{PD}}}.\n\\tag{46}\\]\nThis allows us to use standard control theory results to design the PLL parameters \\(K_\\mathrm{PD}\\), \\(K_\\mathrm{VCO}\\), \\(\\omega_\\mathrm{LP}\\), and \\(N\\) to achieve the desired dynamic response and stability margins. Note that \\(\\omega_\\mathrm{n}\\) and \\(\\zeta\\) are not independent, as they both depend on the same set of PLL parameters! This seriously limits the design of the loop dynamics and means that we need a more complex loop filter \\(H_\\mathrm{LP}(s)\\) to achieve more freedom in selecting \\(\\omega_\\mathrm{n}\\) and \\(\\zeta\\) independently. Often, we want to choose \\(\\zeta = 1/\\sqrt{2} \\approx 0.707\\) to achieve a Butterworth response for a good step response without overshoot.\nThe poles of a second-order system are given by\n\\[\np_{1,2} = -\\omega_\\mathrm{n} \\left( \\zeta \\pm \\sqrt{\\zeta^2 - 1} \\right)\n\\]\nwith \\(\\zeta = 1/\\sqrt{2}\\) resulting in complex conjugate poles at\n\\[\np_{1,2} = -\\frac{\\omega_\\mathrm{n}}{\\sqrt{2}} \\pm j \\frac{\\omega_\\mathrm{n}}{\\sqrt{2}}.\n\\]\nTo summarize: We can make a simple PLL using an XOR for a PD, use a first-order LPF, plus a feedback divider to get frequency multiplication of \\(f_\\mathrm{out} = N \\cdot f_\\mathrm{ref}\\). However, the control over the loop dynamics is very limited, and the locking procedure of the PLL is tricky, as the PD has a limited linear range of ±180°, requiring that the VCO frequency at the start of the locking procedure is already close enough to the desired output frequency. In addition, \\(N\\) is an integer, limiting the frequency resolution of the output frequencies to integer multiples of the reference frequency.\nThe question is how to improve the PLL design to overcome these limitations? We will explore advanced PLL architectures and techniques in the following section."
  },
  {
    "objectID": "rfic.html#sec-pll-charge-pump",
    "href": "rfic.html#sec-pll-charge-pump",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "7.2 Charge-Pump PLL",
    "text": "7.2 Charge-Pump PLL\nThe fundamental limitations of the simple PLL architecture presented in Section 7.1 can be addressed by using a more sophisticated phase detector known as a phase-frequency detector (PFD) in combination with a charge pump (CP) (Gardner 1980). This combination allows for better control over the loop dynamics and improved locking behavior.\nIn order to drive the CP, we envision a PFD with two digital outputs: An “up” signal that indicates when the VCO phase lags behind the reference phase, and a “down” signal that indicates when the VCO phase leads the reference phase (Brown 1971). The PFD compares both the phase and frequency of the reference and feedback signals, providing a more robust locking mechanism. The typical implementation of a PFD is shown in Figure 72.\n\n\n\n\n\n\n\n\nFigure 72: Implementation of a phase-frequency detector.\n\n\n\n\n\nThe PFD operates as follows:\n\nIf the reference signal leads the feedback signal (its rising edge arrives first), the “UP” output is activated, causing the CP to source current into the integration capacitor, increasing the VCO tuning voltage and thus the VCO frequency.\nIf the feedback signal leads the reference signal (its rising edge arrives first), the “DOWN” output is activated, causing the CP to sink current from the integration capacitor, decreasing the VCO tuning voltage and thus the VCO frequency.\nIf both signals are aligned in phase and frequency, both outputs are only activated for a short time (the gate delays of the DFF and the AND), and the CP does not source or sink current; the integration capacitor in the loop filter holds its voltage, maintaining the VCO frequency.\nThe rising edge that arrives last resets both outputs, ensuring that the CP only sources or sinks current for a duration proportional to the phase difference between the two signals.\n\nIn the following, we will look at four different cases of PFD operation to illustrate its behavior.\n\nReference leads feedback: The “UP” output is activated, and the CP sources current to the loop filter, increasing the VCO frequency.\n\n\n\n\n\n\n\n\n\nFigure 73: Reference leading the VCO feedback signal, frequencies are already aligned.\n\n\n\n\n\n\nFeedback leads reference: The “DOWN” output is activated, and the CP sinks current from the loop filter, decreasing the VCO frequency.\n\n\n\n\n\n\n\n\n\nFigure 74: VCO feedback signal leading the reference, frequencies are already aligned.\n\n\n\n\n\n\nBoth signals aligned: Both outputs are briefly activated, but the CP does not source or sink current, maintaining the VCO frequency.\n\n\n\n\n\n\n\n\n\nFigure 75: VCO feedback and reference signal are aligned in phase and frequency.\n\n\n\n\n\n\nLarge frequency difference: The PFD continues to source or sink current until the phases align, allowing the PLL to acquire lock even with large initial frequency offsets.\n\n\n\n\n\n\n\n\n\nFigure 76: VCO feedback and reference signal have different frequencies (the VCO frequency is too high). There will be some periods with a wrong output (as shown here in the first cycle), but on average the phase-frequency detector output will indicate that the VCO frequency must be decreased.\n\n\n\n\n\nLet us now investigate the circuit of the CP. The UP and DOWN signals from the PFD control two current sources/sinks connected to the loop filter capacitor, as shown in Figure 77.\n\n\n\n\n\n\n\n\nFigure 77: Charge pump consisting of two matched current sources and switches. The resistor in series with the capacitor introduces a zero.\n\n\n\n\n\nWhen the UP signal is high, the CP sources a constant current \\(I_\\mathrm{CP}\\) into the loop filter capacitor \\(C_\\mathrm{int}\\), increasing its voltage linearly over time according to the phase error. Conversely, when the DOWN signal is high, the CP sinks a constant current \\(I_\\mathrm{CP}\\) from the capacitor, decreasing its voltage linearly.\nNote that the top and bottom current sources must be well matched to ensure symmetric operation of the CP and should have a wide operating range. The output impedance of the CP is ideally very high so that the charge pump current does not depend on the voltage across the loop filter capacitor. Further, the switches introduce charge injection and clock feedthrough, which can cause unwanted ripples on the loop filter voltage, leading to spurious signals in the PLL output. Careful design of the switches and layout techniques are necessary to minimize these effects.\nThe voltage change on the capacitor can be expressed as\n\\[\n\\Delta V_\\mathrm{LF} = \\frac{1}{C_\\mathrm{LF}} \\int_0^{t_\\mathrm{on}} I_\\mathrm{int} \\cdot dt\n\\tag{47}\\]\nwhere \\(t_\\mathrm{on}\\) is the duration for which the UP or DOWN signal is active, proportional to the phase difference between the reference and feedback signals.\nTaking the Laplace transform of Equation 47, we find the transfer function of the CP from the UP/DOWN control signals to the loop filter voltage as\n\\[\nH_\\mathrm{CP}(s) = \\frac{I_\\mathrm{CP}}{2 \\pi} \\left( \\frac{1}{s C_\\mathrm{int}} + R_\\mathrm{int} \\right)\n\\tag{48}\\]\nwhere we have added a resistor \\(R_\\mathrm{int}\\) in series with the capacitor \\(C_\\mathrm{int}\\) to introduce a zero in the loop filter transfer function, clearly showing the zero introduced by the resistor \\(R_\\mathrm{int}\\) in series with the capacitor \\(C_\\mathrm{int}\\). The need for this zero will become apparent when we analyze the PLL loop dynamics next.\nThe resulting PLL using a PFD-CP combination is shown in Figure 78.\n\n\n\n\n\n\n\n\nFigure 78: Diagram of a PFD-CP PLL. Note that the capacitor \\(C_3\\) has been added to reduce the CP switching ripple, and its presence makes the loop filter 3rd order.\n\n\n\n\n\nNeglecting \\(C_3\\) for now, we can derive the closed-loop transfer function of the PLL from reference phase \\(\\varphi_\\mathrm{ref}(s)\\) to output phase \\(\\varphi_\\mathrm{out}(s)\\) as\n\\[\nH(s) = \\frac{\\varphi_\\mathrm{out}(s)}{\\varphi_\\mathrm{ref}(s)} = N \\cdot \\frac{\\frac{I_\\mathrm{CP} K_\\mathrm{VCO}}{2 \\pi C_\\mathrm{int} N} (1 + s C_\\mathrm{int} R_\\mathrm{int})}{s^2 + s \\frac{I_\\mathrm{CP} K_\\mathrm{VCO} R_\\mathrm{int}}{2 \\pi N} + \\frac{I_\\mathrm{CP} K_\\mathrm{VCO}}{2 \\pi C_\\mathrm{int} N}} = N \\cdot \\frac{s \\cdot 2 \\zeta \\omega_\\mathrm{n} + \\omega_\\mathrm{n}^2}{s^2 + s \\cdot 2 \\zeta \\omega_\\mathrm{n} + \\omega_\\mathrm{n}^2}\n\\tag{49}\\]\nwith\n\\[\n\\omega_\\mathrm{n} = \\sqrt{\\frac{I_\\mathrm{CP} K_\\mathrm{VCO}}{2 \\pi C_\\mathrm{int} N}}\n\\tag{50}\\]\nand\n\\[\n\\zeta = \\frac{R_\\mathrm{int}}{2} \\sqrt{\\frac{I_\\mathrm{CP} K_\\mathrm{VCO} C_\\mathrm{int}}{2 \\pi N}}.\n\\tag{51}\\]\nLooking at Equation 50 and Equation 51, we can see that we now have one additional independent parameter (\\(R_\\mathrm{int}\\)) to set the natural frequency \\(\\omega_\\mathrm{n}\\) and the damping factor \\(\\zeta\\) of the PLL independently (compare this to Equation 45 and Equation 46). This allows us to design the loop dynamics more flexibly to achieve the desired performance.\nWith Equation 51, we can now appreciate the need for the resistor \\(R_\\mathrm{int}\\) in series with the capacitor \\(C_\\mathrm{int}\\) in the loop filter. Without this resistor, the damping factor \\(\\zeta\\) would be zero, leading to an undamped system with oscillatory behavior and poor transient response. The resistor introduces a zero in the loop filter transfer function, providing the necessary damping to stabilize the PLL and improve its dynamic response.\nLooking at Figure 78 and Equation 48, we can also see that the PFD-CP introduces another integrator in the loop. This integrator, together with the integrator from the VCO, makes the PLL a Type-II PLL. A Type-II PLL can achieve zero steady-state phase error for step changes in the reference phase and zero steady-state frequency error for ramp changes in the reference frequency.\nIn conclusion:\n\nThe PLL regulates the phase of a VCO to match the phase of a reference signal, using feedback control. Even with a steady-state phase error, the frequency error is zero (Type-I PLL). In a Type-II PLL, both phase and frequency errors are zero in steady state.\nThe lowpass behavior of the loop filter determines the dynamic response and stability of the PLL.\nFurther, the PLL transfer function from reference phase to output phase has the form of a lowpass, which passes the reference oscillator phase noise to the output inside the passband. The phase noise of the VCO is suppressed inside the loop bandwidth, as the PLL corrects for phase deviations.\nOutside the PLL loop bandwidth, the VCO phase noise dominates, as the PLL cannot correct for phase deviations fast enough.\n\nThis phase noise shaping behavior is illustrated in Figure 79, which shows how the reference and VCO phase noise contributions combine to form the total PLL output phase noise.\n\n\n\n\n\n\n\n\nFigure 79: PLL phase noise contributions at the PLL output showing reference phase noise (increased by 20*log(N) and low-pass shaped), VCO phase noise (high-pass shaped), and total phase noise. The loop bandwidth determines the crossover frequency between reference and VCO phase noise contributions. For effective rejection of the 1/f³ VCO phase noise, a higher-order loop filter (shown is 3rd order) is beneficial.\n\n\n\n\n\nNote that inside the PLL loop bandwidth the reference phase noise is multiplied by the division ratio \\(N\\) in power, i.e., \\(20 \\log_{10}(N)\\) in dB, as the PLL output frequency is \\(N\\) times the reference frequency. In Figure 79, \\(N = 100\\) and a reference phase noise of -140 dBc/Hz is used. Large values of \\(N\\) can significantly increase the reference phase noise contribution at the PLL output, which is why low phase noise references are essential for high-performance PLLs."
  },
  {
    "objectID": "rfic.html#sec-pll-all-digital",
    "href": "rfic.html#sec-pll-all-digital",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "7.3 All-Digital PLL",
    "text": "7.3 All-Digital PLL\nIn the PLL implementations discussed so far, we have used analog components such as the VCO, loop filter, and charge pump. While these analog PLLs can achieve excellent performance, they also come with challenges such as component variability, temperature sensitivity, and integration complexity in modern digital processes. The question arises: Can we implement a PLL using only digital components to overcome these challenges? The answer is yes, leading to the concept of the all-digital phase-locked loop (ADPLL) (Staszewski et al. 2003), where a block diagram is shown in Figure 80.\n\n\n\n\n\n\n\n\nFigure 80: Block diagram of an all-digital PLL. A reference phase signal is compared to the phase of a digitally-controlled oscillator (DCO) output signal in a time-to-digital converter (TDC). The phase error word is digitally low-pass filtered and used to tune the DCO frequency via a frequency control word.\n\n\n\n\n\nConceptually, the ADPLL in Figure 80 is similar to the analog PLL in Figure 69, with the key difference being that all components are implemented digitally. The phase detector is replaced by a time-to-digital converter (TDC), which measures the time difference between the reference and feedback signal edges and converts it into a digital phase error word. The loop filter is implemented as a digital filter (FIR, IIR, or similar), processing the phase error word to generate a tuning word for the digitally controlled oscillator (DCO). The DCO generates the output frequency based on the tuning word, completing the feedback loop.\nThe advantages of the ADPLL are that it can be fully integrated in a digital nm CMOS process, benefiting from scalability, low power consumption, small chip area, and immunity to analog component variations. Additionally, the digital nature of the ADPLL allows for easy programmability and adaptability to different applications. However, the ADPLL also faces challenges such as quantization noise from the TDC and DCO, nonlinearity (DNL and INL) of TDC and DCO, as well as coupling of digital switching noise into the oscillator output signal.\n\n7.3.1 Time-to-Digital Converter\nThe TDC is a crucial component of the ADPLL, responsible for measuring the time difference between the reference and feedback signal edges and converting it into a digital phase error word. The TDC operates by sampling the time interval between two events (the rising edges of the reference and feedback signals) and quantizing this interval into discrete levels. TDCs can be implemented using various techniques. One popular method is using a digital delay line. An exemplary implementation is shown in Figure 81.\n\n\n\n\n\n\n\n\nFigure 81: Basic TDC implementation as a delay line with parallel capture flip-flops. The TDC delay resolution is limited two inverter delays.\n\n\n\n\n\nIn Figure 81, the reference signal edge runs through a series of buffers (delay elements) connected in a delay line configuration. The feedback signal edge is used to sample the state of the delay line at the moment it arrives. The outputs of the DFFs represent the time difference between the reference and feedback signals, which can be encoded into a digital phase error word. The resolution of the TDC is determined by the delay between each stage in the delay line, with finer delays providing higher resolution. However, increasing the resolution also increases the complexity and power consumption of the TDC.\nThe phase noise of the TDC (at the output of the ADPLL) arising from this time measurement quantization can be modeled as\n\\[\n\\mathcal{L}\\left\\{f\\right\\} = 10 \\cdot \\log \\left[ \\frac{(2 \\pi)^2}{12 f_\\mathrm{ref}} \\cdot \\left(  \\frac{\\Delta T_\\mathrm{TDC}}{T_\\mathrm{DCO}} \\right)^2 \\right]\n\\]\nwhere \\(\\Delta T_\\mathrm{TDC}\\) is the time resolution of the TDC, \\(T_\\mathrm{DCO}\\) is the period of the DCO output frequency, and \\(f_\\mathrm{ref}\\) is the reference frequency. This equation shows that improving the TDC resolution (reducing \\(\\Delta T_\\mathrm{TDC}\\)) and increasing the reference frequency can help reduce the phase noise contribution from the TDC.\n\n\n\n\n\n\nNoteTDC Resolution Example\n\n\n\nTo get an idea of the performance requirements for the TDC, we assume a DCO output frequency of 2.4 GHz (e.g., for a Bluetooth application) and a reference frequency of 40 MHz. If we aim for a TDC phase noise contribution of -100 dBc/Hz at the DCO output, we can rearrange the above equation to find the required TDC time resolution:\n\\[\n\\Delta T_\\mathrm{TDC} = T_\\mathrm{DCO} \\cdot \\sqrt{\\frac{12 f_\\mathrm{ref} \\cdot 10^{\\mathcal{L}\\left\\{f\\right\\}/10}}{(2 \\pi)^2}} = \\frac{1}{2.4 \\times 10^9} \\cdot \\sqrt{\\frac{12 \\cdot 40 \\times 10^6 \\cdot 10^{-10}}{(2 \\pi)^2}} \\approx 15\\,\\text{ps}\n\\]\nThis number is challenging but achievable with modern TDC designs in nm CMOS.\n\n\n\n\n7.3.2 Digitally Controlled Oscillator\nWith the implementation of the TDC clarified in Section 7.3.1, we now turn our attention to the digitally controlled oscillator (DCO), which generates the output frequency of the ADPLL based on a digital tuning word. The DCO is a digital equivalent of the VCO used in analog PLLs, and its frequency is adjusted by changing the digital input value. Fundamentally, two possible implementations exist:\n\nAn analog-controlled oscillator (e.g., a VCO) is combined with a digital-to-analog converter (DAC) to convert the digital tuning word into an analog control voltage. This approach is shown in Figure 82.\n\n\n\n\n\n\n\n\n\nFigure 82: Digitally controlled oscillator using a DAC.\n\n\n\n\n\n\nA digitally-controlled oscillator uses a large number of small varactors or switched capacitor banks to adjust the oscillation frequency directly based on the digital tuning word. An example implementation of a switched varactor is shown in Figure 83, or a switched capacitor like shown in Figure 67 is used.\n\n\n\n\n\n\n\n\n\nFigure 83: A switched differential varactor used for fine frequency control in a DCO.\n\n\n\n\n\nIt has to be noted that while an ADPLL seems conceptually simpler than an analog PLL, the design of high-performance TDCs and DCOs can be quite complex and requires careful consideration of quantization noise, linearity, and power consumption. Coupling effects between different circuit blocks can also introduce unwanted spurs and phase noise folding in the ADPLL output. Therefore, while ADPLLs offer many advantages, they also present unique design challenges that must be addressed to achieve adequate performance."
  },
  {
    "objectID": "rfic.html#sec-pll-frac-n",
    "href": "rfic.html#sec-pll-frac-n",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "7.4 Fractional-N PLL",
    "text": "7.4 Fractional-N PLL\nWith the PLL architectures discussed so far, the output frequency resolution is limited to integer multiples of the reference frequency, i.e., \\(f_\\mathrm{out} = N \\cdot f_\\mathrm{ref}\\) with \\(N \\in \\mathbb{N}\\). For a fine output frequency resolution, this requires a very low reference frequency. With the rule of thumb that the PLL loop bandwidth should be less than \\(f_\\mathrm{ref}/10\\) to ensure stable operation, a low reference frequency also limits the achievable loop bandwidth, which is detrimental to phase noise performance and settling time. Further, we have learned in Section 7.2 that large values of \\(N\\) increase the reference phase noise contribution at the PLL output by \\(20 \\log_{10}(N)\\) dB, making it challenging to achieve low phase noise performance with high frequency resolution.\nHow to break these fundamental limitations? The answer lies in the concept of the fractional-N PLL, which allows for non-integer frequency multiplication.\nAssume we want to generate an output frequency of \\(f_\\mathrm{out} = 2.45\\,\\text{GHz}\\) from a reference frequency of \\(f_\\mathrm{ref} = 40\\,\\text{MHz}\\). This requires a frequency multiplication factor of \\(N = 61.25\\), which is not an integer. To achieve this, the fractional-N PLL employs a feedback divider that can switch between two integer division ratios, \\(N_1\\) and \\(N_2\\), such that the average division ratio over time equals the desired fractional value (the averaging is performed by the lowpass loopfilter, providing the averaged tuning voltage to the VCO / the tuning word to the DCO). In this case, we can choose \\(N_1 = 61\\) and \\(N_2 = 62\\). By alternating between these two division ratios in a controlled manner, the PLL can effectively achieve an average division ratio of 61.25, allowing it to generate the desired output frequency of 2.45 GHz. The following sequence illustrates this concept over four reference cycles:\n\\[\nN = N_1, N_1, N_1, N_2 \\implies \\text{Average } N = \\frac{61 + 61 + 61 + 62}{4} = 61.25\n\\]\nA simple sequence like the one above works, but would lead to significant spurs in the PLL output spectrum due to the periodic nature of the division ratio switching, especially when the fractionality is close to an integer, and thus the sequence is long. Consider the case of \\(N = 61.01\\), where we would need to switch to \\(N_2 = 62\\) only once every 100 cycles of \\(N_1 = 61\\):\n\\[\nN = \\frac{99 \\cdot 61 + 1 \\cdot 62}{100} = 61.01\n\\]\nThis periodic switching introduces spurious tones in the PLL output spectrum at multiples of \\(f_\\mathrm{ref}/100\\), which can potentially pass through the loop filter and appear in the output spectrum, degrading the signal quality.\nTo minimize these spurs, more sophisticated techniques such as delta-sigma modulation are employed to randomize the switching sequence, spreading the quantization noise over a wider frequency range and reducing its impact on the PLL output. Additionally, the quantization noise introduced by the fractional division is pushed to higher frequencies where it can be more easily filtered out by the loop filter.\n\n7.4.1 Delta-Sigma Modulator\nFor an in-depth understanding of delta-sigma modulation, we refer to (Schreier, Pavan, and Temes 2017). Here, we briefly summarize the key concepts relevant to fractional-N PLLs. In a nutshell, a delta-sigma modulator (DSM) is a feedback system that shapes quantization noise to higher frequencies, allowing for high-resolution digital-to-analog conversion or frequency synthesis. The basic structure of a first-order DSM is shown in Figure 84.\n\n\n\n\n\n\n\n\nFigure 84: A first-order continuous time delta-sigma modulator.\n\n\n\n\n\nIn Figure 84, the input signal \\(X(s)\\) is transferred to the output \\(Y(s)\\) with the following transfer function (we set \\(Q = 0\\)):\n\\[\nY(s) = X(s) \\cdot \\frac{1}{1 + s T} = X(s) \\cdot H_\\mathrm{LP}(s)\n\\]\nThe quantization noise \\(Q(s)\\) introduced by the quantizer is shaped by the feedback loop, resulting in the following output contribution:\n\\[\nY(s) = Q(s) \\cdot \\frac{s T}{1 + s T} = Q(s) \\cdot H_\\mathrm{HP}(s)\n\\]\nWe can see that the quantization noise is highpass filtered, pushing it to higher frequencies, where it can be more easily filtered out by a subsequent lowpass filter.\nThe digital implementation of a first-order DSM is straightforward and one possible form is shown in Figure 85. At the point of truncation from \\(N+2\\) bits to 1 bit (the MSB) we imagine quantization noise \\(q[n]\\) being added to the signal.\n\n\n\n\n\n\n\n\nFigure 85: A first-order digital delta-sigma modulator.\n\n\n\n\n\nCalculating the \\(z\\)-domain transfer functions from input \\(X(z)\\) to the output \\(Y(z)\\) (the signal transfer function, STF) and quantization noise \\(Q(z)\\) to \\(Y(z)\\) (the noise transfer function, NTF), we find\n\\[\n\\frac{Y(z)}{X(z)} = \\text{STF}(z) = z^{-1} \\implies \\text{STF}(z) = z^{-1}\n\\]\nand\n\\[\n\\frac{Y(z)}{Q(z)} = \\text{NTF}(z) = (1 - z^{-1}) \\implies \\text{NTF} = 1 - z^{-1}.\n\\]\nWe see that the STF is a simple delay, while the NTF has a zero at DC, pushing the quantization noise to higher frequencies. Taking the Laplace transform of the NTF to find the frequency shaped PSD of the quantization noise, we find\n\\[\nS_\\mathrm{y}(f) = S_\\mathrm{q}(f) |H(f)|^2 = S_\\mathrm{q}(f) \\cdot 2 \\left| 1 - \\cos(2 \\pi f T_\\mathrm{s}) \\right| \\implies H(f) = \\sqrt{ 2 \\left| 1 - \\cos(2 \\pi f T_\\mathrm{s}) \\right| }.\n\\]\nwith \\(T_\\mathrm{s}\\) being the sampling period of the DSM. The resulting \\(H(f)\\) is shown in Figure 86. With the assumed PLL loop bandwidth of 100 kHz and a reference frequency of 40 MHz, we can see that the quantization noise is significantly attenuated inside the PLL loop bandwidth, minimizing its impact on the PLL output phase noise. The general trend is that higher-order DSMs and faster sampling frequencies lead to better noise shaping and lower in-band quantization noise.\n\n\n\n\n\n\n\n\nFigure 86: NTF H(f) of a first-order delta-sigma modulator showing the high-pass noise shaping characteristic. The quantization noise is suppressed at low frequencies and increases towards the Nyquist frequency. An exemplary loop filter bandwidth of 300 kHz for a 40 MHz reference frequency is indicated.\n\n\n\n\n\nIf we analyze the signal properties of the number sequence generated by the first-order DSM in Figure 85, we find that its randomization properties are not ideal, leading to spurs in the PLL output spectrum. To improve the randomization properties, higher-order DSMs are used in fractional-N PLLs, especially 2nd or 3rd order. A consequence of a higher-order DSM is that the produced sequence can take more values than just \\(N\\) and \\(N+1\\). The range of produced values is shown in Table 5.\n\n\n\nTable 5: DSM output ranges and characteristics for different orders\n\n\n\n\n\nDSM Order\nOutput Range\nNoise Shaping\n\n\n\n\n1\n\\(N, N+1\\)\n20 dB/decade\n\n\n2\n\\(N-1 \\ldots N+2\\)\n40 dB/decade\n\n\n3\n\\(N-3 \\ldots N+4\\)\n60 dB/decade\n\n\n\n\n\n\nA standard 3rd-order DSM implementation has issues with stability due to the phase shift of three integrators in the feedback loop. To mitigate this, a well-known architecture called the MASH (multi-stage noise shaping) DSM is often used (Matsuya et al. 1989), which consists of multiple first-order DSM stages cascaded together. An example Python implementation of a 3rd-order MASH DSM is shown in Figure 87, showing a snippet of the time-domain output and the resulting frequency spectrum, showing the noise shaping. As can be clearly seen, even using a 3rd-order MASH, there are strong spurious signals showing up in the spectrum. The time series shows that for some static input values (like here 0.125) patterns with short period can exist.\n\n\n\n\n\n\n\n\n\nFigure 87: Time series and the spectrum of a MASH 3rd-order modulator.\n\n\n\n\n\nSource: MASH Modulator (3rd Order)\nAs potential solution, we can add a very small dithering signal (i.e., adding a random signal to the input), which breaks these patterns. As shown in Figure 88, dithering indeed causes enough randomization so that the noise shaping can be clearly seen without noticable spurs (this is evident when comparing the time series of Figure 87 and Figure 88). Care has to be taken that the added random noise is small enough to not cause issues with inband noise.\n\n\n\n\n\n\n\n\n\nFigure 88: Time series and the spectrum of a MASH 3rd-order modulator with added dither.\n\n\n\n\n\n\nA block diagram of a 3rd-order MASH 1-1-1 implementation including dither injection (shown in blue) is depicted in Figure 89. The dither is typically implemented digitally as a pseudo-random bit sequence (PRBS) generator, often in the form of a linear-feedback shift register (LFSR).\n\n\n\n\n\n\n\n\nFigure 89: Block diagram of a 3rd-order MASH 1-1-1 delta-sigma modulator with three cascaded first-order stages and digital noise cancellation logic including dither injection.\n\n\n\n\n\n\n\n7.4.2 Fractional-N PLL Implementation\nA block diagram of a fractional-N PLL using a DSM and a multi-modulus divider (MMD) is shown in Figure 90. In order to ease the timing loop of the jittered clock coming from the MMD, the DSM is typically clocked by the MMD’s output.\n\n\n\n\n\n\n\n\nFigure 90: Block diagram of a fractional-N PLL. The VCO signal is divided by a sequence of integer divider values div[k] with (on average) is given by \\(N.f\\), where \\(N\\) is the integer part of the divider value, and \\(f\\) the fractional part of the divider.\n\n\n\n\n\nFor a practical implementation of a fractional-N PLL it is of utmost importance that the path from the MMD output to the loop filter output is linear, as only the averaging/lowpass filtering of the noise-shaped quantization noise reduces the noise. Any nonlinearity in this path will lead to distortion of the quantization noise, causing quantization noise folding back into the PLL loop bandwidth, degrading the PLL output phase noise!\nOne prominent source of nonlinearity is the difference between the source and sink currents of the charge pump, as well as any nonlinearity present in the PFD/CP operating point of \\(\\Delta \\varphi \\approx 0\\) (like a deadzone). Since a Type-II PLL ideally operates at exactly this point, with random excursions around it due to the DSM-induced jitter, any nonlinearity here will seriously degrade the PLL performance. There are predominantly two options to mitigate this issue:\n\nUse a Type-I PLL architecture with \\(\\Delta \\varphi \\neq 0\\).\nIntroduce a phase offset in a Type-II PLL to operate away from \\(\\Delta \\varphi = 0\\).\n\nA simple way to introduce a phase offset is to add a fixed current source/sink to the charge pump, as shown in Figure 91.\n\n\n\n\n\n\n\n\nFigure 91: Charge pump with offset current for use in a fractional-N Type-II PLL.\n\n\n\n\n\nAs we have seen multiple times in the PLL discussion, using a higher \\(f_\\mathrm{ref}\\) is beneficial for phase noise performance, loop bandwidth, and TDC resolution (in an ADPLL). However, in many applications the choice of reference frequency is constrained by other factors. One way to achieve a higher reference frequency is by using a frequency doubler. As shown in Figure 92, a frequency doubler can be implemented using an XOR gate and a delay line. The delay line is set to a quarter of the period of the reference frequency, i.e., \\(T_\\mathrm{d} = T_\\mathrm{ref} / 4\\). The XOR gate then produces an output frequency of \\(f_\\mathrm{double} = 2 \\cdot f_\\mathrm{ref}\\). For stable operation, this delay line must be designed to be relatively insensitive to process, voltage, and temperature (PVT) variations, or must be adjusted with a control loop.\n\n\n\n\n\n\n\n\nFigure 92: Implementation of reference frequency doubler. Ideally, \\(T_\\mathrm{d}\\) should be set to \\(T_\\mathrm{ref} / 4\\).\n\n\n\n\n\nAs a final remark, we note that the output of the MMD in Figure 90 is a jittered clock due to the changing division ratio and the multiple gates and flip-flops that make up the MMD. This jitter degrades the PLL phase noise performance. Fortunately, there is an easy fix for this issue by using a retiming flip-flop after the MMD, clocked by the oscillator output, as shown in Figure 93.\n\n\n\n\n\n\n\n\nFigure 93: Block diagram of a fractional-N PLL including a retiming flip flop to reduce the MMD-related jitter.\n\n\n\n\n\nThis retiming flip-flop effectively synchronizes the MMD output to the oscillator clock domain, significantly reducing the jitter seen by the loop filter and improving the overall phase noise performance of the fractional-N PLL."
  },
  {
    "objectID": "rfic.html#sec-poweramp-classes",
    "href": "rfic.html#sec-poweramp-classes",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "8.1 Amplifier Classes",
    "text": "8.1 Amplifier Classes\nPower amplifiers can be classified into different classes based on their conduction angle, which is the portion of the input signal cycle during which the transistor conducts current. The most common classes are Class A, Class B, Class AB, Class C, Class D, Class E, and Class F. Each class has its own trade-offs between efficiency, linearity, and complexity. The following table summarizes the main characteristics of each amplifier class:\n\n\n\nTable 6: Amplifier Classes\n\n\n\n\n\n\n\n\n\n\n\n\nClass\nConduction Angle\nEfficiency\nLinearity\nDescription\n\n\n\n\nA\n360°\nLow\nHigh\nBias current is larger than peak output current\n\n\nB\n180°\nMedium\nMedium\nConducts half the cycle (zero bias current)\n\n\nAB\n180°-360°\nMedium\nMedium-high\nBetween A and B (a little bit of bias current)\n\n\nC\n&lt;180°\nHigh\nLow\nConducts less than half the cycle\n\n\nD\nN/A\nVery high\nLow-high\nSwitching amplifier, uses PWM or similar\n\n\nE\nN/A\nVery high\nLow\nSwitching amplifier with tuned load\n\n\nF\nN/A\nVery high\nLow\nUses harmonic tuning for efficiency\n\n\nG\nN/A\nVery high\nMedium\nMulti-level supply voltage for efficiency\n\n\nH\nN/A\nVery high\nMedium\nAdaptive supply voltage for efficiency\n\n\n\n\n\n\n\nClass A amplifiers are the most linear but also the least efficient, as they conduct current throughout the entire input signal cycle. They are typically used in applications where linearity is critical, such as low power analog signal processing. Drain efficiency is limited to \\(\\eta \\leq 50\\%\\).\nClass B amplifiers are more efficient than Class A, as they conduct current only during half of the input signal cycle. However, they suffer from crossover distortion at the zero-crossing point of the input signal. The upper limit for drain efficiency is \\(\\eta \\leq 78.5\\%\\).\nClass AB amplifiers are a compromise between Class A and Class B, providing better linearity than Class B while still being more efficient than Class A. They are widely used in small-signal applications and linear RF power amplifiers, where better efficiency than Class A is desired.\nClass C amplifiers are highly efficient but exhibit significant distortion, as they conduct current for only a part of the input signal cycle. They are typically used in applications where efficiency is more important than linearity, such as RF transmitters for constant-envelope modulation schemes (e.g., FM, FSK). Drain efficiency can reach up to \\(\\eta \\leq 100\\%\\), but only as the output power falls to zero.\nClass D, E, and F amplifiers are switching amplifiers that achieve very high efficiencies by operating the transistor as a switch, either fully on or fully off. They use various techniques such as pulse-width modulation (PWM), tuned loads, and harmonic tuning to optimize efficiency. These classes are commonly used in high-power applications, such as audio amplifiers and RF transmitters.\nClass G and H amplifiers use multiple supply voltages or adaptive supply voltages to improve efficiency, and can be combined with the other classes. They are often used in applications where power consumption is a concern, such as battery-powered devices. Class AB combined with Class G or H techniques is state-of-the-art in mobile device PAs. Class G and H are sometimes also referred to as “envelope tracking” techniques."
  },
  {
    "objectID": "rfic.html#sec-poweramp-advanced",
    "href": "rfic.html#sec-poweramp-advanced",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "8.2 PA Advanced Techniques",
    "text": "8.2 PA Advanced Techniques\nTo further improve the performance of power amplifiers, several advanced techniques can be employed:\n\nPredistortion: This technique involves applying an inverse distortion to the input signal before it enters the PA, effectively canceling out the nonlinearity of the PA. Digital predistortion (DPD) is commonly used in modern communication systems to improve linearity and reduce spectral regrowth.\nEnvelope Tracking: This technique dynamically adjusts the supply voltage of the PA based on the envelope of the input signal, improving efficiency by reducing power consumption during low signal levels (see Class G and Class H).\nLoad Modulation: This technique involves varying the load impedance seen by the PA to optimize performance for different signal conditions. This can be achieved using tunable matching networks or switched load networks. An active method is the Doherty amplifier architecture, which uses two amplifiers (a main and a peaking amplifier) to improve efficiency at back-off power levels."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Radio-Frequency Integrated Circuits",
    "section": "",
    "text": "Radio-Frequency Integrated Circuits\n\n\n\nQuarto Publish\n\n\n(c) 2025 Harald Pretl and co-authors, Department for Integrated Circuits (ICD), Johannes Kepler University, Linz (JKU)\nThis is the material for a graduate-level radio-frequency integrated circuit course, held at JKU under course number 336.023 (“VO Integrierte Hochfrequenz-Schaltungstechnik”). Follow this link to access the material.\nAll course material is made publicly available and shared under the Apache-2.0 license.\nWe happily accept pull requests to fix typos or add content! If you want to discuss something that is not clear, please open an issue!"
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html",
    "href": "content/pll/delta_sigma_modulator.html",
    "title": "MASH Modulator (3rd Order)",
    "section": "",
    "text": "Copyright (C) 2026 Harald Pretl and co-authors (harald.pretl@jku.at)\nLicensed under the Apache License, Version 2.0\nThis notebook implements a 3rd order MASH (1-1-1) modulator that converts a constant input value (0-1) into a multi-bit sequence. The output shows the time-domain sequence, individual stage outputs, and its spectrum via FFT."
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html#import-libraries",
    "href": "content/pll/delta_sigma_modulator.html#import-libraries",
    "title": "MASH Modulator (3rd Order)",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html#mash-modulator-class",
    "href": "content/pll/delta_sigma_modulator.html#mash-modulator-class",
    "title": "MASH Modulator (3rd Order)",
    "section": "MASH Modulator Class",
    "text": "MASH Modulator Class\nThe MASH (Multi-stAge noise SHaping) modulator consists of three cascaded 1st-order stages with digital noise cancellation logic.\n\nclass MASHModulator:\n    \"\"\"3rd order MASH (Multi-stAge noise SHaping) modulator\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the 3-stage MASH modulator.\"\"\"\n        # Three 1st order stages\n        self.integrator1 = 0.0\n        self.integrator2 = 0.0\n        self.integrator3 = 0.0\n        \n        # Storage for digital cancellation logic (need 2 delays)\n        self.y1_prev = 0\n        self.y1_prev2 = 0\n        self.y2_prev = 0\n        self.y2_prev2 = 0\n        self.y3_prev = 0\n        self.y3_prev2 = 0\n    \n    def reset(self):\n        \"\"\"Reset all states to zero\"\"\"\n        self.integrator1 = 0.0\n        self.integrator2 = 0.0\n        self.integrator3 = 0.0\n        self.y1_prev = 0\n        self.y1_prev2 = 0\n        self.y2_prev = 0\n        self.y2_prev2 = 0\n        self.y3_prev = 0\n        self.y3_prev2 = 0\n    \n    def step(self, input_value):\n        \"\"\"\n        Perform one step of MASH modulation.\n        \n        Parameters:\n        -----------\n        input_value : float\n            Input signal value (0-1)\n        \n        Returns:\n        --------\n        tuple\n            (combined_output, y1, y2, y3) - combined output and individual stage outputs\n        \"\"\"\n        # Stage 1: 1st order modulator with input signal\n        self.integrator1 += input_value\n        y1 = 1 if self.integrator1 &gt;= 1.0 else 0\n        e1 = self.integrator1 - y1  # Quantization error\n        self.integrator1 = e1  # Store error for next iteration\n        \n        # Stage 2: 1st order modulator fed with quantization error from stage 1\n        self.integrator2 += e1\n        y2 = 1 if self.integrator2 &gt;= 1.0 else 0\n        e2 = self.integrator2 - y2\n        self.integrator2 = e2\n        \n        # Stage 3: 1st order modulator fed with quantization error from stage 2\n        self.integrator3 += e2\n        y3 = 1 if self.integrator3 &gt;= 1.0 else 0\n        e3 = self.integrator3 - y3\n        self.integrator3 = e3\n        \n        # Digital noise cancellation logic for MASH 1-1-1\n        # Output = Y1 + (1-z^-1)*Y2 + (1-z^-1)^2*Y3\n        # Expanding (1-z^-1)^2 = 1 - 2*z^-1 + z^-2:\n        # Output = Y1 + Y2 - Y2[z^-1] + Y3 - 2*Y3[z^-1] + Y3[z^-2]\n        \n        combined = y1 + (y2 - self.y2_prev) + (y3 - 2*self.y3_prev + self.y3_prev2)\n        \n        # Update delay elements\n        self.y1_prev2 = self.y1_prev\n        self.y1_prev = y1\n        \n        self.y2_prev2 = self.y2_prev\n        self.y2_prev = y2\n        \n        self.y3_prev2 = self.y3_prev\n        self.y3_prev = y3\n        \n        return combined, y1, y2, y3\n    \n    def modulate(self, input_value, length, dither_amplitude=0.0, seed=None):\n        \"\"\"\n        Generate MASH modulated sequence.\n        \n        Parameters:\n        -----------\n        input_value : float\n            Constant input value (0-1)\n        length : int\n            Length of output sequence\n        dither_amplitude : float\n            Peak dither amplitude added to input (uniform in [-A, +A])\n        seed : int or None\n            RNG seed for reproducible dithering\n        \n        Returns:\n        --------\n        tuple\n            (combined_output, y1, y2, y3) - arrays of combined and individual outputs\n            Combined output range: -3 to +4 (multi-bit output)\n        \"\"\"\n        self.reset()\n        rng = np.random.default_rng(seed)\n        combined_output = np.zeros(length)\n        y1_output = np.zeros(length)\n        y2_output = np.zeros(length)\n        y3_output = np.zeros(length)\n        \n        for i in range(length):\n            dither = rng.uniform(-dither_amplitude, dither_amplitude) if dither_amplitude &gt; 0 else 0.0\n            combined, y1, y2, y3 = self.step(input_value + dither)\n            combined_output[i] = combined\n            y1_output[i] = y1\n            y2_output[i] = y2\n            y3_output[i] = y3\n        \n        return combined_output, y1_output, y2_output, y3_output"
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html#configuration-parameters",
    "href": "content/pll/delta_sigma_modulator.html#configuration-parameters",
    "title": "MASH Modulator (3rd Order)",
    "section": "Configuration Parameters",
    "text": "Configuration Parameters\nSet the input parameters for the MASH modulator simulation.\n\n# Parameters\ninput_value = 0.125  # Constant input (0-1)\nsequence_length = 2**15  # Length of sequence\ndisplay_samples = 100  # Number of samples to display in time plot\ndither_seed = 0  # RNG seed for reproducible dithering (None for random)\n\n# Dithering configurations to compare\ndither_amplitude_no_dither = 0.0  # No dithering\ndither_amplitude_with_dither = 0.000001  # With dithering\n\nprint(\"=\" * 60)\nprint(\"MASH 3rd Order Modulator - Dithering Comparison\")\nprint(\"=\" * 60)\nprint(f\"Input Value:           {input_value}\")\nprint(f\"Sequence Length:       {sequence_length}\")\nprint(f\"Dither (no):           {dither_amplitude_no_dither}\")\nprint(f\"Dither (with):         {dither_amplitude_with_dither}\")\nprint(\"=\" * 60)\n\n============================================================\nMASH 3rd Order Modulator - Dithering Comparison\n============================================================\nInput Value:           0.125\nSequence Length:       32768\nDither (no):           0.0\nDither (with):         1e-06\n============================================================"
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html#generate-mash-sequence",
    "href": "content/pll/delta_sigma_modulator.html#generate-mash-sequence",
    "title": "MASH Modulator (3rd Order)",
    "section": "Generate MASH Sequence",
    "text": "Generate MASH Sequence\nCreate the modulator instance and generate the output sequence.\n\n# Create modulator\nmash = MASHModulator()\n\n# Generate sequence WITHOUT dithering\nprint(\"\\nGenerating MASH sequence WITHOUT dithering...\")\nmash_combined_no_dither, mash_y1_no_dither, mash_y2_no_dither, mash_y3_no_dither = mash.modulate(\n    input_value,\n    sequence_length,\n    dither_amplitude=dither_amplitude_no_dither,\n    seed=dither_seed,\n)\n\n# Generate sequence WITH dithering\nprint(\"Generating MASH sequence WITH dithering...\")\nmash_combined_with_dither, mash_y1_with_dither, mash_y2_with_dither, mash_y3_with_dither = mash.modulate(\n    input_value,\n    sequence_length,\n    dither_amplitude=dither_amplitude_with_dither,\n    seed=dither_seed,\n)\n\n\nGenerating MASH sequence WITHOUT dithering...\nGenerating MASH sequence WITH dithering..."
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html#statistics",
    "href": "content/pll/delta_sigma_modulator.html#statistics",
    "title": "MASH Modulator (3rd Order)",
    "section": "Statistics",
    "text": "Statistics\nAnalyze the output statistics of the MASH modulator.\n\nprint(\"\\nMASH Modulator Statistics:\")\nprint(\"\\n--- WITHOUT Dithering ---\")\nmash_mean_no = np.mean(mash_combined_no_dither)\nmash_min_no = np.min(mash_combined_no_dither)\nmash_max_no = np.max(mash_combined_no_dither)\nmash_unique_no = np.unique(mash_combined_no_dither)\nprint(f\"  Mean output:       {mash_mean_no:.6f}\")\nprint(f\"  Deviation:         {abs(mash_mean_no - input_value):.6f}\")\nprint(f\"  Output range:      {mash_min_no:.0f} to {mash_max_no:.0f}\")\nprint(f\"  Unique values:     {len(mash_unique_no)}\")\n\nprint(\"\\n--- WITH Dithering ---\")\nmash_mean_with = np.mean(mash_combined_with_dither)\nmash_min_with = np.min(mash_combined_with_dither)\nmash_max_with = np.max(mash_combined_with_dither)\nmash_unique_with = np.unique(mash_combined_with_dither)\nprint(f\"  Mean output:       {mash_mean_with:.6f}\")\nprint(f\"  Deviation:         {abs(mash_mean_with - input_value):.6f}\")\nprint(f\"  Output range:      {mash_min_with:.0f} to {mash_max_with:.0f}\")\nprint(f\"  Unique values:     {len(mash_unique_with)}\")\n\n\nMASH Modulator Statistics:\n\n--- WITHOUT Dithering ---\n  Mean output:       0.125000\n  Deviation:         0.000000\n  Output range:      -2 to 2\n  Unique values:     5\n\n--- WITH Dithering ---\n  Mean output:       0.125000\n  Deviation:         0.000000\n  Output range:      -3 to 3\n  Unique values:     7"
  },
  {
    "objectID": "content/pll/delta_sigma_modulator.html#visualization---comparison",
    "href": "content/pll/delta_sigma_modulator.html#visualization---comparison",
    "title": "MASH Modulator (3rd Order)",
    "section": "Visualization - Comparison",
    "text": "Visualization - Comparison\nCompare MASH modulator outputs with and without dithering.\n\nfig = plt.figure()\n\ntime = np.arange(min(display_samples, len(mash_combined_no_dither)))\n\n# Plot 1: Time domain (no dither)\nax1 = plt.subplot(1, 2, 1)\nax1.step(time, mash_combined_no_dither[:display_samples], where='post', linewidth=1.5, color='C0')\nax1.axhline(y=input_value, color='r', linestyle='--', linewidth=2, \n            label=f'Expected = {input_value:.4f}')\nax1.set_xlabel('Sample Number')\nax1.set_ylabel('Output (multi-bit)')\nax1.grid(True, alpha=0.3)\nax1.legend()\nax1.set_ylim(-3.5, 4.5)\n\n# Plot 2: Frequency spectrum (no dither)\nax2 = plt.subplot(1, 2, 2)\nN = len(mash_combined_no_dither)\nfft_freq = np.fft.fftfreq(N)\npos_freq = fft_freq &gt;= 0\nfft_no = np.fft.fft(mash_combined_no_dither)\npsd_no = 20 * np.log10(np.abs(fft_no) / N + 1e-12)\nax2.plot(fft_freq[pos_freq], psd_no[pos_freq], linewidth=0.5, color='C0')\nax2.set_xlabel('Normalized Frequency (f/fs)')\nax2.set_ylabel('PSD (dB)')\nax2.grid(True, alpha=0.3)\nax2.set_xlim(0, 0.5)\nax2.set_ylim(-120, 0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Time series and the spectrum of a MASH 3rd-order modulator.\n\n\n\n\n\n\nfig = plt.figure()\n\ntime = np.arange(min(display_samples, len(mash_combined_with_dither)))\n\n# Plot 1: Time domain (with dither)\nax1 = plt.subplot(1, 2, 1)\nax1.step(time, mash_combined_with_dither[:display_samples], where='post', linewidth=1.5, color='C0')\nax1.axhline(y=input_value, color='r', linestyle='--', linewidth=2, \n            label=f'Expected = {input_value:.4f}')\nax1.set_xlabel('Sample Number')\nax1.set_ylabel('Output (multi-bit)')\nax1.grid(True, alpha=0.3)\nax1.legend()\nax1.set_ylim(-3.5, 4.5)\n\n# Plot 2: Frequency spectrum (with dither)\nax2 = plt.subplot(1, 2, 2)\nN = len(mash_combined_with_dither)\nfft_freq = np.fft.fftfreq(N)\npos_freq = fft_freq &gt;= 0\nfft_no = np.fft.fft(mash_combined_with_dither)\npsd_no = 20 * np.log10(np.abs(fft_no) / N + 1e-12)\nax2.plot(fft_freq[pos_freq], psd_no[pos_freq], linewidth=0.5, color='C0')\nax2.set_xlabel('Normalized Frequency (f/fs)')\nax2.set_ylabel('PSD (dB)')\nax2.grid(True, alpha=0.3)\nax2.set_xlim(0, 0.5)\nax2.set_ylim(-120, 0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Time series and the spectrum of a MASH 3rd-order modulator with added dither."
  }
]
::: {.content-hidden}
Copyright (C) 2025 Harald Pretl and co-authors (harald.pretl@jku.at)

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
:::

# Fundamentals {#sec-fundamentals}

In this section, we will discuss a few important concepts which will be instrumental in the further study of RF circuits and systems. As signals in RF circuits and systems are often limited on the top end by linearity, and on the bottom end by noise, we will discuss these two topics in some detail.

## Channel Capacity {#sec-channel-capacity}

In @sec-intro we have already discussed the fact that we need to pack information into a minimum bandwidth, as the available spectrum is limited. To appreciate the limits of information transfer, we need to understand how much information can be transmitted over a given bandwidth. This limit is given by the _Shannon-Hartley theorem_, which gives the maximum data rate $C$ (in bit/s) that can be transmitted over a communication channel with bandwidth $B$ (in Hz) and signal-to-noise ratio $\text{SNR}$ (with linear units):

$$
C = B \cdot \log_2(1 + \text{SNR})
$$ {#eq-shannon-hartley}

This formula gives us a theoretical upper limit on the data rate that can be achieved with a given bandwidth and SNR _under optimal conditions_. It is important to note that this limit is only achievable with ideal coding and modulation schemes, which are not practical in real-world systems. However, it provides a useful benchmark for evaluating the performance of communication systems.

::: {#nte-channel-limit .callout-note}
## Channel Capacity Example
Let us calculate the channel capacity for a system with a bandwidth of 2 MHz and an SNR of 7 dB (the BW and minimum SNR of Bluetooth LE for 1 Mbps). First, we need to convert the SNR from dB to linear units:

$$
\text{SNR} = 10^{{7}/{10}} \approx 5
$$

Now we can use @eq-shannon-hartley to calculate the channel capacity:

$$
C = B \cdot \log_2(1 + \text{SNR}) = 2\,\text{MHz} \cdot \log_2(1 + 5) = 2\,\text{MHz} \cdot 2.585 \approx 5.2\,\text{Mbps}
$$

This sounds reasonable, as the user data rate for Bluetooth LE is 1 Mbps for the given SNR, which allows for quite some overhead for coding and protocols. 
:::

## Linearity {#sec-linearity}

As we have already seen in @sec-intro-wireless the transmitter has to process large signals without distorting them, while the receiver has to process small signals in the presence of large signals. Both situations mean we need metrics and models to quantify and discuss linearity properties.

We are going to use a very simple, time-invariant model to study linearity, based on a Taylor polynomial.

::: {.callout-important}
## Linearity and Time Invariance in RF Systems
We use time invariance to simplify the mathematics. In practice, many circuits and systems will show time variant behavior which leads to quite a few very interesting and important phenomena! A time-invariant nonlinear system is also called a "memoryless" system, as the output at time $t$ only depends on the input at time $t$.

In contrast, a system with memory (i.e., time-variant) will have an output at time $t$ which depends on the input at time $t$ and also on past inputs (e.g., at times $t - \Delta T$, $t - 2 \Delta T$, etc.). Examples of systems with memory are filters, which have a frequency-dependent response, or power amplifiers with thermal memory effects.
:::

We model a nonlinear circuit block with the following Taylor polynomial:

$$
y(t) = \alpha_0 + \alpha_1 x(t) + \alpha_2 x(t)^2 + \alpha_3 x(t)^3 + \ldots
$$ {#eq-nonlinear-model}

Usually, the blocks under study will have higher order nonlinear terms but we often stop at 3rd order to keep things simple. For practical work, higher order terms should be included if necessary.

Which $x(t)$ should we use to study wireless systems? Often, the bandwidth $f_\mathrm{BW}$ of a transmit signal is much smaller than the center frequency $f_0$, i.e., $f_\mathrm{BW} \ll f_0$. In this case using a sinusoidal signal as a model is both simple to handle and approximately correct.

### Single-Tone Linearity {#sec-linearity-single}

We thus set (with $A$ being the amplitude of the input signal and $\omega = 2 \pi f$ the angular frequency)

$$
x(t) = A \cos(\omega t)
$$

and insert it into @eq-nonlinear-model. After some simple trigonometric manipulations we are at

$$
\begin{split}
y(t) &= 
\underbrace{ \frac{1}{2} \alpha_2 A^2 }_\text{dc component}
+
\underbrace{ \left(\alpha_1 A + \frac{3}{4} \alpha_3 A^3 \right) \cos(\omega t) }_\text{fundamental} \\
&+
\underbrace{ \frac{1}{2} \alpha_2 A^2 \cos(2 \omega t) }_\text{2nd harmonic}
+
\underbrace{ \frac{1}{4} \alpha_3 A^3 \cos(3 \omega t) }_\text{3rd harmonic}
\end{split}
$$ {#eq-linearity-single}

Looking at @eq-linearity-single we can make a few interesting observations:

- Even-order nonlinearity ($\alpha_2$) creates low-frequency components; it effectively adds frequency components related to the envelope $A$. If $A$ is a constant then this results in a dc term; if $A(t)$ is time variant it will create a squared version of it at low frequencies.
- The $\alpha_1$ term is the gain of the circuit block.
- Odd-order nonlinearity ($\alpha_3$) can impact the gain of the fundamental term passing through the block. Depending on the sign of $\alpha_3$ this can lead to gain contraction or expansion.
- Even- and odd-order nonlinearities create additional frequency components, so-called harmonics of the fundamental frequency. These harmonics are often unwanted, as they are far outside the wanted transmission frequency range, and need to be minimized, by either
  1. use a lowpass filter to filter these harmonics, or
  2. increase the linearity, i.e., make the $\alpha_2$, $\alpha_3$, etc., small enough.

The created harmonics are illustrated in @fig-harmonics. Note that measuring harmonics to quantify the nonlinearity metrics like $\alpha_2$ and $\alpha_3$ is often not very accurate, as these harmonics are often filtered in bandwidth-limited systems.

{{< include /content/fundamentals/_fig_harmonics.qmd >}}

How can we quantify the nonlinearity with a one-tone test? We can sweep the input signal $x(t)$ in amplitude, and observe the output $y(t)$. If the observed gain drops by 1 dB from the small-signal value we note the input power, and call this point the **1dB compression point** ($P_\mathrm{1dB}$). We should always add whether this 1dB compression point is input- or output-referred to avoid ambiguity. The diagram in @fig-1db-compression-point shows this test ($\alpha_1 = 100$, $\alpha_3 = -0.2$).

{{< include /content/fundamentals/_fig_1db_compression.qmd >}}

::: {.callout-important}
## Compressive vs. Expansive Behavior
Note that for compressive behaviour, $\alpha_3$ and $\alpha_1$ have different signs, while for expansive behaviour, they have the same sign.

At some point, every circuit block will show compressive behavior, as the maximum signal amplitude will be limited by power supply voltages, device breakdown voltages, etc.
:::

### Multi-Tone Linearity

We now elevate our investigations and apply two sinusoids with different frequencies and different amplitudes and see which signals we get at the output of the nonlinear block. The two-tone test and resulting third-order intermodulation products (IM3) are illustrated in @fig-im3-tones.

$$
x(t) = A_1 \cos(\omega_1 t) + A_2 \cos(\omega_2 t)
$$

We apply the above stimulus to our nonlinear model described by @eq-nonlinear-model and again, after some trigonometric manipulations, arrive at:

$$
y(t) = y'(t) + y''(t) + y'''(t)
$$ {#eq-linearity-double}

As many different frequency components are created by this simple two-tone test (and nonlinearity only up to 3rd order) we split the result into different equations and look at the result separately.

First, we start with the fundamental tones:

$$
\begin{split}
    y'(t) &= \left( \underbrace{\alpha_1 A_1 + \frac{3}{4} \alpha_3 A_1^3}_\text{compression/expansion} + \underbrace{\frac{3}{2} \alpha_3 A_1 A_2^2}_\text{cross-modulation/desens} \right) \cos(\omega_1 t) \\
    &+ \left( \underbrace{\alpha_1 A_2 + \frac{3}{4} \alpha_3 A_2^3}_\text{compression/expansion} + \underbrace{\frac{3}{2} \alpha_3 A_2 A_1^2}_\text{cross-modulation/desens} \right) \cos(\omega_2 t)
\end{split}
$$ {#eq-linearity-double-a}

As shown in @eq-linearity-double-a, interesting things happen:

- We (again) have the gain compression/expansion effect as already discussed in @sec-linearity-single.
- In addition, we have **cross-modulation**, i.e., the envelope of one tone (e.g., $A_2(t)$ of the tone at $\omega_2$) impacts the envelope of the other tone at $\omega_1$. This can lead to unwanted signal distortion, even if there is a large frequency separation between $\omega_1$ and $\omega_2$!
- Further, since the sign of $\alpha_3$ is usually opposite to $\alpha_1$, this can also lead to **desensitization** ("desens"). If, for example, $A_2 \gg A_1$, then there would be no compression due to the tone $\omega_1$ itself, however, the large tone at $\omega_2$ will lead to gain compression of the tone at $\omega_1$; this effect is called desense.

We now look at the next class of generated tones:

$$
\begin{split}
    y''(t) &= \frac{1}{2} \alpha_2 A_1^2 + \frac{1}{2} \alpha_2 A_2^2 \\
    &+ \alpha_2 A_1 A_2 \cos[ (\omega_1 - \omega_2) t] \\
    &+ \alpha_2 A_1 A_2 \cos[ (\omega_1 + \omega_2) t]
\end{split}
$$ {#eq-linearity-double-b}

As we can see in @eq-linearity-double-b new tones are created (besides the low frequency components we already know from the single-tone test) at the sum and difference of $\omega_1$ and $\omega_2$. These new frequency components are called "**intermodulation products of second order**" (IM2). These tones are created by the even-order nonlinearity ($\alpha_2$). These IM2 products are far away from the wanted tones, so are often not very problematic in amplifiers (but there can be exceptions!). However, they can be very problematic in frequency conversion blocks like mixers. We will come back to this point when discussing zero-IF receivers.

We now investigate the next couple of tones:

$$
\begin{split}
    y'''(t) &= \frac{3}{4} \alpha_3 A_1^2 A_2 \cos[(2 \omega_1 + \omega_2) t] \\
    &+ \frac{3}{4} \alpha_3 A_1^2 A_2 \cos[(2 \omega_1 - \omega_2) t] \\
    &+ \frac{3}{4} \alpha_3 A_1 A_2^2 \cos[(2 \omega_2 + \omega_1) t] \\
    &+ \frac{3}{4} \alpha_3 A_1 A_2^2 \cos[(2 \omega_2 - \omega_1) t]
\end{split}
$$ {#eq-linearity-double-c}

The tones shown in @eq-linearity-double-c are called "**intermodulation products of third order**" (IM3), and are caused by the odd nonlinearities (like $\alpha_3$). While the IM3 tones located at $2 \omega_1 + \omega_2$ and $\omega_1 + 2 \omega_2$ are similar to the sum IM2 tone and far away from $\omega_1$ and $\omega_2$, the other two tones are concerning.

Expressing $\Delta \omega = \omega_2 - \omega_1$ (and assuming $\omega_1 < \omega_2$), the building law of $2 \omega_1 - \omega_2 = \omega_1 - \Delta \omega$ and $2 \omega_2 - \omega_1 = \omega_2 + \Delta \omega$ results in new tones right besides $\omega_1$ and $\omega_2$, with a frequency separation only defined by $\Delta \omega$. This situation is illustrated in @fig-im3-tones.

{{< include /content/fundamentals/_fig_im3_tones.qmd >}}

This close localization of the IM3 tones can also be utilized to characterize nonlinear performance. Using gain compression or harmonic generation (H3) it can be very difficult to extract nonlinearity of third order ($\alpha_3$). However, using a two-tone test, the IM3 tones can be readily measured, even if the measured signal path shows a **bandpass characteristic**! As RF systems frequently employ bandpass filters to suppress out-of-band signals, this is a very important property of the two-tone test.

The resulting test is called a two-tone test yielding the third-order intercept point (IP3). This test is widely used in RF design to characterize the linearity of amplifiers, mixers, and complete transceiver systems. The power relationship between fundamental tones and IM3 products as a function of input power is shown in @fig-im3-power-sweep.

{{< include /content/fundamentals/_fig_im3_power_sweep.qmd >}}

Note that, as shown in @fig-im3-power-sweep, the IM3 products rise with a slope of 3 dB/dB, i.e., if the input power is increased by 1 dB, the IM3 products increase by 3 dB. The fundamental tones rise with a slope of 1 dB/dB (as long as we are in the linear region). The IP3 point is defined as the intersection of the **extrapolated** linear lines of fundamental and IM3 products. As both lines have different slopes, this intersection point is usually far outside the actual operating range of the circuit block under test!

When calculating the IIP3 (input-referred IP3) we can use the following formula, assuming equal input power per tone. It is important to always check the slope of the IM3 products to ensure that we are indeed in the third-order region! If the input power per tone is $P_\mathrm{in}$ (in dBm) and the input-referred power of one IM3 tone is $P_\mathrm{IM3}$ (in dBm), then the input-referred IP3 is given by

$$
\text{IIP3} = P_\mathrm{in} + \frac{P_\mathrm{in} - P_\mathrm{IM3}}{2}
$$ {#eq-iip3}

Further, for midly nonlinear systems (i.e., $\alpha_3$ is dominating), the IIP3 can be approximated from the 1dB compression point as

$$
\text{IIP3}|_\mathrm{dBm} \approx P_\mathrm{1dB}|_\mathrm{dBm} + 9.6\,\text{dB}.
$$ {#eq-iip3-approx}

If we have two blocks which are cascaded, and we know the gain and IIP3 of both blocks, we can calculate the overall IIP3 of the cascade with the following approximation. An exact calculation is very involved, as the nonlinearities of the first block (and the resulting tones) will be processed by the second block, creating even more tones; this process escalates very quickly. However, for practical purposes, the following approximation is often sufficient:

$$
\frac{1}{\text{IIP3}_\text{total}} \approx \frac{1}{\text{IIP3}_1} + \frac{G_1}{\text{IIP3}_2} + \frac{G_1 G_2}{\text{IIP3}_3} 
$$ {#eq-iip3-cascade}

Here $G_1$ is the linear gain of the first block, and $\text{IIP3}_1$, $\text{IIP3}_2$ are the input-referred IP3 of the first and second block, respectively. Note that all powers have to be in linear units (i.e., Watts) when using @eq-iip3-cascade. An even more simplified version of @eq-iip3-cascade can be used with all quantities given in dBm and dB, respectively:

$$
\text{IIP3}_\text{total} \approx \min \{  \text{IIP3}_1, \text{IIP3}_2 - G_1, \text{IIP3}_3 - G_1 - G_2 \}
$$ {#eq-iip3-cascade-simple}

A typical RF system cascade with multiple blocks and their individual IIP3 contributions is shown in @fig-iip3-cascade.

{{< include /content/fundamentals/_fig_iip3_cascade.qmd >}}

::: {#nte-iip3-cascade .callout-note}
## Simple IIP3 Cascade Calculation
Let's calculate the overall IIP3 of two cascaded blocks. The first block is a low-noise amplifier with an IIP3 of -10 dBm and a gain of 20 dB. The second block is a mixer that has a gain of 10 dB and an IIP3 of 5 dBm. What is the overall IIP3?

Using @eq-iip3-cascade-simple we can quickly estimate:

$$
\text{IIP3}_\text{total} \approx \min \{ -10\,\text{dBm}, 5\,\text{dBm} - 20\,\text{dB} = -15\,\text{dBm} \} = -15\,\text{dBm}
$$

We see that the overall IIP3 is limited by the linearity of the second block, as the first block amplifies all signals (including blockers) by 20 dB before they reach the second block.
:::

## Noise {#sec-noise}

Just as nonlinearity is a limiting factor for large signals, noise is the limiting factor for small signals. Noise is present in all electronic circuits and systems, and it is impossible to avoid it. However, we can try to minimize its impact on system performance.

Noise is usually characterized by its power spectral density (PSD) in units of Watts per Hertz (W/Hz). For example, thermal noise at room temperature has a PSD of approximately $k T = 4 \times 10^{-21}$ W/Hz, or --174 dBm/Hz (with the Boltzmann constant $k = 1.38 \times 10^{-23}\,\text{J/K}$). This means that if we have a bandwidth of 1 MHz, the total thermal noise power would be:

$$
P_\mathrm{thermal} = \text{PSD} \cdot B = -174\,\text{dBm/Hz} + 10 \log_{10} \left( \frac{1\,\text{MHz}}{1\,\text{Hz}} \right) = -114\,\text{dBm}
$$

The PSD of noise can be flat vs. frequency (which is called "white noise"), or can decrease with frequency (e.g., "flicker noise" or "1/f noise"). Further, noise can be generated by resistors (thermal noise), semiconductors (shot noise, generation-recombination noise), etc. A detailed discussion of noise sources can be found in [@Gray_Meyer_5th_ed] or [@Razavi_Analog_CMOS].

### Types of Noise Generation

**Resistors** generate thermal noise, which is white noise with a PSD of $4 k T R$ (in V$^2$/Hz) when looking at the voltage across the resistor, or $4 k T / R$ (in A$^2$/Hz) when looking at the current through the resistor. This noise is generated by the random thermal motion of charge carriers in the resistor.

::: {.callout-important}
## Thermal Noise
Note that the simple approximation given above is only valid for reasonably high frequencies and typical temperatures, and is known as the Rayleigh-Jeans approximation of Planck's blackbody radiation accounting for quantum effects and is given by [@Pozar_RF]

$$
\text{PSD} = \frac{4 R h f}{e^{{h f}/{k T}} - 1}
$$ 

where $h$ is the Planck constant ($h = 6.626 \times 10^{-34}$ Js) and $f$ is the frequency. The Rayleigh-Jeans approximation is valid for $f \ll k T / h$, which is approximately 6 THz at room temperature (290 K).

We can integrate the above PSD over the full frequency range and show the rms noise voltage of a resistor $R$ is bounded to

$$
\overline{v_\mathrm{n}^2} = \int_0^\infty \frac{4 R h f}{e^{{h f}/{k T}} - 1} df = \frac{2 (\pi k T)^2}{3 h} \cdot R
$$

which equates to approximately 13 mVrms noise voltage for a 1 k$\Omega$ resistor at room temperature (which is impossible to measure in practice, as there will be some form of bandwidth limitation in any real measurement setup).
:::

**MOSFETs** generate several types of noise, the most important ones being the thermal noise of the channel and flicker noise.

The thermal noise of the channel can be modeled as a current noise source between drain and source with a PSD of $\overline{I_\mathrm{n}^2} = 4 k T \gamma g_{d0}$ (in A$^2$/Hz), where $\gamma$ is a process-dependent parameter (usually between 2/3 and 2). The parameter $g_{d0}$ is the small-signal output conductance of the MOSFET in triode, i.e., $g_{d0} = \gds$, or equal to $g_{d0} = \gm$ when in saturation.

In saturation, it is often useful to express the thermal noise as a voltage noise source at the gate with a PSD of $\overline{V_\mathrm{n}^2} = 4 k T \gamma / \gm$ (in V$^2$/Hz). We can see that we can lower this noise of the MOSFET by increasing the transconductance $\gm$, which can be achieved by increasing the bias current.

In addition, at high frequencies, the MOSFET also has induced gate-current noise, which is correlated with the channel thermal noise. A detailed discussion of this noise source can be found in [@Razavi_Analog_CMOS].

Flicker noise is usually modeled as a voltage noise source at the gate with a PSD of $K_f / (\Cox W L f)$ (in V$^2$/Hz), where $K_f$ is a process-dependent parameter, $\Cox$ is the oxide capacitance per unit area, $L$ and $W$ are the length and width of the MOSFET, and $f$ is the frequency. Note that we can lower the flicker noise by increasing the area of the MOSFET ($W L$), however, this increases the parasitic capacitances associated with the MOSFET, and this often prohibitive for RF operation!

In **bipolar junction transistors (BJTs)**, the most important noise source is the shot noise due to the diffusion current in the base-emitter junction. Its PSD can be modeled as a current noise source between collector and emitter with a PSD of $2 q I_\mathrm{C}$ (in A$^2$/Hz), where $q$ is the elementary charge ($q = 1.6 \times 10^{-19}$ C) and $I_\mathrm{C}$ is the DC collector current.

::: {.callout-important}
## Equivalence of Shot and Thermal Noise
Note that it has been shown in [@Sarpeshkar_1993] that thermal noise and shot noise are actually equivalent, as both are generated by the random, thermally agitated motion of charge carriers!
:::

Ideal **capacitors** and **inductors** do not generate noise, however, real capacitors and inductors have parasitic resistances which generate thermal noise.

In RF systems additional noise sources can be present. One noteworthy example is the [**cosmic microwave background**](https://en.wikipedia.org/wiki/Cosmic_microwave_background) radiation, which can be modeled as a noise temperature of approximately 3 K. While this is negligible compared to thermal noise at room temperature (approximately 290 K), it can be significant in very low-noise systems, such as radio telescopes pointing to the sky. Another important noise source in RF systems is the **atmospheric noise**, which is generated by natural phenomena like lightning or in the ionosphere.

::: {.callout-note}
##  A Note on Circuit Noise Calculations
When doing circuit noise calculations, it is instructive to keep the following points in mind:

- For circuit calculations involving noise sources it is convenient to replace the power spectral
density by equivalent sinusoidal generators in small bandwidths.
- The noise power spectral density in a small bandwidth $\Delta f$ is given by $\overline{V_\mathrm{n}^2} = \overline{v_\mathrm{n}^2} / \Delta f$ and $\overline{I_\mathrm{n}^2} = \overline{i_\mathrm{n}^2} / \Delta f$.
- The quantities $\overline{V_\mathrm{n}^2}$ and $\overline{I_\mathrm{n}^2}$ can be considered the mean-square value of sinusoidal generators. Using these values, network noise calculations reduce to familiar sinusoidal circuit-analysis calculations using $V_\mathrm{n}$ and $I_\mathrm{n}$.
- Multiple _independent_ noise sources can be calculated individually at the output, and the
total noise in bandwidth $\Delta f$ is calculated as a mean-square value by adding the individual
mean-square contributions from each sinusoid.
:::

### Noise in Impedance-Matched Systems

We now want to calculate the maximum noise power that can be extracted from a noisy source. We assume the following situation as shown in @fig-noise-matched-system. Note that the voltage source $\overline{V_\mathrm{n,s}^2}$ models the thermal noise of the source resistor $R_\mathrm{s}$ resulting in a Thevenin equivalent circuit.

{{< include /content/fundamentals/_fig_noise_matched_system.qmd >}}

We know that the noise of the source resistor is given by $\overline{V_\mathrm{n,s}^2} = 4 k T R_\mathrm{s}$. We assume the load resistor $R_\mathrm{load}$ as noiseless and matched to the source resistor, i.e., $R_\mathrm{load} = R_\mathrm{s}$ for [maximum power transfer](https://en.wikipedia.org/wiki/Maximum_power_transfer_theorem). The noise power spectral density delivered to the load resistor is then given by

$$
P_\mathrm{n,load} = \frac{\overline{V_\mathrm{n,load}^2}}{R_\mathrm{load}} = \frac{\overline{V_\mathrm{n,d}^2}}{4 R_\mathrm{s}} = k T
$$ {#eq-noise-power-matched}

The calculation of @eq-noise-power-matched confirms the initial statement that the maximum noise power spectral density that can be extracted from a noisy source is $k T$ (in W/Hz). This result is independent of the actual value of the source resistance $R_\mathrm{s}$.

We can further generalize the thermal noise of any impedance as

$$
\overline{V_\mathrm{n}^2} = 4 k T \Re \{ Z \}
$$ {#eq-thermal-noise-impedance}

as for example in the complex impedance $Z_\mathrm{ant}$ of an antenna. Since an antenna is a reciprocal device, if we measure its radiation impedance $Z_\mathrm{rad}$ (for example with a vector network analyzer), we can calculate its thermal noise with @eq-thermal-noise-impedance  to $\overline{V_\mathrm{n}^2} = 4 k T \Re \{ Z_\mathrm{rad} \}$.

### Noise Figure {#sec-noise-figure}

In RF systems, we often want to quantify the noise performance of a circuit block or a complete system. The most widely used metric is the **noise factor (F)**, which is defined as the ratio of the signal-to-noise ratio (SNR) at the input to the SNR at the output of a circuit block or system. If we express the noise factor in dB, we call it the **noise figure (NF)** [@Pozar_RF]. The noise factor is given by

$$
F = \frac{\text{SNR}_\mathrm{in}}{\text{SNR}_\mathrm{out}} = \frac{(P_\mathrm{s}/P_\mathrm{n})_\mathrm{in}}{(P_\mathrm{s}/P_\mathrm{n})_\mathrm{out}}
$$ {#eq-noise-figure}

where $P_\mathrm{s}$ is the signal power and $P_\mathrm{n}$ is the noise power. The noise factor is always larger than or equal to 1 (or 0 dB), as no circuit can improve the SNR!

::: {.callout-important}
## SNR Improvement
Note that the SNR can be improved by filtering, as filtering reduces the noise power. If the noise bandwidth is larger than the signal bandwidth, then the SNR can be improved without affecting the signal. However, this is not considered in the noise factor, as the noise factor assumes that both signal and noise pass through the same bandwidth.
:::

Let us look at a simple model of a noise circuit block as shown in @fig-noise-block. The input signal $S_\mathrm{in}$ is accompanied by noise $N_\mathrm{in}$. By definition it is assumed that the input noise power results from a matched resistor at $T_0 = 290\,\text{K}$, so that $N_\mathrm{in} = k T_0$. The circuit block has a power gain $G$ and adds its own noise $N_\mathrm{dut}$ to the output signal. For simplicity, we assume that the input and output of the circuit block are **impedance matched** to avoid reflections.

{{< include /content/fundamentals/_fig_noise_block.qmd >}}

The output signal and noise powers are then given by

$$
S_\mathrm{out} = G S_\mathrm{in}
$$

$$
N_\mathrm{out} = G N_\mathrm{in} + N_\mathrm{dut}
$$
The resulting noise factor can then be calculated as

$$
F = \frac{{S_\mathrm{in}}/{N_\mathrm{in}}}{{S_\mathrm{out}}/{N_\mathrm{out}}} = \frac{1}{G} \frac{G N_\mathrm{in} + N_\mathrm{dut}}{N_\mathrm{in}} = 1 + \frac{N_\mathrm{dut}}{G N_\mathrm{in}},
$$

in other words, the noise factor is 1 plus the ratio of the noise added by the device under test (DUT) to the amplified input noise.

Note that a noiseless block ($N_\mathrm{dut} = 0$) has a noise factor of $F=1$. A passive block with loss factor $L$ (and impedance matched at input and output) has a noise factor of $F=L$ (in linear units), as it attenuates the signal and $N_\mathrm{out} = N_\mathrm{in} = k T$ if everything is in thermal equilibrium.

{{< include /content/fundamentals/_fig_noise_cascade.qmd >}}

If we have a cascade of multiple blocks, as shown in @fig-noise-cascade, we can calculate the overall noise factor with the **Friis formula** [@Pozar_RF]

$$
F_\mathrm{total} = 1 + (F_1 - 1) + \frac{F_2 - 1}{G_1} + \frac{F_3 - 1}{G_1 G_2}
$$ {#eq-friis}

where $F_i$ and $G_i$ are the noise factor and power gain of the $i$-th block, respectively. Note that all gains have to be in linear units (not dB) when using @eq-friis. We can interpret @eq-friis as follows:

- The overall noise factor $F_\mathrm{total}$ is always larger than or equal to the noise factor of the first block ($F_1$).
- The noise factor of the first block is the most important one, as the noise factors of the following blocks are reduced by the gain of all preceding blocks. This is especially important in RF receivers, where the first block is usually a low-noise amplifier (LNA) with a very low noise figure (e.g., 1 dB or less) and a high gain (e.g., 10 dB or more). This ensures that the noise of the following blocks is negligible.
- The noise factor of the last block is reduced by the gain of all preceding blocks, so it is usually not very important.

Here we also see a trade-off between noise and linearity, as shown by @eq-iip3-cascade and @eq-friis. For low noise, we should try to maximize $G_1$, however, this will affect linearity (IIP3) in a negative way. As in many other situation in RF design, we have to find a good compromise between conflicting requirements.

### Sensitivity {#sec-sensitivity}

In RF receivers, we often want to know the minimum input signal power that can be detected with a certain SNR. This minimum input signal power is called the **sensitivity** of the receiver. The sensitivity can be calculated as

$$
P_\mathrm{in, min} = P_\mathrm{n} \cdot \text{SNR}_\mathrm{min} \cdot F
$$ {#eq-sensitivity}

where $P_\mathrm{n}$ is the noise power at the input, $\text{SNR}_\mathrm{min}$ is the minimum detectable SNR, and $F$ is the noise factor of the receiver. The input noise power can be calculated as

$$
P_\mathrm{n} = k T B
$$
    
where $k$ is the Boltzmann constant, $T$ is the temperature in Kelvin, and $B$ is the bandwidth of the receiver. Expressing @eq-sensitivity in dBm we get the following formula:

$$
P_\mathrm{in, min}|_\mathrm{dBm} = -174\,\text{dBm/Hz} + \text{NF} + 10 \log_{10}(B/\text{Hz}) + \text{SNR}_\mathrm{min}|_\mathrm{dB} 
$$ {#eq-sensitivity-dbm}

where -174 dBm/Hz is the thermal noise PSD at room temperature (290 K). We can see that the sensitivity improves with lower noise figure, smaller bandwidth, and lower minimum detectable SNR.

::: {#nte-wifi-sensitivity .callout-note}
## Sensitivity Calculation for WiFi
Let's calculate the sensitivity of a WiFi receiver operating at 5 GHz with a bandwidth of $B = 80\,\text{MHz}$, a noise figure of $NF = 7\,\text{dB}$, and a minimum detectable SNR of 25 dB. This high SNR means that a high-order modulation scheme (like 64-QAM) is used for high data rates.

Using @eq-sensitivity-dbm we get:
$$
P_\mathrm{in, min} = -174\,\text{dBm/Hz} + 7\,\text{dB} + 10 \log_{10} (80 \times 10^6) + 25\,\text{dB} \approx -63\,\text{dBm}
$$

This means that the minimum input signal power that can be detected by the WiFi receiver is approximately -63 dBm.
:::

## Modulation {#sec-fundamentals-modulation}

In order to transmit information via an EM wave, we need to modulate the EM wave with the information signal. Looking at a simple sinusoidal carrier wave

$$
s(t) = A \cos(\omega_0 t + \varphi)
$$

we see that we can change one or more of the following parameters to encode information:

- Amplitude $A(t)$ (amplitude modulation, AM; the digital form is called amplitude-shift keying, **ASK**)
- Frequency $\omega_0(t)$ (frequency modulation, FM; the digital form is called frequency-shift keying, **FSK**)
- Phase $\varphi(t)$ (phase modulation, PM; the digital form is called phase-shift keying, **PSK**)
- Amplitude $A(t)$ and phase $\varphi(t)$ (quadrature amplitude modulation, **QAM**)

The modulation formats FM and PM have the advantage that the carrier amplitude is constant, which makes them more robust against nonlinear distortion.

QAM is widely used in modern communication systems, as it allows to transmit more bits per symbol by combining amplitude and phase modulation. The form with 4 different symbols is called QPSK. Higher-order modulation like 16-QAM, for example, uses 16 different symbols, which can encode 4 bits per symbol (as $2^4 = 16$). Even higher-order QAM formats like 64-QAM (6 bits per symbol), 256-QAM (8 bits per symbol), 1024-QAM (10 bits per symbol), or 4096-QAM (12 bits per symbol) are also used in modern systems like WiFi or LTE.

{{< include /content/fundamentals/_fig_16qam_constellation.qmd >}}

Shown in @fig-16qam-constellation is the "constellation diagram" of a 16-QAM modulation format. The constellation points are arranged in a square grid, with each point representing a unique combination of amplitude and phase. The distance between the constellation points determines the robustness against noise and interference; larger distances result in better performance, but also require more bandwidth. The mapping of bits to constellation points is called "bit mapping" or "symbol mapping". The example in @fig-16qam-constellation uses a Gray code mapping, which minimizes the number of bit errors in case of a symbol error.

The constellation diagram can be imagined as a complex plane, where the x-axis represents the in-phase component (I) and the y-axis represents the quadrature component (Q) of the modulated signal. During transmission of a specific symbol, the RF carrier is modulated to the corresponding amplitude and phase, resulting in a specific point in the constellation diagram. In @fig-16qam-constellation, the amplitude and phase information for two consecutive symbols, $A_i$/$\varphi_i$ and $A_{i+1}$/$\varphi_{i+1}$, is shown. If we have a bitrate with a bit duration of $T_\mathrm{b}$, the symbol duration for 16-QAM (4 bits per symbol) is $T_\mathrm{s} = 4 T_\mathrm{b}$. During the first $T_\mathrm{s}$, the carrier is modulated to $A_i$/$\varphi_i$, and during the next $T_\mathrm{s}$, it is modulated to $A_{i+1}$/$\varphi_{i+1}$.

The table below shows the SNR requirements for different modulation formats to achieve a bit error rate (BER) of $10^{-5}$ in an additive white Gaussian noise (AWGN) channel. As we can see, higher-order modulation formats require higher SNR to achieve the same BER.

| Modulation | Bits/Symbol | Required SNR (dB) |
|------------|--------------|--------------------|
| BPSK       | 1            | 9.6                |
| QPSK       | 2            | 12.6               |
| 16-QAM     | 4            | 18.2               |
| 64-QAM     | 6            | 24.4               |  
| 256-QAM    | 8            | 30.6               |
| 1024-QAM   | 10           | 36.9               |
| 4096-QAM   | 12           | 43.2               |
: SNR requirements for different modulation schemes to achieve BER = $10^{-5}$ in AWGN channel {#tbl-modulation-snr}

## Pulse Shaping and Spectral Efficiency {#sec-fundamentals-pulse-shaping}

When we modulate symbols onto a carrier, we usually do not transmit the symbols as pure sinusoids, but rather as pulses with a certain shape. The pulse shape determines the bandwidth of the transmitted signal and its spectral efficiency. A common pulse shape is the rectangular pulse, which has a sinc-shaped spectrum. However, the sinc function $\sin(\pi x)/ \pi x$ has side lobes that extend to infinity, which can cause interference with adjacent channels.

For reference, the spectrum of a random binary sequence with equal probability of 0s and 1s, using rectangular pulses with a duration of $T_\mathrm{b}$ is given by ($S(f)$ is the two-sided power spectral density):

$$
S(f) = \frac{T_\mathrm{b}}{4} \text{sinc}^2(fT_\mathrm{b}) + \frac{1}{4} \delta(f) = \frac{T_\mathrm{b}}{4} \left( \frac{\sin(\pi f T_\mathrm{b})}{\pi f T_\mathrm{b}} \right)^2 + \frac{1}{4} \delta(f)
$$

To avoid this, we can use pulse shapes that have better spectral properties, such as the raised cosine pulse or the root-raised cosine pulse. The raised-cosine pulse has a roll-off factor $\alpha$ that determines the excess bandwidth beyond the Nyquist bandwidth. The **root-raised cosine (RRC)** pulse is used in practical systems, as it can be implemented with a matched filter at the receiver.

The raised-cosine pulse $p(t)$ (with a spectrum shaped like a raised cosine) is given by:

$$
p(t) = \frac{\sin(\pi t / T_\mathrm{b})}{\pi t / T_\mathrm{b}} \cdot \frac{\cos(\alpha \pi t / T_\mathrm{b})}{1 - (2 \alpha t / T_\mathrm{b})^2}
$$

Setting $\alpha = 0$ results in a sinc pulse in the time domain (with a perfect bandwidth containment in the frequency domain), while $\alpha = 1$ results in a pulse with double the Nyquist bandwidth. The pulse shape for $\alpha = 0$ and $\alpha = 0.22$ (used in 3G) is shown in @fig-raised-cosine-pulse.

{{< include /content/fundamentals/_fig_raised_cosine_pulse.qmd >}}

Another often-used pulse shape is the Gaussian pulse, which is used in Gaussian minimum-shift keying (GMSK, used in 2G) modulation, or in Gaussian frequency-shift keying (GFSK, used in Bluetooth). The Gaussian pulse has a smooth shape and a narrow spectrum. The Gaussian pulse is given by:

$$
p(t) = \frac{\sqrt{\pi}}{\alpha} e^{-(\pi t / \alpha)^2} \quad \text{with} \quad \alpha = \frac{\sqrt{\ln 2}}{\sqrt{2}} \cdot \frac{T_\mathrm{b}}{B T_\mathrm{b}}
$$

where $B T_\mathrm{b}$ controls the width of the pulse. The spectrum of the Gaussian pulse is also Gaussian-shaped, which helps to minimize inter-symbol interference (ISI). 

The Gaussian pulse for $BT=0.5$ as used in Bluetooth is shown in @fig-gaussian-pulse.

{{< include /content/fundamentals/_fig_gaussian_pulse.qmd >}}

For both the raised-cosine and Gaussian pulse, the trade-off between time- and frequency-domain containment is clearly visible. This is also captured in "**Küpfmüller's uncertainty principle**", which states that the product of the time duration and the bandwidth of a pulse is lower-bounded by a constant. In other words, if we want to have a pulse that is very short in time, it will have a wide bandwidth, and vice versa.

## Orthogonal Frequency-Division Multiplexing (OFDM) {#sec-fundamentals-ofdm}

As we have seen in the previous section, if we make the symbol rate high, we need to use pulses with a wide bandwidth. The problem with a wide bandwidth in wireless communication is **multi-path propagation**, which causes frequency-selective fading. This means that some frequencies are attenuated more than others, which can cause errors in the received signal. Equalizing such a frequency-selective channel can be very complex, especially if the channel changes rapidly (as in mobile communication). We now face a dilemma: How can we achieve high data rates (which require high symbol rates and thus wide bandwidth) while avoiding frequency-selective fading? The key idea, implemented in **OFDM**, is to split the wideband channel into multiple narrowband sub-channels (subcarriers), each with a low symbol rate. This way, each subcarrier experiences flat fading, which is much easier to equalize.

The key question is now how to implement this idea efficiently, as we now have to apply modulation to hundreds or thousands of individual subcarriers. The solution is to use the **inverse fast Fourier transform (IFFT)** at the transmitter to generate the time-domain OFDM signal from the frequency-domain symbols, and the **fast Fourier transform (FFT)** at the receiver to recover the frequency-domain symbols from the time-domain OFDM signal. This is illustrated in @fig-ofdm-transceiver.

{{< include /content/fundamentals/_fig_ofdm_transceiver.qmd >}}

The OFDM transmitter takes a block of $N$ symbols (e.g., 64-QAM symbols) and maps them onto $N$ subcarriers, thereby reducing the symbol rate for each subcarrier to $T_\mathrm{b} / N$. The IFFT then generates the time-domain OFDM signal, which is transmitted over the wireless channel. Before transmission the **cyclic prefix (CP)** is added to each OFDM symbol.

At the receiver, first the CP is removed, and then the FFT recovers the frequency-domain symbols, which can then be equalized (fairly simply by multiplying each subcarrier with a complex factor to correct amplitude and phase) and demodulated.

A key property of OFDM is that the subcarriers are **orthogonal** to each other, which means that they do not interfere with each other, even if they overlap in frequency. This is achieved by choosing the subcarrier spacing $\Delta f$ such that it is equal to the symbol rate $1/T_\mathrm{b}$, i.e., $\Delta f = 1/T_\mathrm{b}$. This way, the integral of the product of two different subcarriers over one symbol period is zero, which means that they are orthogonal.

To further improve the robustness against multi-path propagation, a CP is added to each OFDM symbol. The CP is a copy of the last part of the OFDM symbol, which is added to the beginning of the symbol. This way, if there are delayed copies of the OFDM symbol due to multi-path propagation, they will still fall within the CP and will not cause inter-symbol interference (ISI). The length of the CP should be longer than the maximum delay spread of the channel.

::: {#nte-lte-ofdm .callout-note}
## OFDM in LTE
In LTE OFDM is used for the downlink (base station to user equipment) with the following parameters:

- Subcarrier spacing: 15 kHz
- CP length: 5.2 µs (normal), 4.7 µs (extended)
- Number of subcarriers: 1200 (for 20 MHz bandwidth)
- Modulation: QPSK, 16-QAM, 64-QAM, 256-QAM

From the subcarrier spacing we can calculate the symbol duration as $T_\mathrm{b} = 1 / \Delta f = 1 / 15\,\text{kHz} \approx 66.7\,\mu\text{s}$.

We can calculate the raw bitrate for a 20 MHz LTE channel as

$$
\text{Bitrate} = N_\mathrm{sc} \cdot N_\mathrm{sym} \cdot \frac{1}{T_\mathrm{b} + T_\mathrm{CP}} = 1200 \cdot 8 \cdot \frac{1}{66.7\,\mu\text{s} + 5.2\,\mu\text{s}} \approx 133\,\text{Mbps}
$$

Without the overhead for control channels and error correction coding a user datarate of approximately 100 Mbps can be achieved in a 20 MHz LTE channel.
:::

## Multiple Access Techniques {#sec-fundamentals-multiple-access}

In wireless communication systems, multiple users need to share the same frequency spectrum. This is achieved by using **multiple access techniques**, which allow multiple users to transmit and receive data simultaneously without interfering with each other. The most common multiple access techniques are:

1. **Time division multiple access (TDMA)**: Users are assigned specific time slots for transmission, allowing multiple users to share the same frequency channel by dividing the time into slots.

2. **Frequency division multiple access (FDMA)**: Users are assigned specific frequency bands within the overall frequency spectrum, allowing multiple users to transmit simultaneously on different frequencies.

3. **Code division multiple access (CDMA)**: Users are assigned unique spreading codes, allowing them to transmit simultaneously over the same frequency band. The receiver uses the code to extract the desired signal. A variant of CDMA is frequency-hopping spread spectrum (FHSS), where the carrier frequency is changed rapidly according to a pseudo-random sequence known to both the transmitter and receiver. This is used in Bluetooth.

4. **Orthogonal frequency division multiple access (OFDMA)**: A variant of OFDM, where multiple users are assigned different subcarriers for transmission, allowing for efficient use of the frequency spectrum. This is used in 4G LTE and 5G NR.

5. **Spatial division multiple access (SDMA)**: Uses multiple antennas to create spatially separated channels, allowing multiple users to transmit simultaneously in the same frequency band.

In addition, all of these techniques can be combined to create more efficient and flexible communication systems. For example, OFDMA can be used in conjunction with SDMA to allow multiple users to share the same frequency resources while also taking advantage of spatial diversity. Also, TDMA can be combined with FDMA to create a hybrid multiple access scheme (which has been used in 2G GSM).